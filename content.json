{"meta":{"title":"着陆的橘子","subtitle":null,"description":null,"author":"AaronShi","url":"https://aaronshi32.github.io"},"pages":[],"posts":[{"title":"基础知识-SQL语法","slug":"基础知识-SQL语法","date":"2018-08-08T00:44:29.000Z","updated":"2018-08-08T04:40:29.533Z","comments":true,"path":"2018/08/08/基础知识-SQL语法/","link":"","permalink":"https://aaronshi32.github.io/2018/08/08/基础知识-SQL语法/","excerpt":"","text":"SQL 语法基础DDL: create table/alter table/drop table/create index/drop index/create database/alter database DML: select/update/delete/insert into TOP: select top number|percent from …. 其中 number 可以接 order by 排序, percent 不可以 通配符: ‘%’ 若干个, ‘_’ 一个, ‘[ANF]’ 包含 A 或 N 或 F, ‘[!ANF]’ 不包含 A 或 N 或 F Between..And: 在 mysql 中不包含右边界 AS: 别名 select name as n, sale as s … JOIN table ON col.A = col.B: 内 JOIN 只返回两张表有的记录, 左 JOIN 返回左表全部记录, 右 JOIN 返回右表全部记录, Full JOIN 返回全部表记录 UNION: 用于合并两个或多个 SELECT 语句的结果集, 内部的 SELECT 语句必须拥有相同数量的列, 列也必须拥有相似的数据类型, UNION 会去重, 若不去重则用 UNION ALL SELECT INTO: 从一个表中选取数据，然后把数据插入另一个表中. SELECT * INTO new_table_name FROM old_tablename CREATE INDEX index_name ON table_name (column_name): 创建索引 ALTER TABLE table_name ADD column_name datatype: 添加列 ALTER TABLE table_name DROP COLUMN column_name: 删除列 ALTER TABLE table_name ALTER COLUMN column_name datatype: 修改列类型 CREATE VIEW view_name AS …: 创建视图 GROUP BY: 合计函数 (比如 SUM) 常常需要, 按列规约结果 HAVING：在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。 高级MySQL IF( expr1 , expr2 , expr3 ) CASE WHEN: case 列名 when 条件 then 结果 else 其它结果 end 别名 123456789101112SELECT CASE detail.`status` WHEN '0' THEN '未开仓'WHEN '1' THEN '已开仓'WHEN '2' THEN '已平仓'ELSE '取消订单' ENDstatus,CASE o.type WHEN '0' THEN '单期'WHEN '1' THEN '多期'ELSE '策略' END typeNameFROM t_order_detail detailLEFT JOIN t_order o ON o.id = detail.orderId IFNULL( expr1 , expr2 ): 在 expr1 的值不为 NULL的情况下都返回 expr1，否则返回 expr2","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"Java分布式服务框架","slug":"Java分布式服务框架","date":"2018-07-06T07:13:28.000Z","updated":"2018-07-12T01:31:14.116Z","comments":true,"path":"2018/07/06/Java分布式服务框架/","link":"","permalink":"https://aaronshi32.github.io/2018/07/06/Java分布式服务框架/","excerpt":"","text":"简述业界主流(O:Open Source): Thrift(O), Avro-RPC(O), Hessian(O), gRPC(O), Dubbo(O), HSF, Coral Service(亚马逊), DSF(华为) 分布式服务框架包括: RPC组件, 配置化服务发布, 基于服务注册中心的订阅和发布, 服务治理 RPC 组件: 通信框架, 编码, 协议栈涉及到的技术: Socket 通信, 多线程, 协议栈 -&gt; Netty 关键字: 长连接, NIO(多路复用) epoll 没有最大连接句柄 1024/2048 的限制, 意味着只需要一个线程负责 Selector 的轮询, 就可以接入成千上万的客户端 可靠性设计靠心跳来实现: TCP 层面的心跳检测(Keep-Alive), 协议层的心跳检测, 应用层的心跳检测 Netty 的心跳检测实际上利用了链路空闲检测机制实现的, 默认为读写空闲(链路持续时间 t 没有接受或者发送消息) Netty 的 EventLoopGroup 线程组会默认创建 CPU Core * 2 个线程, 使用的时候一定要评估线程数指定, 最好不要使用默认, 或者创建一个数组, 按照 Hash 复用 EventLoopGroup 服务组件: 路由基于服务注册中心(例如: Zookeeper)的订阅发布机制, 消费者可通过主动查询和被动通知的方式获取服务提供者的地址, 消费者本地也缓存服务地址列表, 当注册中心挂掉时, 仍可向服务发起通信 消费者访问服务的负载均衡: 随机, 轮循, 服务调用延时(消费者缓存服务调用延时, 计算权重, 让延时高的服务接收更少的消息), 一致性哈希 本地路由优先策略: injvm(本地 JVM 中), innative(相同物理机或者VM) 一致性哈希: 希望在增删节点(集群)的时候，让尽可能多的数据不失效, 精华:每个实际节点的N个虚拟节点尽量随机分布在整数环上,增加cache节点时,就能尽量保证移动到新cache节点的key来自于不同的cache节点, 从而保证负载均衡, 即 hash(key) -&gt; 虚拟节点 -&gt; 真实节点 集群容错什么场景需要容错: 通信链路故障, 服务端超时, 服务端调用异常失败 容错策略: 失败自动切换(Failover), 失败通知(Failback), 失败缓存(Failcache), 快速失败(Failfast) 不同的容错策略适用于不同的业务服务, 容错接口开放使得服务提供者能够按需配置自己的策略 HSF 默认采取失败自动切换的容错 服务调用: 同步, 异步, 泛化用户发起远程服务调用之后, 经历层层业务逻辑处理、消息编码、最终序列化后的消息会被放入到通信框架的消息队列中 异步实际上是返回 Future 对象, 调用者可以通过 get 阻塞等待获取结果, 也可以通过 Future-Listener 进行回调 泛化调用: 客户端没有API接口及数据模型时, 泛化引用将参数及返回值中的所有 POJO 均用 Map 表示; 服务端没有API接口及数据模型时, 泛化实现将参数及返回值中的所有 POJO 均用 Map 表示; 常用于通用测试","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"Java并发编程实践","slug":"Java并发编程实践","date":"2018-07-04T05:36:08.000Z","updated":"2018-08-24T08:43:17.469Z","comments":true,"path":"2018/07/04/Java并发编程实践/","link":"","permalink":"https://aaronshi32.github.io/2018/07/04/Java并发编程实践/","excerpt":"","text":"线程安全定义: 1)不管运行时线程采取何种调度方式, 2)代码中不需要任何额外的同步或协同, 这个类都能表现出正确的行为, 无状态的对象, 不可变对象一定是线程安全的 同步机制: synchronized, volatile, lock, atom 两个维度: 防止某个线程正在使用对象状态而另一个线程在同时修改状态(同步机制); 确保当一个线程修改了对象状态后, 其他线程能够看到发生的状态变化(可见性) synchronized 是可重入的, 重入的一种实现方法是为每个锁关联一个获取计数值和一个所有者线程。当计数值为0时, 这个锁就被认为是没有被任何线程持有 重排序: 在没有同步的情况下, 编译器、处理器以及运行时等都可能对操作的执行顺序进行一些意想不到的调整。在缺乏足够同步的多线程程序中, 要相对内存操作的执行顺序进行判断, 几乎无法得出正确的结论 进一步衍生: 重排序的As-if-serial语义意思是，所有的动作(Action)都可以为了优化而被重排序，但是必须保证它们重排序后的结果和程序代码本身的应有结果是一致的。Java编译器、运行时和处理器都会保证单线程下的as-if-serial语义。比如，为了保证这一语义，重排序不会发生在有数据依赖的操作之中 加锁的意义: 保证互斥行为(可见性和原子性)以及内存可见性, 确保所有线程都能看到共享变量的最新值, 所有执行读操作或者写操作的线程都必须在同一个锁上同步 volatile 的实现原理: 不会将该变量上的操作重排序(内存屏障), 也不会放入缓存寄存器中 同步机制可确保原子性和可见性, volatile 只能保证可见性(++count 无法保证原子性) ThreadLocal, 线程封闭, 不共享的变量: 为变量在每个线程中都创建一个副本，每个线程可以访问自己内部的副本变量, 实现为: ThreadLocal -&gt; Map -&gt; thread.threadLocals -&gt; 内部类 ThreadLocalMap, 场景: 数据库连接, Session 管理 想要做到线程安全: 同步保证原子性操作, 轻量 Volatile 保证可见性, 不变的对象一定是线程安全的(final) 内存屏障:1) Happens-before的前后两个操作不会被重排序且后者对前者的内存可见2) LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。 LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。 StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见 volatile语义中的内存屏障: 在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障； 在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障； final语义中的内存屏障: 写final域：在编译器写final域完毕，构造体结束之前，会插入一个StoreStore屏障，保证前面的对final写入对其他线程/CPU可见，并阻止重排序。 读final域：在上述规则2中，两步操作不能重排序的机理就是在读final域前插入了LoadLoad屏障。 锁的实现乐观锁和悲观锁 乐观锁, 持有后再请求, 告知失败, 请求线程可在一定时间内重试, 不会挂起 悲观锁, 持有后再请求, 被挂起, 直到锁释放后恢复线程执行, 有可能获得该锁 构建线程安全类同步容器: Vector 和 HashTable, 方法级加锁(synchronized), 并发效率低 并发容器: ConcurrentHashMap, ConcurrentLinkedQueue, CopyOnWriteArrayList 等, 灵活加锁, 并发效率高 阻塞队列实现生产者-消费者: BlockingQueue 的实现有 LinkedBlockingQueue 和 ArrayBlockingQueue 都是 FIFO 队列, PriorityBlockingQueue 是优先级队列, 其中 put 和 take 方法是阻塞, offer 和 poll 是非阻塞, 对于无边界的队列而言永远不会阻塞 同步工具类: 信号量(Semaphore), 栅栏(Barrier, 调用 await 的线程会中断, 直到等于 partion 时再开始执行), 闭锁(Latch, 调用 await 的线程会中断, 直到 countDown 为 0 被唤醒) 区别性: 闭锁是一次性的, 不能反复使用, 栅栏可以多次使用; 闭锁用于等待事件，而栅栏用于等待其他线程 工作密取: 消费者各自拥有一个双端队列 Deque, 如果一个消费者完成了自己双端队列中的全部工作，那么他就可以从其他消费者的双端队列末尾秘密的获取工作。具有更好的可伸缩性, 使用场景为网络爬虫 CopyOnWrite 容器适用于读多写少的并发场景, set 方法加锁, get 方法不加锁, set 方法是 copy 一份新的内容, 修改, 再讲原引用指向新的, 这篇文章一目了然: https://www.cnblogs.com/dolphin0520/p/3938914.html 结构化并发程序无限制创建线程的不足: 线程生命周期开销非常高, 资源消耗大, 稳定性低 Executor 线程池框架, 抽象了执行任务的主要对象不是 Thread, 而是 Executor, 其实现了对生命周期的支持(ExecutorService), 以及统计信息收集, 应用程序管理机制和性能监控等机制, Executor 基于生产者 - 消费者模式 Executors 是线程池的工厂类, 本质上是初始化 ThreadPoolExecutor, 实际开发的时候应该避免使用工厂类 ExecutorService 封装了线程生命周期接口 线程中断, 取消, 关闭调用 interrput 并不意味着立即停止目标线程正在进行的工作, 而只是传递了请求中断的消息, 通常, 中断是实现取消的最合理方式 通过 Future 和 Executor 框架可以构建可取消的任务 ExecutorService 提供了正常关闭 shutdown() 和 立刻关闭 shutdownNow()两种方法, 正常关闭速度慢但线程安全, 它会等到队列中所有任务都执行完成后才关闭 还可以通过 JVM 钩子关闭线程, 但是极度不推荐, 线程可分为两种: 普通线程和守护线程 使用 newTaskFor 钩子函数来改进用来封装非标准取消的方法。这是 ThreadPoolExecutor 的新特性, 当提交一个 callable 给 ExecutorService 时，submit 返回一个 Future，可以用 Future 来取消任务。 newTaskFor 钩子是一个工厂方法，创建一个 Future 来代表任务，这个 Future 属于由 FutureTask 实现的 RunnableFuture 接口，这个接口可以自定义 cancel 方法，实现自定义的取消方式 线程池线程池大小如何界定, 是 Fix 还是 Cache? 计算密集型: 线程池大小为: Ncpu + 1, I/O 密集型: 线程池大小为: 2 * Ncpu newFixedThreadPool corePoolSize == maximumPoolSize workQueue 是无界队列(容易造成资源耗尽) 当需要限制当前任务的数量以满足资源管理需求时, 那么可以选择固定大小的线程池(服务器网络请求限制) newCachedThreadPool workQueue 是有界队列, 饱和之后执行拒绝策略, 而实现中采用了 SynchronousQueue(同步移交) 提供比固定大小的线程池更好的排队性能(而不是一直hang住等待线程) 1234567public ThreadPoolExecutor(int corePoolSize, // 基本大小 int maximumPoolSize, // 最大大小 long keepAliveTime, // TimeUnit unit, // 时间单位 BlockingQueue&lt;Runnable&gt; workQueue, // 任务队列 ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123;...&#125; 只有当任务相互独立时, 为线程池或者工作队列设置界限才是合理的, 如果任务之间存在依赖性, 那么有界的线程池或队列就可能导致线程 “饥饿” 死锁问题 活跃性, 性能和测试死锁与线程获得锁的顺序有关, 在数据库系统的设计中考虑了死锁的检测以及恢复, 即随机选择放弃一个事务, 让其他事务执行 可伸缩性是指: 当增加计算资源时(例如 CPU, 内存, 存储容量或 I/O 带宽), 程序的吞吐量或者处理能力能相应增加 缩小锁的范围(方法 -&gt; 对象 -&gt; 并发容器), 减小锁的粒度, 锁分段(ConcurrentHashMap) 锁的实现锁干了什么? 答: 对共享资源的互斥性以及内存可见性 内置锁有什么功能局限? 答: 无法中断一个正在等待获取锁的线程, 或者无法在请求获取一个锁时无限等待下去, 阻塞加锁 显示锁有什么好处? 答: 锁粒度更低, 可实现如 ConcurrentHashMap 式的分段加锁 内置锁: Synchronized 和 volatile 显示锁: ReentrantLock 内置锁的实现原理 synchronized 有以下3种应用方式 修饰实例方法，作用于当前实例加锁 修饰静态方法，作用于当前类对象加锁 修饰代码块，指定加锁对象，对给定对象加锁 同步代码块的实现是基于进入和退出管程(Monitor)对象实现, 翻译成字节码是 monitorenter 和 monitorexit 同步方法的实现是JVM通过ACC_SYNCHRONIZED访问标志来辨别一个方法是否声明为同步方法 volatile 的实现原理: 不会将该变量上的操作重排序(内存屏障), 也不会放入缓存寄存器中 volatile语义中的内存屏障: 在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障； 在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障； Synchronized 可确保原子性和可见性, volatile 只能保证可见性(++count 无法保证原子性) 显示锁的实现原理 123456789//juc lockpublic interface lock&#123; void lock(); void lockInterruptibly() throws InterruptedException; //中断锁 boolean tryLock(); //轮询锁 boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; //定时锁 void unlock(); Condition newCondition();&#125; 公平性: 在公平锁上, 线程将按照它们发出请求的顺序来获取锁, 非公平则允许插队, 取决于场景中挂起和恢复线程的开销大小, 在激烈竞争的情况下, 则采取非公平锁比较好 实现: CopyOnWriteArrayList 一个资源可以被多个读操作访问, 或者被一个写操作访问, 但两者不能同时进行 内部是非公平锁, 读不加锁, 写加锁 显示锁的底层是 AQS(AbstractQueuedSynchronizer), 它使得一组线程(称之为等待线程集合)能够通过某种方式来等待特定的条件变成真, 并自动唤醒, 与传统队列不同的是它的对象是一个个正在等待的线程 总结: 与内置锁相比, 显示锁提供了一些扩展功能, 在处理锁的不可用性方面有着更高的灵活性 读写锁允许多个读线程并发地访问被保护的对象, 当访问以读取操作为主的数据结构时, 它能够提高程序的可伸缩性 无锁CAS(Compare-and-Swap): 操作系统级别支持的一种无锁操作, 通过代码中显示的重试机制来实现 1234567public synchronized int compareAndSwap(int expectedValue, int newValue)&#123; int oldValue = value; //当前值 if(oldValue == expectedValue)&#123; //当前值 == 期望值 才改变当前值 value = newValue; &#125; return oldValue; //返回当前值&#125; 原子变量类是基于 CAS 实现的, 比锁的粒度更细, 量级更轻, 以下是 i++ 原子性的实现 12345int v;do&#123; v = value.get();&#125;while(v != value.compareAndSwap(v, v + 1));return v + 1; 在高度竞争的情况下, 锁的性能将超过原子变量的性能, 但通常竞争量很小的情况下, 原子变量更优, 这是因为锁在发生竞争时会挂起线程, 降低了CPU使用率, 调用原子变量的类可自行负责竞争管理(重试, 退让)","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"},{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"基础知识-MySQL","slug":"基础知识-MySQL","date":"2018-06-21T11:20:04.000Z","updated":"2018-08-16T12:15:56.086Z","comments":true,"path":"2018/06/21/基础知识-MySQL/","link":"","permalink":"https://aaronshi32.github.io/2018/06/21/基础知识-MySQL/","excerpt":"","text":"MySQL架构概述MySQL 最重要、最与众不同的特性是它的存储引擎架构，这种架构的设计将查询处理（Query Processing）及其他系统任务（Server Task）和数据的存储/提取相分离。本文概要地描述了MySQL的服务器架构以及各存储引擎之间的主要区别。 MySQL逻辑架构下图展示了MySQL的逻辑架构图。 MySQL的逻辑架构分为三层。 第一层: 最上层的服务并不是MySQL所独有，大多数基于网络的客户端/服务器的工具或者服务都有类似架构。比如连接处理、授权认证、安全等。 第二层: 大多数MySQL的核心服务都在，包括查询、分析、优化、缓存以及所有的内置函数，所有的跨存储引擎的功能都在这一层实现：存储过程、触发器、视图。 第三层: 包含了存储引擎。存储引擎负责MySQL中数据的存储和提取。服务器通过API与存储引擎进行通信，这些接口屏蔽了存储引擎间的差异性，使得这些差异对上层的查询过程透明。存储引擎不会去解析SQL（除InnoDB会解析外键之外），不同存储引擎之间也不会相互通信，而只是简单地响应上层服务器的请求。 连接管理与安全性 每一个客户端连接都会在服务器进程中拥有一个线程，这个连接的查询只会在这个单独的线程中执行。服务器会缓存线程，因此不需要为每个新建的连接创建和销毁线程。 但客户端连接到MySQL服务器时，服务器需要对其进行认证。认证基于用户名、原始主机信息和密码。一旦客户端连接成功，服务器会继续验证该客户端是否具有执行某个特定查询的权限。 优化与执行 MySQL会解析查询，并创建内部数据结构（解析树），然后对其进行各种优化，包括重写查询、决定表的读取顺序，以及选择合适的索引等。 优化器不关心表使用的哪种存储引擎，但存储引擎对于优化查询是有影响的。 对于SELECT语句，在查询解析之前，服务器会先检查查询缓存（Query Cache），如果能够在其中找到对应的查询，服务器就不必再执行查询解析 并发控制读写锁 共享锁 读锁 非阻塞 排他锁 写锁 阻塞 写锁比读锁有更高的优先级，可以插入到读锁队列的前面 锁粒度 锁策略：在锁的开销和数据的安全性之间寻求平衡 表锁 开销最小 行锁 最大并发度，最大锁开销 事务事务是一组原子性的 SQL 查询，或者说一个独立的工作单元。事务内的语句要么全部执行成功，要么全部执行失败。 START TRANSACTION 开始事务 COMMIT 提交事务 ROLLBACK 撤销事务 事务的特征ACID atomicity 原子性 一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚。 consistency 一致性 数据库总是从一个一致性的状态转换到另外一个一致性的状态。 isolation 隔离性 通常来说，一个事务所做的修改在最终提交前，对其它事务是不可见的。 durability 持久性 一旦事务提交，则其所做的修改就会永久保存到数据库中 隔离级别 SQL标准定义了四种隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见的。较低级别的隔离通常可以执行更高的并发，系统的开销也更低。 READ UNCOMMITED（未提交读） 事务中的修改即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这被称为脏读（Dirty Read）。 READ COMMIT（提交读） 一个事务从开始到提交之前，所做的任何修改对其它事务都是不可见的。这个级别有时候也叫不可重复读（nonrepeateble read），因为两次执行相同的查询，可能会得到不一样的结果。 REPEATABLE READ（可重复读） 解决了脏读和不可重复读，理论上，无法解决幻读的问题。幻读（Phantom Read），是指当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入了新的记录，当之前的事务再次读取范围的记录时，会产生幻行。 InnoDB 和 XtraDB 存储引擎通过多版本并发控制解决了幻读的问题。可重复读是 MySQL 的默认隔离级别 SERIALIZABLE（可串行化） SERIALIZABLE是最高的隔离级别。它通过强制事务串行执行，避免前面说的幻读的问题。 死锁 死锁是指两个或者多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环的现象。当多个事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定一个资源时，也会产生死锁。InnoDB 目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。 死锁的产生有双重因素：有些是真正的数据冲突，有些是由于存储引擎的实现方式导致的。死锁发生后，只有部分或者完全回滚其中一个事务才能打破死锁。大多数情况下，只需重新执行因死锁回滚的事务即可。 事务日志 事务日志可以帮助提高事务的效率。使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到硬盘。事务日志采用的是追加的方式，事务日志持久后，内存中被修改的数据在后台可以慢慢刷回到磁盘。通常称为预写式日志（Write-Ahead Logging），修改数据需要写两次磁盘。 如果数据的修改已经记录到日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据。 MySQL中的事务 MySQL 中提供了两种事务型的存储引擎：InnoDB 和 NDB Cluster。另外还有一些第三方的存储引擎，比如 XtraDB 和 PBXT。 MySQL 默认采用自动提交（AUTOCOMMIT）模式。也就是说，如果不是显式地开始一个事务，则每个查询都被当做一个事务执行提交操作。在当前连接中，可以通过设置AUTOCOMMIT变量来启用或者禁用自动提交模式。还有一些命令，在执行前强制执行 COMMIT 提交当前的活动事务。如A LTER TABLE、LOCK TABLES 等。MySQL 可以通过执行 SET TRANSACTION LEVEL 命令来设置隔离级别。MySQL能够识别所有的4个ANSI隔离级别，InnoDB 引擎也支持所有的隔离级别。 MySQL 服务器层不管理事务，事务是由下层的存储引擎实现的。所有在同一个事务中，使用多种存储引擎是不可靠的。InnoDB 采用的两阶段锁定协议（two-phase locking protocol）。在事务执行过程中，随时都可以执行锁定，锁只有在执行COMMIT或者ROLLBACK 的时候才会释放，并且所有的锁是在同一时刻被释放。InnoDB 会根据隔离级别在需要的时候自动加锁。InnoDB 也支持通过特定的语句进行显示锁定，如SET … LOCK IN SHARE MODE等。 多版本并发控制（MVCC）可以认为MVCC是行级锁的一个变种，但是在很多情况下避免了加锁操作，开销更低。 MVCC的实现，是通过保存数据在某一个时间点的快照来实现的。不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始时间的不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。 不同存储引擎的 MVCC 实现不同，典型的有乐观并发控制和悲观并发控制。InnoDB 的 MVCC，是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。存储的不是实际的时间值，而是系统版本号。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。保存这两个额外的系统版本号，使大多数读操作都可不用加锁。不足之处是每行记录都需要额外的存储空间。MVCC 只在 REPEATABLE READ 和 READ COMMITTED 两个隔离级别下工作。 存储引擎InnoDB MySQL默认的事务型引擎。InnoDB采用MVCC来支持高并发，并且实现了四个标准的隔离级别。默认REPEATABLE READ，通过间隙锁（next-key locking）策略防止幻读的出现。间隙锁使得InnoDB不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定，以防止幻影行的插入。 InnoDB 是基于聚簇索引建立的，对之间查询有很高的性能。它的二级索引（非主键索引）中必须包含主键列，如果主键列很大的话，其它所有索引都会很大。 从磁盘读取数据时采用可预测性预读，能够自动在内存中创建hash索引以加速读操作的自适应哈希索引(adaptive hash index)，以及能够加速插入操作的插入缓冲区（insert buffer）等。参见 官方手册《InnoDB事务模型和锁》 InnoDB可以通过一些机制和工具支持真正的热备份，Oracle的MySQL Enterprise Backup、Percona的XtraBackup可以做到这一点。MySQL的其它存储引擎不支持热备份，要获取一致性视图需要停止对所有表的写入。 MyISAM MyISAM 支持全文本索引、压缩、空间函数（GIS）等。不支持事务和行级锁，崩溃后无法安全恢复。 MyISAM 将表存在两个文件中：数据文件（.MYD）索引文件（.MYI）。如果表是变长行，则默认配置只能处理256TB的数据。 特性加锁与并发对整张表而不是行加锁。但是在表有读取查询的同时，也可以往表中插入新的记录（并发插入，concurrent insert） 修复CHECK TABLE mytabl 检查表的错误。REPAIR TABLE mytable 进行修复。当MySQL服务器关闭，可以用myisamchk命令行工具进行检查和修复。 索引即使是 BLOB 和 TEXT 等长字段，也可以基于其前 500个 字符创建索引。MyISAM也支持全文本索引，是一种基于分词创建的索引，可以支持复杂的查询。 延迟更新索引键（Delay Key Write）在创建 MyISAM 表的时候，如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立刻将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区（in-memory key buffer），只有在清理键缓存区或者关闭表的时候才会将对应的索引块写入到磁盘，极大地提升了写入性能。 压缩表如果表在创建并导入数据后，不会再进行修改操作，那么这样的或许适合采用MyISAM压缩表。可以使用 myisampack 对 MyISAM 表进行压缩。压缩表能极大减少磁盘空间占用和磁盘 I/O。 MySQL内建的其它存储引擎 Archive、Blackhole、CVS、Frederated、Memory、NDB 集群引擎 总结MySQL 拥有分层的架构，上层是服务器层的服务和查询执行引擎，下层则是存储引擎。 MySQL 最初基于 ISAM 构建（后来被 MyISAM 取代），其后陆续添加了更多的存储引擎和事务支持。从 MySQL 5.5 起，InnoDB 成为默认的存储引擎。InnoDB 对绝大多数用户来说都是最佳选择。","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"网络基础知识","slug":"网络基础知识","date":"2018-06-19T11:48:05.000Z","updated":"2018-08-09T00:57:28.121Z","comments":true,"path":"2018/06/19/网络基础知识/","link":"","permalink":"https://aaronshi32.github.io/2018/06/19/网络基础知识/","excerpt":"","text":"详解 HTTP 关键字: TCP/IP 四层模型TCP/IP 协议族是指通常与网络相关联的协议集合, 分为四层: 应用层(HTTP, DNS, FTP, SSH, SMTP), 传输层(TCP, UDP), 网络层(IP, ARP), 数据层(操作系统、网卡、驱动、光纤等) HTTP 数据 -&gt; TCP 首部 (HTTP 数据) -&gt; IP 首部 (TCP 首部 (HTTP 数据)) -&gt; 以太网首部 (IP 首部 (TCP 首部 (HTTP 数据))) 我们把这种数据信息包装的做法称之为 封装(encapsulate) ARP 是一种解析地址的协议, 根据 IP 查出 MAC 地址 TCP 三次握手(SYN - SYN/ACK - ACK) 后建立连接, 进行 HTTP 数据通信, 在四次挥手后断开连接(FIN - ACK - FIN - ACK) URL 是 URI 的子集 关键字: HTTP 协议HTTP 是不保存状态的协议(stateless), Session, Cookie 技术解决于此 HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范, 规范把 HTTP 请求分为三个部分：状态行、请求头、消息主体 支持的 Method: GET/POST/PUT/DELETE/HEAD/OPTIONS/TRACE/CONNECT 查询服务端支持的方法: OPTIONS * HTTP/1.1 每次访问都要建立 TCP 的四次握手, 因此 keep-alive 可以持久化连接, 期间进行数据通信 管道(pipelining) 是将多个 HTTP 请求整批提交的技术(请求1 -&gt; 请求2 -&gt; 请求3 -&gt; 响应1 -&gt; 响应2 -&gt; 响应3), 仅 HTTP/1.1 支持此技术, 只有 GET 和 HEAD 请求可以进行管线化 关键字: HTTP 报文HTTP 报文分为: 报文首部 + CRLF + 报文主体, 其中请求报文的主体一般为空 提升传输速率的方式有: 压缩 和 分块传输 关键字: HTTP 状态码1XX: 信息性状态码(Informactional), 接收的请求正在处理 2XX: 成功状态码(Success), 请求正常处理完毕 3XX: 重定向状态码(Redirection), 需要进行附加操作以完成请求 4XX: 客户端错误状态码(Client Error), 服务器无法处理请求 5XX: 服务器错误状态码(Server Error), 服务器处理请求出错 关键字: HTTP 首部Content-Type: 标识报文主体的对象类型 Cache-Control: 缓存控制, 强刷是 no-cache Connection: keep-alive or close 持久化连接 Upgrade: 用于检测 HTTP 协议及其他协议是否可使用更高的版本进行通信 Accept: text/html, text/plain, text/css, application/xhtml+xml, application/xml, image/jpeg, image/gif, image/png, video/mpeg, video/quicktime, application/octet-stream, application/zip 关键字: HTTPSHTTPS = 加密 + 认证 + 完整性保护 + HTTP HTTPS 并非是应用层的一种新协议, 只是 HTTP 通信接口部分用 SSL(Secure Socket Layer) 和 TLS(Transport Layer Security) 协议代替而已, HTTP - &gt; SSL -&gt; TCP = HTTPS SSL 是广泛使用的网络安全技术, 核心就是公开密钥加密的加密处理方式 HTTPS 通信步骤: 1-9 步建立 SSL, 10 - 11 进行数据传输, 12 断开 HTTPS 慢, 认证需要证书, 贵 关键字: HTTP 认证四种方式: BASIC, DIGEST, SSL客户端认证, FormBase 基于表单认证 关键字: 追加协议SPDY: 多路复用流, 赋予请求优先级, 压缩 HTTP 首部, 推送功能, 服务器提示功能 WekSocket: 使用浏览器全双工通信 ws:// WebDAV: 服务器管理文件 HTTP/2.0 关键字: 攻击XSS: 被动攻击, HTML内置好恶意代码, 等待用户访问触发 CSRF: 被动攻击, 利用已完成认证的用户(Cookie)进行非预期的操作 详解 TCP/IP关键字: OSI 参考模型应用层: 针对特定应用的协议 表示层: 设备固有数据格式和网络标准数据格式的转换 会话层: 通信管理, 负责建立和断开通信连接 传输层: 管理两个节点之间的数据传输, 负责可靠传输 网络层: 地址管理与路由选择 数据层: 互连设备之间传送和识别数据帧 物理层: 物理信号(01高低电压)传输 关键字: TCP/IP 协议群包含了: 应用协议(HTTP, SMTP, FTP, TELNET, SNMP), 传输协议(TCP, UDP), 网际协议(IP, ICMP, ARP), 路由控制协议(RIP, OSPF, BGP) RFC 全称为 Request For Comment: 征求意见文档, 目前在互联网上有各种 RFC 的文档标识声明 UDP 应用于 广播, 多播, 单播, 任播等通信和多媒体领域 FTP 进行文件传输时会建立两个 TCP 连接, 分别是发出传输请求时所要用到的控制连接和实际传输数据所要用到的数据连接 关键字: 数据链路网络拓扑: 总线型, 环型, 星型, 网状型 半双工与全双工通信: 只发送或者只接收的通信方式称为半双工, 同时发送接收的称为全双工 无线通信种类: RFID, 蓝牙(PAN), Wi-Fi(LAN), WiMAX(MAN), 通信网络(WAN) PPP 指 1对1 连接计算机的协议, 点对点 关键字: IP 协议IP(Internet Protocol): IP 寻址、路由、IP 分包与组包 三大作用模块, IP 为了实现简单化与高速化采用面向无连接的方式 IP 地址: 32位正整数, 最多允许 43 亿台计算机连接到网络, 每个网卡(NIC)都会配置一个地址, 一台路由器至少两个地址 网络标识表明网段, 同一网段内的网络标识必须相同, 主机标识表明主机号, 同一网段内不允许重复 192.168.128.11/24 24 表示从头数到第几位为止属于网络标识, 11 是主机标识, 路由器根据网络标识转发包 根据网络标识位数不同而分类: A类(8位, 首位0), B类(16位, 首位10), C类(24位, 首位110), D类(32位: 多播) 主机标识全部为1, 就成了广播地址 子网掩码: 实际上就是将原来 A 类, B类, C类等分类中的主机地址部分用作子网地址, 可将原网络分为多个物理网络 32位, 对应 IP 地址网络标识部分全部为 1, 对应 IP 地址主机标识部分全部为 0 路由: netstat -rn IPv6 地址: 128位正整数 关键字: DNS, ICMP, ARP, NATDNS: 如同互联网中的分布式数据库, 主机名与 IP 地址的对应信息叫做 A 记录, 反之, 从 IP 地址检索主机名称的信息叫做 PTRCNAME: 主机别名对应的规范名称, A: 主机名的IPv4地址, AAAA: 主机名的IPv6地址 APR: 用于IP 解析为 MAC 地址的协议, RARP: MAC 地址定位 IP 的协议 ICMP: 用于确认 IP 包是否成功送达目标地址, 通知在发送过程中 IP 包被废弃的具体原因, 改善网络设置等 DHCP: 用于分配 IP 地址, 即插即用 NAT: 用于在本地网络中使用私有地址, 在连接互联网时转而使用全局 IP 地址的技术 关键字: TCP, UDP通信中通常采用 5 个信息来识别一个通信, 他们是: TCP 首部（源 IP 地址, 目标 IP 地址, 协议号）, IP 首部（源端口号, 目标端口号）, 只要其中某一项不同, 则被认为是其他通信 端口号由其使用的传输层协议决定, ssh 22, https 443, ftp 21, dns 53, snmp 161 UDP 适用于 包总量较少的通信(DNS, SNMP等), 视频、音频等多媒体通信(即时通信), 限定于 LAN 等特定网络中的应用通信, 广播通信 TCP 确认应答和窗口控制 TCP 通过肯定的确认应答(ACK) 实现可靠的数据传输 TCP 通过窗口提高传输速度, 窗口大小是指无需等待确认应答而可以继续发送数据的最大值 因此, 在窗口比较大, 又出现报文段丢失的情况下, 同一个需要的确认应答将会被重复不断的返回, 而发送端主机如果连续3次收到同一个确认应答, 就会将其所对应的数据进行重发, 这种机制比之前提到的超时管理更加高效 拥塞控制及慢启动 简单的说，就是TCP传输过程中，为了避免一下子将网络冲爆，引入的机制。而慢启动，顾名思义，一开始慢慢传，发现没有问题，再增加传输速度。而一旦发现传输有超时，协议会认为网络拥堵，又降低传输速度。起始的传输速度，就是由初始拥塞窗口，initial congestion window，简称initcwnd参数控制的 关键字: 路由协议路由算法: 距离向量算法, 链路状态算法 主要路由协议: RIP(base on UDP), RIP2(base on UDP), OSPFbase on IP), EGP(base on IP), BGP(base on TCP)","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"面试基础问题","slug":"面试基础问题","date":"2018-05-23T10:47:41.000Z","updated":"2018-10-26T07:54:00.455Z","comments":true,"path":"2018/05/23/面试基础问题/","link":"","permalink":"https://aaronshi32.github.io/2018/05/23/面试基础问题/","excerpt":"","text":"数据库 查询表中重复字段 SELECT Distinct(email) FROM Person A WHERE EXISTS(SELECT email FROM Person B WHERE A.email = B.email HAVING count(*)&gt;=2) 索引原理, SQL执行策略 Mysql目前主要有以下几种索引类型：FULLTEXT, HASH, BTREE, RTREE 索引我们分为四类来讲 单列索引(普通索引，唯一索引，主键索引)、组合索引、全文索引、空间索引 最左前缀原则: 在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边, 比如复合索引(a, b, c)会作用于包含 a,b,c/a,b/a 的 where 语句, 因此 a 位置应该是查询最频繁的列 where 执行顺序是从左往右执行的，在数据量小的时候不用考虑，但数据量多的时候要考虑条件的先后顺序，此时应遵守一个原则：排除越多的条件放在第一个 from 语句多表做笛卡尔乘积, 从右往左, 小数量的表放在最右边 如何成功避开索引: like语句，‘%w’不会使用索引，‘w%’会使用索引 列类型为字符串类型，查询时没有用单引号引起来 在where查询语句中使用表达式 在where查询语句中对字段进行NULL值判断 在where查询中使用了or关键字, myisam表能用到索引， innodb不行;(用UNION替换OR,可以使用索引) where中复合索引未按顺序查询的 事务的实现 START TRANSACTION …. COMMIT …. ROLLBACK ACID, 其中隔离性通过 锁 实现, 一致性通过 undo log 实现, 原子性和持久性通过 redo log 来实现 redo(重做) 和 undo(回滚) 比较: 都是恢复操作: redo: 恢复提交事务修改的页操作/ undo: 回滚行记录到某个特定版本 记录内容不同: redo: 是物理日志，记录的是物理的修改操作/ undo: 是逻辑日志，根据每行记录进行记录 读取方式不同: redo : 在数据库运行时，不需要读取操作(注：数据库恢复时，才用redo)/ 在数据库运行时，需要随机读取(注：回滚时用) 两种隔离级别: READ COMMIT、REPEATABLE READ A, B 账户各有 1000 元, 当 A 账户的某一个事务修改为 800 元并提交, B 账户的一个事务在提交前后读取到两个不同的 A 账户金额, 即: 同一个事务中查询结果未能保持一致 未解决上述问题, 使得当前事务每次读取的结果集都相同，而不管其他事务有没有提交, 但出现幻读的情形引出 MVCC 多版本并发控制: MVCC 是通过保存数据在某个时间点的快照来实现的 InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID. MySQL 锁: 读锁：也叫共享锁、S锁，若事务T对数据对象A加上S锁，则事务T可以读A但不能修改A，其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S 锁。这保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。 写锁：又称排他锁、X锁。若事务T对数据对象A加上X锁，事务T可以读A也可以修改A，其他事务不能再对A加任何锁，直到T释放A上的锁。这保证了其他事务在T释放A上的锁之前不能再读取和修改A。 表锁：操作对象是数据表。Mysql大多数锁策略都支持(常见mysql innodb)，是系统开销最低但并发性最低的一个锁策略。事务t对整个表加读锁，则其他事务可读不可写，若加写锁，则其他事务增删改都不行。 行级锁：操作对象是数据表中的一行。是MVCC技术用的比较多的，但在MYISAM用不了，行级锁用mysql的储存引擎实现而不是mysql服务器。但行级锁对系统开销较大，处理高并发较好。 优化点 为什么不使用 select * 不需要的字段会增加数据传输的时间，即使mysql服务器和客户端是在同一台机器上，使用的协议还是tcp，通信也需要额外的时间。 select 可能会获取到自己不需要的列，如果以后表结构修改了，同样也可能会对代码产生影响。比如表增加了一个字段，而我代码与其对接的对象属性里没有这个字段，select 就会导致报错 drop, truncate 和 delete drop 删除表数据和定义, truncate 删除表数据, 不可 rollback, 使用的系统和事务日志资源少, delete 删除表数据, 按行删, 可接 where 语句, 可 rollback binlog 记录数据库增删改, 不记录查询的二进制日志, 用于数据恢复 语法 集合类的实现 TreeMap 红黑树, 默认按 key 升序遍历 HashMap threshold(最大容量) = len(初始化容量) * load factor(负载因子) LinkedHashMap 双向链表加HashMap, accessOrder 为 false 维持 Key 的插入顺序遍历, 为 true 时可实现 LRU 网络HTTP1.1的Pipeling方式本质是将请求串行化处理，后面的请求必须等待前面请求的返回才能被执行，一旦有某请求超时等，后续请求只能被阻塞，即线头阻塞； HTTP/2多个请求可同时在一个连接上并行执行。某个请求任务耗时严重，不会影响到其它连接的正常执行； 数据结构树 BST 二叉排序树, O(logN) 操作复杂度, 但是依赖于输入的建树顺序, 容易退化成 O(N) AVL 平衡二叉排序树, 左右子树高度差小于1, 确保复杂度为O(logN) B树家族, 多路搜索树, 为什么B+树适合索引: 数据存在叶节点, 每次检索都是到根到叶节点的过程, 其他节点存储的数据就是索引 关键字的数量不同；B+树中分支结点有m个关键字，其叶子结点也有m个，其关键字只是起到了一个索引的作用，但是B树虽然也有m个子结点，但是其只拥有 m-1 个关键字。 存储的位置不同；B+ 树中的数据都存储在叶子结点上，也就是其所有叶子结点的数据组合起来就是完整的数据，但是B树的数据存储在每一个结点中，并不仅仅存储在叶子结点上。 分支结点的构造不同；B+ 树的分支结点仅仅存储着关键字信息和儿子的指针（这里的指针指的是磁盘块的偏移量），也就是说内部结点仅仅包含着索引信息。 查询不同；B树在找到具体的数值以后，则结束，而B+树则需要通过索引找到叶子结点中的数据才结束，也就是说B+树的搜索过程中走了一条从根结点到叶子结点的路径。 BRT 红黑树, 自平衡二叉搜索树, 变色/左旋/右旋, 插入、删除、查找都是 O(logN) 框架Bean 生命周期(模板答案）: 通过工厂创建/通过上下文创建 实例化一个 Bean(也就是 new 操作) 设置 Bean 的属性值(也就是 IOC 注入, 从 xml 中读取, 调用 set 接口设置) 如果 Bean 实现了 BeanNameAware 接口，会调用它实现的 setBeanName(String) 方法(也就是设置 Bean 的名字) 如果 Bean 实现了 BeanFactoryAware 接口，会调用它实现的 setBeanFactory (设置 Bean 的创建工厂） (上下文独有) 如果 Bean 实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法 (设置上下文) 如果 Bean 关联了 BeanPostProcessor 接口，将会调用 postProcessBeforeInitialization(Object obj, String s)方法(修改 Bean 内容, 还有 afterPropertiesSet 等等) 如果 Bean 在配置文件中配置了 init-method 属性会自动调用其配置的初始化方法(调用 init-method 初始化方法) 如果 Bean 关联了 BeanPostProcessor 接口，将会调用 postProcessAfterInitialization(Object obj, String s)方法 如果 Bean 实现了 DisposableBean 接口，会调用那个其实现的destroy()方法； 如果 Bean 配置了destroy-method 属性，会自动调用其配置的销毁方法(调用 destroy-method 销毁方法) Bean 单例实现 发生在 AbstractBeanFactory 的 getBean -&gt; doGetBean -&gt; getSingleton 进行bean的创建, 双重判断加锁的单例模式 https://www.cnblogs.com/zhaoyan001/p/6365064.html 静态常量, 静态代码块线程安全, 但不是 lazy-init getInstance 加锁, 单锁, 线程不安全 double-check 加锁, 线程安全 推荐静态内部类, 调用内部类时候才会创建 123456789101112public class Singleton &#123; private Singleton() &#123;&#125; private static class SingletonInstance &#123; private static final Singleton INSTANCE = new Singleton(); &#125; public static Singleton getInstance() &#123; return SingletonInstance.INSTANCE; &#125;&#125; Spring 事务 spring支持编程式事务管理和声明式事务管理两种方式。 编程式事务管理使用TransactionTemplate或者直接使用底层的PlatformTransactionManager。对于编程式事务管理，spring推荐使用TransactionTemplate。 声明式事务管理建立在AOP之上的。其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。声明式事务最大的优点就是不需要通过编程的方式管理事务，这样就不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中做相关的事务规则声明(或通过基于@Transactional注解的方式)，便可以将事务规则应用到业务逻辑中 异常处理 Service 层处理事务回滚(@Transactional), Controller 处理业务异常(通常自己封装Runtime) 然后用 @ControllerAdvice + @ExceptionHandler 进行友好展示 AOP的两种实现 SpringAOP 是 Spring 这个庞大的集成框架为了集成 AspectJ 而出现的一个模块 从实现上来讲 SpringAOP 是 基于代理实现的(Proxying), 可选两种方式 JDK动态代理(Dynamic Proxy) 基于标准JDK的动态代理功能 只针对实现了接口的业务对象 CGLIB 通过动态地对目标对象进行子类化来实现AOP代理，上面截图中的SampleBean$$EnhancerByCGLIB$$1767dd4b即为动态创建的一个子类 需要指定@EnableAspectJAutoProxy(proxyTargetClass = true)来强制使用 当业务对象没有实现任何接口的时候默认会选择CGLIB AspectJ 是 基于字节码操作(Bytecode Manipulation) @Around -&gt; @Before -&gt; @After -&gt; @AfterReturning 其中 Around 参数为 ProceedingJoinPoint, 需要调用 proceedingJoinPoint.proceed() 才可调用方法 I/O 与 进程进程有内核态和用户态两种运行方式, 用户态可以使用 CPU 和内存来完成一些任务, 内核态可以对硬件外设进行操作(读取磁盘文件、发送数据网络) I/O 模型 服务器端编程经常需要构造高性能的IO模型，常见的IO模型有四种： 同步阻塞IO（Blocking IO）：即传统的IO模型。 同步非阻塞IO（Non-blocking IO）：默认创建的socket都是阻塞的，非阻塞IO要求socket被设置为NONBLOCK。注意这里所说的NIO并非Java的NIO（New IO）库。 多路复用IO（IO Multiplexing）：即经典的Reactor设计模式，有时也称为异步阻塞IO，Java中的Selector和Linux中的epoll都是这种模型。 异步IO（Asynchronous IO）：即经典的Proactor设计模式，也称为异步非阻塞IO。 同步和异步的区别在于多个任务和事件发生时，一个事件的发生或执行是否会导致整个流程的暂时等待 阻塞和非阻塞的区别在于当发出请求一个操作时，如果条件不满足，是会一直等待还是返回一个标志信息。 缓存Redis与Memcached的区别与比较 Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中 Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的IO复用模型。 Tair MDB: 适用容量小（一般在M级别，50G之内), 读写QPS高（万级别）的缓存场景, 是内存型产品 RDB: 可持久化, Redis LDB: 适用于确实有持久化需求，读写QPS较高（万级别）的应用场景, 目前使用的 SSD 硬盘, 基于开源的 LevelDB 引擎 PDB: 即将上线 FASTDUMP: ODPS 快速写入到分布式缓存Tair的解决方案 MDB: key 最大 1K, value 最大 1M, 是典型 slab 的最大限制(内存分配机制) MDB 架构: client, 两台 ConfigServer(互为主备, 路由), 多个 DataServer(存储引擎) 两台 Configserver 互为主备。通过和 DataSrver 之间的心跳检测获取集群中存活可用的 DataServer，构建数据在集群中的分布信息（对照表）。 Dataserver 负责数据的存储，并按照 Configserver 的指示完成数据的复制和迁移工作。Client 在启动的时候，从 Configserver 获取数据分布信息，根据数据分布信息，和相应的 Dataserver 进行交互，完成用户的请求。 ConfigServer 的考点: 一致性 Hash 相邻的虚拟节点可能是多台不同的物理机，这样可以分散压力。无虚拟节点的话，只是简单的把压力转给另一台物理机 数据结构","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"Linux 常用命令","slug":"Linux","date":"2018-05-08T08:49:05.000Z","updated":"2018-05-09T01:39:37.464Z","comments":true,"path":"2018/05/08/Linux/","link":"","permalink":"https://aaronshi32.github.io/2018/05/08/Linux/","excerpt":"","text":"系统信息类1cat /proc/meminfo | grep MemTotal //查看系统内存信息 系统日志类系统日志一般都存在/var/log下 123cat /etc/syslog.conf //查看系统日志配置位置dmesg //查看kernel信息","categories":[],"tags":[{"name":"系统","slug":"系统","permalink":"https://aaronshi32.github.io/tags/系统/"}],"keywords":[]},{"title":"读书笔记-深入分析 Java Web 技术内幕（三）","slug":"读书笔记-深入分析 Java Web 技术内幕（三）","date":"2018-04-28T02:15:26.000Z","updated":"2018-05-14T02:05:48.977Z","comments":true,"path":"2018/04/28/读书笔记-深入分析 Java Web 技术内幕（三）/","link":"","permalink":"https://aaronshi32.github.io/2018/04/28/读书笔记-深入分析 Java Web 技术内幕（三）/","excerpt":"","text":"第三部分: Java 服务端技术1. Servlet 工作原理解析Servlet 是 Java Web 的核心技术, Servlet 容器是一个独立发展的标准化产品: Jetty, Tomcat, JBoss 等 以 Tomcat 为例, Context 容器负责管理 Servlet 容器(隶属于组件 Container), 一个 Context 对应一个 Web 工程 1234567Context ctx = new StandardContext();...ctx.addLifecycleListener(new DefaultWebXmlListener())...ContextConfig ctxCfg = new ContextConfig();ctxCfg.setDefaultWebXml(\"org/apache/catalin/startup/NO_DEFAULT_XML\");... Tomcat 的启动逻辑是基于观察者模式设计的, 所有的容器都会继承 Lifecycle 接口, 所有容器的修改和状态变更会有由它去通知已经注册的观察者(Listener) Tomcat 的启动创建了 Context 容器, 在 Context 容器中, Web 应用得以初始化, 而这个初始化的过程中, web.xml 决定了 Servlet 对象的创建和初始化 123456789&lt;servlet&gt; &lt;servlet-name&gt;yunos-mdm&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:conf/spring/mvc-config.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; // 如果大于 0, 那么在 Context 容器启动时就会被实例化&lt;/servlet&gt; Servlet 的体系结构围绕着: ServletConfig, ServletRequest, ServletResponse, ServletContext(上下文) 四个类 Servlet 最终的工作原理是靠映射完成的, 这个类就是 org.apache.tomcat.util.http.mapper Listener 是基于观察者设计模式的, Filter 是基于责任链设计模式的 2. 深入理解 Session 与 Cookie当前 Cookie 有两个版本(0,1), 不同的浏览器支持 Cookie 的数量和大小都不同, Cookie 被加到 header 中发生在构建响应 Response 中, 基于这些限制, Session 则创建在服务端, 以一个 id 作为标识, 客户端使用这个标识可通过 URL Path Paramter, Cookie 中的一个字段来实现 服务器管理 Session 的容器: org.apache.cataline.Manager, 它来负责接管过期回收, 服务器关闭序列化到磁盘(SESSIONS.ser)等问题 Session 的安全性优于 Cookie, 适合存储用户隐私和重要的数据, 引出分布式 Session 框架的核心解决要点 订阅服务器(如 Zookeeper) 负责 Cookie 项的统一配置推送 分布式缓存系统(如 Tair, Memcache) 负责 Session 的快速存储和读取, 而实现就是配置一个 SessionFilter 在创建 Session 之前进行拦截读存等操作 处理 Cookie 被盗取的情况: 当用户登录成功后根据用户的私密信息生成一个签名, 以表示当前这个唯一的合法登录状态, 即 Session 签名; 处理 Cookie 压缩的情况: 可配置一个 Filter 在页面输出是对 Cookie 进行全部的压缩或者部分压缩, 使用 gzip 和 defalte 算法, 节省带宽; 处理表单重复提交的问题: crsf 的 hidden token 验证 3. Tomcat 的系统架构与设计模式Tomcat 核心架构: Server - 若干个 Service - N 个 Connector 和 1 个 Container, 整个 Tomcat 生命周期由 Server 控制 1234// conf/server.xmlService 是 Catalina// 其中 Context 是在 context.xml 中Container 是 Engine - Host - Context - Wrapper 实现上来讲 StandServer、StandService 配合 LifeCycle 接口实现全程事件监听和触发控制 Connector 组件是 Tomcat 中的两个核心组件之一, 它的主要任务是负责接收浏览器发过来的 TCP 的连接请求, 创建一个 Request 和 Response 对象分别用于和请求端交换数据, 然后会产生一个线程来处理这个请求, 而处理这个请求的线程就是 Container 组件要做的事情了 123&lt;Connector port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"15000\" redirectPort=\"8443\" maxParameterCount=\"1000\" maxThreads=\"250\" maxPostSize=\"2097152\" acceptCount=\"200\" useBodyEncodingForURI=\"true\" /&gt; Tomcat 使用的设计模式有 外观模式(门面模式): Request 和 Response 对象封装、StandardWrapper 到 ServletConfig 封装、ApplicationContext 到 ServletContext 封装, 一句话, Facade 类控制暴露的数据 观察者模式: LifeCycle 接口接管 Tomcat 生命周期 命令模式: Connector 是通过命令模式调用 Container 的 责任链模式: Container 是通过责任链模式一步步传递给 Servlet 的 4. Jetty 的工作原理解析Jetty 应该是目前最活跃也很有前景的一个 Servlet 引擎, 任何被扩展的组件都可以作为一个 Handler 添加到 Server 中, Jetty 将帮你管理这些 Handler 整个 Jetty 核心是由 Server 和 Connector 两个组件构成, Server 组件是基于 Handler 容器工作的, 关联了 Connector, 生命周期也是观察者设计模式(LifeCycle) Jetty 可以基于两种协议工作, 一种是 Http(作为提供独立的 Web 服务), 另一种是 AJP(集成到 JBoss 和 Apache 中), Connector 的工作方式默认都是 NIO, 但支持 BIO apr(Apache Portable Runtime/Apache可移植运行时库)，Tomcat将以JNI的形式调用Apache HTTP服务器的核心动态链接库来处理文件读取或网络传输操作, 从而大大地提高Tomcat对静态文件的处理性能。从操作系统级别来解决异步的IO问题, 大幅度的提高性能。 Tomcat apr也是在Tomcat上运行高并发应用的首选模式 与 Tomcat 的比较 Jetty 的架构比 Tomcat 简单, 何被扩展的组件都可以作为一个 Handler 添加到 Server 中, Jetty 将以责任链的方式来管理这些 Handler 性能方面 Jetty 默认 NIO, Tomcat 默认 BIO, 前者适用于大量长连接的场景(聊天软件), 后者适用于短连接的场景 最新的 Servlet 特性 Jetty 的支持比 Tomcat 早许多, 这也是架构导致的原因 5. Spring 框架的设计理念与设计模式分析Spring 的三大核心组件: Core, Context, Bean, Bean 包装的是 Object, Context 来维护 Bean 的关系集合(IoC 容器, 从 Context 中获得各种 Bean), Core 用来发现、建立和维护每个 Bean 之间的关系所需要的一系列工具 Bean 组件在 Spring 的 org.springframework.beans 包下, 主要解决: Bean 的定义、创建、及对 Bean 的解析 Bean 的创建是典型的工厂模式, 主要由三个子类: ListenBeanFactory, HierachicalBeanFactory, AutowireCapableBeanFactory, Bean 的解析主要就是对 Spring 配置文件的解析 Context 组件在 Spring 的 org.springframework.context 包下, 作为 Spring 的 IoC 容器, 完成了标识应用环境、利用 BeanFactory 创建 Bean 对象, 保存对象关系表, 能够捕获各种事件的工作 Core 组件把所有资源都抽象成了一个接口的方式来进行访问(ResourceLoader) IoC 容器实际上是 Context 组件结合其他两个组件共同构建了一个 Bean 的关系网, 在这个构建过程中, BeanFactoryPostProcessor 和 BeanPostProcessor 分别在构建 BeanFactory 和 构建 Bean 对象时调用, InitializingBean 和 DisposableBean 分别在 Bean 的实例创建和销毁时调用 AOP 是基于动态代理实现的, 拦截器就是其中的一个特性 6. Spring MVC 的工作机制与设计模式要使用 SpringMVC, 只需要在 web.xml 中配置一个 DispatchcerServlet 用于路径映射, 定义一个 ViewResolver 用于视图解析器, 再定义一个业务逻辑的处理流程规则 DispatcherServlet 初始化调用了 initStrategies 方法 12345678910protected void initStrategies(ApplicationContext context)&#123; initMultipartResolver(context); // 用于处理文件上传服务 initLocaleResolver(context); // 国际化问题 initThemeResolver(context); // 主题 initHandlerMappings(context); // 映射 initHandlerAdapters(context); // 规则 initHandlerExceptionResolvers(context); // 异常处理 initRequestToViewNamerTranslator(context); // View 前缀后缀 initViewResolvers(context); // View 解析成页面&#125; 7. 深入分析 iBatis 框架之系统架构与映射原理ORM 框架干的事情: 1. 根据 JDBC 规范建立与数据库的连接 2. 通过反射打通 Java 对象与数据库参数交互之间相互转化的关系 做的事情如下, iBatis 就是把这几行代码分解包装 123456789Class.forName(\"xxx.xxx.xxx.xxx.Driver\");Connection conn = DriverManager.getConnection(url, user, password);PreparedStatement st = conn.prepareStatement(sql);st.setInt(0, 1);st.execute();ResultSet rs = st.getResultSet();while(rs.next())&#123; String result = rs.getString(colname);&#125;","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-深入分析 Java Web 技术内幕（二）","slug":"读书笔记-深入分析 Java Web 技术内幕（二）","date":"2018-04-23T08:15:26.000Z","updated":"2018-05-02T10:24:01.338Z","comments":true,"path":"2018/04/23/读书笔记-深入分析 Java Web 技术内幕（二）/","link":"","permalink":"https://aaronshi32.github.io/2018/04/23/读书笔记-深入分析 Java Web 技术内幕（二）/","excerpt":"","text":"第二部分: Java 基础知识1. Javac 编译原理Java 源代码经历词法分析器、语法分析器、语义分析，代码生成最终转换为字节码 词法分析器: Scanner(扫描全文件)、Lexer(识别符合 Java 语法规范的 Token 序列) 语法分析器: AST(抽象语法树) 语义分析器: 让 AST 的信息更加完善, 包含了: 映射符号表、处理注解、处理标注(@Unchecked), 检查变量合法性、数据流分析(还是各种语法检查)、语义分析(优化代码逻辑, 消除无用代码, 解除语法糖) 代码生成器: 遍历语法树, 生成字节码, 写入 class 文件 以上 4 个环节都会对语法树进行不同额处理操作, 采用的设计模式是访问者模式 想要操作 Java 源码: 业内 Eclipse JDT 2. 深入 class 文件结构有两种翻译 class 的工具: Oolong 是 JVM 的汇编语言, 使用方式: java COM.sootNsmoke.oolong.Gnoloo Message.class 生成 Message.j 文件 通过 JDK 自带的 javap, 使用方式: javap -verbose Message &gt; message.txt 3. 深入分析 ClassLoader 工作机制 将 class 加载到 JVM 中 defineClass(byte[], int, int): 将字节流解析成 JVM 能够识别的 Class 对象 findClass(String): 类的加载规则实现于此 loadClass(String): 获取类的对象 resolveClass(Class&lt;?&gt;): 让 JVM 加载类的同时并链接 审查每个类应该由谁加载: 父优先的等级加载机制 Bootstrap ClassLoader: 主要加载 JVM 自身工作需要的类, 这个 ClassLoader 完全是由 JVM 控制, 没有子类, 参数: -Xbootclasspath: ExtClassLoader: 用于加载目标为 System.getProperty(“java.ext.dirs”) 的类, 是 AppClassLoader 的父类, 参数:-Djava.ext.dirs AppClassLoader: 用于加载 classpath 的类, 也是所有自实现的类加载器的父类, 参数: -Djava.class.path= 重解析: 将字节码重新解析成统一要求的对象格式 字节码验证, Class类数据结构分析及、相应的内存分配、最后的符号表的链接 类中的静态属性和初始化赋值, 静态代码块的执行 常见的类加载错误分析 ClassNotFoundException 检查当前 classpath 目录下有没有指定的文件存在, 用 this.getClass().getClassLoader().getResource(“”).toString() 获取 classpath 路径 NoClassDefFoundError 使用 new 关键字、属性引用某个类、继承了某个接口或类、方法的某个参数中引用了某个类时, 触发 JVM 隐式加载这些类发现不存在 Tomcat 的 ClassLoader 层级 ExtClassLoader -&gt; AppClassLoader -&gt; StandardClassLoader -&gt; WebappClassLoader 加载 Tomcat 容器本身仍然是 AppClassLoader, 在 WebAppClassLoader 中增加了缓存, 取代了加载时优先查找 JVM 的 findLoaderClass 缓存 类的热部署原理: 创建不同的 ClassLoader 实例对象, 然后通过这个不同的实例对象类加载同名的类 4. JVM 体系结构与工作方式CPU 是跟指令集挂钩的, 顾名思义就是计算机能识别的机器语言, 如 RISC, CISC, MMX 等, 来自于 Intel 和 AMD JVM 也有用一套指令集, 俗称 JVM 字节码指令集, 用来执行 class 的字节码 JVM 结构组成: 类加载器 执行引擎(负责执行 class 文件中包含的字节码指令, 相当于实际机器上的 CPU, 目标是内存区的栈) SUN 的 Hotspot 是基于栈的执行引擎 Google 的 Dalvik 是基于寄存器的执行引擎 内存区(内存管理: 方法区、Java 堆、栈、PC寄存器、本地方法区) 本地方法调用(调用 C/C++ 实现本地方法的代码返回结果) 在执行引擎技术里面衍生了 JIT: 在Java编程语言和环境中，即时编译器（JIT compiler，just-in-time compiler）是一个把 Java 的字节码（包括需要被解释的指令的程序）转换成可以直接发送给处理器的指令的程序 5. JVM 内存管理Java 涉及到需要分配的内存有: 堆(-Xmx 和 -Xms), 线程(为其分配个堆栈), 类和类加载器(存活于 PermGen 区), NIO(直接使用本地内存), JNI(加载实现的类库) 在 Java 虚拟机规范中将 Java 运行时数据划分为 6 种, 分别为: PC 寄存器, Java 栈(为线程分配的空间), 堆(线程共享), 方法区(存储的是类结构信息, 类加载器加载完就存放与此, 线程共享, 永久区中), 本地方法区(为 Native 方法准备的空间), 运行时常量池(存放每个 class 文件中的常量表, 隶属于方法区的一部分) 根可达算法决定了哪些对象是垃圾, 垃圾回收算法决定了怎么回收: 基于分代的GC(Young, Old, Perm 区依次递进) Serial Collector 算法 &amp; Parallel Collector 算法 &amp; CMS Collector 算法 GC 日志参数: -verbose:gc 可以辅助输出一些详细的 GC 信息 -XX:+PrintGCDetails 输出 GC 详细信息 -XX:+PrintGCApplicationStoppedTime 输出 GC 造成应用程序暂停的时间 -XX:+PrintGCDateStamps GC 发生的时间信息 -XX:+PrintHeapAtGC 在 GC 前后输出堆中各个区域的大小","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-深入分析 Java Web 技术内幕（一）","slug":"读书笔记-深入分析 Java Web 技术内幕（一）","date":"2018-04-19T02:33:12.000Z","updated":"2018-04-23T08:04:46.283Z","comments":true,"path":"2018/04/19/读书笔记-深入分析 Java Web 技术内幕（一）/","link":"","permalink":"https://aaronshi32.github.io/2018/04/19/读书笔记-深入分析 Java Web 技术内幕（一）/","excerpt":"","text":"第一部分: Java Web 开发中的基础知识1. 深入 Web 请求过程1.1 HTTP 散点知识 Apache, IIS, Nginx, Tomcat, JBoss 都是基于 HTTP 的服务器, HTTP 是应用层的协议, 采用无状态的短连接通信方式 在浏览器里输入 www.baidu.com 敲击回撤的瞬间, 完成了以下操作 浏览器请求 DNS 把域名解析成对应的 IP（域名到IP地址映射: DNS域名服务） 浏览器根据这个 IP 在互联网上找到对应服务器, 发起 GET 请求, 返回数据资源 请求 CDN 寻求静态资源 请求的数据会先到负载均衡, 缓存, 如需要最终才到数据库 HttpClient 工具包 和 curl 命令都能够发起 HTTP 请求, 浏览器正是做了这些工作, 实际上就是建立一个 Socket 通信的过程 要理解 HTTP, 最重要的就是要熟悉 HTTP 中的 HTTP Header, HTTP Header 控制着用户浏览器的渲染行为和服务器的执行逻辑 显而易见, HTTP Request Header 是客户端要告诉服务器的信息, 而 HTTP Response Header 是服务器返回要告诉客户端的信息 Ctrl + F5 的实现逻辑是: 在请求头中加入了 Cache-Control: no-cache 和 Pragma: no-cache, 用于指定所有缓存机制在整个请求/响应链中必须服从的指令 其他用于标识缓存过期的头字段有: Expires, Last-Modified, Etag 1.2 DNS 散点知识 DNS 将域名转化为 IP 的先后关键步骤: [本机完成] 1) 浏览器缓存 - 2) 操作系统hosts文件 [本机发起请求] 3) LDNS 即在操作系统中配置的 DNS 服务器地址 - 4) 直接请求 Root 域名服务器(全球就9台) [本机得到反馈结果] 5) 返回给 LDNS 一个主域名服务器地址 gTLD [本机发起请求] 6) LDNS 去请求 gTLD [本机得到反馈结果] 7) 返回给 LDNS 网站注册的域名服务器 - 8）请求这个注册的域名服务器得到 IP - 9） LDNS 存储这个映射缓存 - 10) 存到本机(浏览器) 跟踪域名解析结果: dig www.baidu.com [+trace] [+cmd] 、 nslookup 清除域名缓存结果: ipconfig /flushdns 、/etc/init.d/nscd restart 几种域名解析的方式主要分为 A记录、MX记录、CNAME记录、NS记录 和 TXT记录 A记录: 域名对应 IP, 多个域名可以对应一个 IP, 但反之不能 MX记录: 邮件服务器对应的 IP CNAME记录: 全称是 Canonical Name(别名解析), 一个域名可以设置多个别名 NS记录: 为某个域名指定 DNS 解析服务器, 也就是这个域名有指定的 IP 的 DNS 服务器去解析 TXT记录: 为某个主机名或域名设置说明类的文字 1.3 CDN 散点知识 形象的比喻: CDN = 镜像(Mirror) + 缓存(Cache) + 整体负载均衡(GSLB) 负载均衡的框架通常有三种: 链路负载均衡(DNS), 集群负载均衡(软硬件(LVS + HAProxy + F5)), 操作系统负载均衡(多队列网卡) CDN 的动态加速技术原理就是在 CDN 的 DNS 解析中通过动态的链路探测来寻找回源最好的一条路径, 从而加速用户访问的效率, 节省带宽 2. 深入分析 Java I/O 的工作机制2.1 Java I/O 类库的基础架构: java.io 基于字节操作的 I/O 接口: InputStream 和 OutputStream 基于字符操作的 I/O 接口: Writer 和 Reader 基于磁盘操作的 I/O 接口: File 基于网络操作的 I/O 接口: Socket 前两者是数据格式, 后两者是传输方式, 共同影响着 I/O 的效率 字符到字节必须经过编码转换, 这个过程相当耗时 在纯 Java 环境下, Java 序列化能够很好的工作, 但是在多语言环境下, 用 Java 序列化存储后, 很难用其他语言还原出结果。在这种情况下, 还是要尽量存储通用的数据机构, 如 JSON 或者 XML 结构数据, 当前也有比较好的序列化工具支持, 如 Google 的 protobuf 等 适配器和装饰器模式, 适配器是适配接口方便调用(InputStreamReader 适配了 InputStream 对象的 Reader 接口), 装饰器的增加接口功能提升效率(FileInputStream 装饰了 InputStream) 2.2 网络 I/O 的工作机制 影响网络传输的因素: 网络带宽, 传输距离, TCP 拥塞控制(为 TCP 设置一个缓冲区, 让传输方和接收方的步调一致) Socket 一般都基于 TCP/IP 的流套接字 NIO 是相对于 BIO: 阻塞 I/O 来的, BIO 会导致线程阻塞等待操作系统处理, 在大规模访问量时性能堪忧, 当然可以采取线程池的方法来缓解, 但线程池中线程的创建与回收也需要成本, 大量长连接的请求会一直占用线程池, 线程优先级难以掌控, 众多因素呼唤出了 NIO NIO 的两个关键词: Channel 和 Selector, Channel 好比传输方式, Selector 是这些传输方式的监控调度, Selector 是一个阻塞线程专门处理连接请求, 监控注册在其上面的 Channel 是否有数据传输发生, 一旦监控到了, 则让 Channel 进行相应的数据传输, 而 Channel 的数据传输发生在另一个非阻塞线程, 这样 Selector 不停的分发 I/O 链路让 Channel 进行通信, 换句话说: Selector 检测到通信信道 I/O 有数据传输时, 通过 select() 方法取得 SocketChannel, 将数据读取或者写入 Buffer 缓冲区 NIO 提供了两个优化的文件访问方法: FileChannel.transferTo / FileChannel.transferFrom 和 FileChannel.map 2.3 I/O 调优 磁盘 I/O 优化 增加缓存, 减少磁盘访问次数 优化磁盘的管理系统, 设计最优的磁盘方式策略, 以及磁盘的寻址策略(这是底层操作系统考虑的) 设计合理的磁盘存储数据块, 以及访问这些数据块的策略, 比如索引 应用合理的 RAID 策略提升磁盘 I/O 网络 I/O 优化 TCP 网络参数调优 1.1 cat /proc/net/netstat: 查看 TCP 的统计信息 1.2 cat /proc/net/snmp: 查看当前系统的连接情况 1.3 netstat -s: 查看网络的统计信息 1.4 ab -c 30 -n 100000 ip:port: 向 ip 并发发送 30 个请求共 100000 个 减少网络交互的次数: 两端设置缓存, 合并请求 减少网络传输数据量的大小: 压缩 尽量减少编码: 提前将字符转化成字节 交互方式的场景选择: 同步与异步/阻塞与非阻塞 3. 深入分析 Java Web 中的中文编码问题3.1 几种常见的编码 在计算机中存储信息的最小单元是 1 个字节, 即 8 个 bit, 所以能表示的字符范围是 0~255 个, 人类要表示的符号太多, 无法用 1 个字节来完全表示, 要解决这个矛盾必须要有一个新的数据结构 char, 而从 char 到 byte 则必须编码 存储空间与编码效率: ASCII 码, ISO-8859-1, GB2312, GBK(兼容前者, 表示的汉字更多) UTF-16, UTF-8, 其中 UTF-8 是最适合中文的编码方式(不用查码表, 计算可寻, 单字节1汉字节3, 相对于UTF16扩张一倍空间而言, UTF8省), 编码的发生场景常见于存储数据到磁盘或者数据要经过网络传输 不同的编码决定着最终存储的大小, 看的是字节数而不是字符数, 一个 int 用 4 个字节存储, 一个 char 用 2 个字节存储 一次 HTTP 请求在很多地方需要编码: URL 的编解码、HTTP Header 的编解码、POST 表单的编解码、HTTP BODY 的编解码 123456789101112&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-Redis 独立功能的实现","slug":"读书笔记-Redis 独立功能的实现","date":"2018-04-10T02:14:49.000Z","updated":"2018-04-11T01:16:17.794Z","comments":true,"path":"2018/04/10/读书笔记-Redis 独立功能的实现/","link":"","permalink":"https://aaronshi32.github.io/2018/04/10/读书笔记-Redis 独立功能的实现/","excerpt":"","text":"Redis：独立功能的实现1. 发布与订阅Redis 的发布与订阅功能由 PUBLISH、(UN)SUBSCRIBE、PSUBSCRIBE 等命令组成 12345678struct redisServer &#123; // 保存所有频道的订阅关系 dict *pubsub_channels; // 保存所有模式的订阅关系 list *pubsub_patterns;&#125; Redis 将所有频道的订阅关系都保存在服务器状态的 pubsub_channels 字典里面, 这个字典的键是某个被订阅的频道, 而键的值则是一个链表, 链表里面记录了所有订阅这个频道的客户端 服务器数据结构在 pubsub_channels 字典保存了所有频道的订阅关系: SUBSCRIBE 命令负责将客户端和被订阅的频道关联到这个字典里面, 而 UNSUBCRIBE 命令则负责解除客户端和被退订频道之间的关联 服务器数据结构在 pubsub_patterns 链表保存了所有模式的订阅关系: PSUBCRIBE 命令负责将客户端和被订阅的模式记录到这个链表中, 而 PUNSUBSCRIBE 命令则负责移除客户端和被退订模式之间的关联 PUBLISH 命令通过访问 pubsub_channels 字典来向频道的所有订阅者发送消息, 通过访问 pubsub_patterns 链表来向所有匹配频道的模式的订阅者发送消息 PUBSUB 命令的三个子命令都是读取通过 pubsub_channels 字典和 pubsub_patterns 链表中的信息来实现的 2. 事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务(transaction)功能 WATCH 命令是一个乐观锁, 它可以在 EXEC 命令执行之前, 监事任意数量的数据库键, 并在 EXEC 命令执行时, 检查被监视的键是否至少有一个已经被修改过了, 如果是的话, 服务器将拒绝执行事务, 并向客户端返回代表事务执行失败的空回复 事务在执行过程中不会被中断, 当事务队列中的所有命令都被执行完毕之后, 事务才会结束 带有 WATCH 命令的事务会将客户端和被监视的键在数据库的 watched_keys 字典中进行关联, 当键被修改时, 程序会将所有监视被修改键的客户端的 REDIS_DIRTY_CAS 标志打开 只有在客户端的 REDIS_DIRTY_CAS 标志未被打开时, 服务器才会执行客户端提交的事务, 否则将拒绝 Redis 的事务总是具有 ACID 中的原子性、一致性和隔离性, 当服务器运行在 AOF 持久化模式下, 并且 appendfsync 选项的值为 always 时, 事务也具有耐久性 3. 排序 SORT 命令通过将被排序键包含的元素载入到数组里面, 然后对数组进行排序来完成对键进行排序工作 在默认情况下, SORT 命令假设被排序键包含的都是数字值, 并且以数字值的方式来进行排序 如果 SORT 命令使用了 ALPHA 选项, 那么 SORT 命令假设被排序键包含的都是字符串值, 并且以字符串的方式来进行排序 SORT 命令的排序操作由快速排序算法实现 当 SORT 命令使用了 BY 选项时, 命令使用其他键的值作为权重来进行排序操作 除了 GET 选项之外, 调整选项的摆放位置不会影响 SORT 命令的排序结果 4. 慢查询日志Reids 的慢日志查询功能用于记录执行时间超过给定时长的命令请求, 用户可以通过这个功能产生的日志来监视和优化查询速度 Redis 服务器将所有的慢查询日志保存在服务器状态的 slowlog 链表中, 每个链表节点都包含一个 slowlogEntry 结构, 每个 slowlogEntry 结构代表一条慢查询日志 打印和删除慢查询日志可以通过遍历 slowlog 链表来完成 slowlog 链表的长度就是服务器所保存慢查询在日志的数量 新的慢查询日志会被添加到 slowlog 链表的表头, 如果日志的数量超过 slowlog-max-len 选项的值, 那么多出来的日志会被删除 5. 监视器 客户端可以通过执行 MONITOR 命令转换成监视器, 接收并打印服务器处理的每个命令请求信息 当一个客户端变为监视器时, 该客户端的 REDIS_MONITOR 标识会被打开 服务器将所有监视器都记录在 monitors 链表中 每次处理命令请求时, 服务器都会遍历 monitors 链表, 将相关信息发送给监视器","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-Redis 多机数据库的实现","slug":"读书笔记-Redis 多机数据库的实现","date":"2018-04-05T09:56:15.000Z","updated":"2018-04-10T02:21:18.187Z","comments":true,"path":"2018/04/05/读书笔记-Redis 多机数据库的实现/","link":"","permalink":"https://aaronshi32.github.io/2018/04/05/读书笔记-Redis 多机数据库的实现/","excerpt":"","text":"Redis：多机数据库的实现1. 复制1.1 旧版本复制功能的实现(2.8版本以前)复制功能分为 同步(sync) 和 命令传播(command propagate) 两个操作 SYNC 命令是一个非常耗费资源的操作, 生成 RDB 占据了大量的 CPU, 内存和磁盘 I/O 资源, 传输 RDB 占据了大量的网络资源, 接收并载入 RDB 会导致阻塞, 而旧版本的复制无法高效的完成: 断线重复制的情况, 即从服务器断线后的一致性保证, 哪怕只有几秒的短线, 只能通过 SYNC 来保证 1.2 新版本复制功能的实现(2.8版本以后)PSYNC 命令, 具备完整重同步(适用于初次复制的情况) 和 部分重同步(适用于断线重复制) 部分重同步功能由以下三个部分构成: 主服务器的复制偏移量(replication offset) 和 从服务器的复制偏移量 主服务器的复制积压缓冲区(replication backlog) 服务器的运行 ID(run ID) 通过对比主从服务器的复制偏移量, 程序可以很容易的指导主从服务器是否处于一致状态 复制积压缓冲区是一个固定长度的FIFO队列, 保存着主服务器的写命令, 如果它里面操作的复制偏移量小于主从不一致的偏移量, 则完整重同步, 反之大于, 则部分重同步, 这个复制积压缓冲区的大小可以配置, 约等于: service_reready_seconds write_size_per_second 服务平均恢复时长 每秒写命令产生的数据大小 服务器的运行ID是检测 从服务器前后是否连接的是相同的主服务器, 若是的话则部分重同步, 若不是只能完整重同步 在复制操作刚开始的时候, 从服务器会成为主服务器的客户端, 并通过向主服务器发送命令请求来执行复制操作, 而在复制操作的后期, 主从服务器会相互成为对方的客户端, 涵盖了: 设置主服务器的地址和端口/建立套接字连接/发送PING命令/身份验证/发送端口信息/同步/命令传播 等操作 主服务器通过向从服务器传播命令来更新从服务器的状态, 保持主从服务器一致, 而从服务器则通过向主服务器发送命令来进行心跳检测, 以及命令丢失检测(偏移量校验) 2. SentinelSentinel 是 Redis 高可用性的解决方案: 由一个或多个 Sentinel 实例组成的 Sentinel 系统, 监视着 Redis 的主从服务器, 当发现主服务器下线了, 会从从服务器中选择一个当主服务器, 并继续监控下线的主服务器, 一旦恢复上线, 会设置为新的主服务器的从服务器 启动一个 Sentinel 命令如下: 123redis-sentinel /path/to/your/sentinel.conforredis-server /path/to/your/sentinel.conf --sentinel 配置项: down-after-milliseconds 选项指定了 Sentinel 判断实例进入主观下线所需要的时间长度, 多个 Sentinel 监视同一个主服务器的标准是不同的 主观下线靠 PING 返回无效命令的累计时长, 客观下线靠询问其他 Sentinel 状态 选择 Sentinel 领头羊的算法为: Raft Sentinel 只是一个运行在特殊模式下的 Redis 服务器, 它使用了和普通模式不同的命令表, 所以 Sentinel 模式能够使用的命令和普通 Redis 服务器能够使用的命令不同 Sentinel 会读入用户指定的配置文件, 为每个要被监视的主服务器创建相应的实例结构, 并创建连向主服务器的命令连接和订阅连接, 其中命令连接用于向主服务器发送命令请求, 而订阅连接则用于接收指定频道的消息 Sentinel 通过向主服务器发送 INFO 命令来获得主服务器属下所有从服务器的地址信息, 并为这些从服务器创建相应的实例结构, 以及连向这些从服务器的命令连接和订阅连接 在一般情况下, Sentinel 以每十秒一次的频率向被监视的主服务器和从服务器发送 INFO 命令, 当主服务器处于下线状态, 或者 Sentinel 正在对主服务器进行故障转移操作时, Sentinel 向从服务器发送 INFO 命令的频率会改为每秒一次 对于监视同一个主服务器和从服务器的多个 Sentinel 来说, 它们会以每两秒一次的频率, 通过向被监视服务器的 sentinel:hello 频道发送消息来向其他 Sentinel 宣告自己的存在 每个 Sentinel 也会从 sentinel:hello 频道中接收其他 Sentinel 发来的信息, 并根据这些信息为其他 Sentinel 创建相应的实例结构, 以及命令连接 Sentinel 只会与主服务器和从服务器创建命令连接和订阅连接, Sentinel 之间只创建命令连接 Sentinel 以每秒一次的频率向实例(包括主服务器、从服务器、其他 Sentinel)发送 PING 命令, 并根据实例对 PING 命令的恢复来判断实例是否在线, 当一个实例在指定的时长中连续向 Sentinel 发送无效的回复时, Sentinel 会将这个实例判断为主观下线 当 Sentinel 将一个主服务器判断为主观下线时, 它会向同样监视这个主服务器的其他 Sentinel进行询问, 看他们是否同意这个主服务器已经进入主观下线状态 当 Sentinel 收集到足够多的主观下线投票时, 它会将主服务器判断为客观下线, 并发起一次针对主服务器的故障转移操作 3. 集群 节点通过握手来将其他节点添加到自己所处的集群当中 集群中的 16384 个槽截图分别指派给集群中的各个节点, 每个节点都会记录哪些槽指派给了自己, 而哪些槽又被指派给了其他节点 节点在接到一个命令请求时, 会先检查这个命令请求要处理的键所在的槽是否由自己负责, 如果不是的话, 节点将向客户端返回一个 MOVED 错误, MOVED 错误携带的信息可以指引客户端转向至正在负责相关槽的节点 对 Redis 集群的重新分片工作是由 redis-trib 负责执行的, 重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点 如果节点 A 正在迁移槽 i 至节点 B, 那么当节点 A 没能在自己的数据库中找到命令指定的数据库键时, 节点 A 回向客户端返回一个 ASK 错误, 指引客户端到节点 B 继续查找指定的数据库键 MOVED 错误表示槽的负责权已经从一个节点转移到了另一个节点, 而 ASK 错误只是两个节点在迁移槽的过程中使用的一种临时措施 集群里的从节点用于复制主节点, 并在主节点下线时, 代替主节点继续处理命令请求 集群中的节点通过发送和接收消息来进行通信, 常见的消息包括: MEET, PING, PONG, PUBLISH, FAIL 五种","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-Redis 单机数据库的实现","slug":"读书笔记-Redis 单机数据库的实现","date":"2018-04-02T05:55:51.000Z","updated":"2018-04-05T09:56:01.609Z","comments":true,"path":"2018/04/02/读书笔记-Redis 单机数据库的实现/","link":"","permalink":"https://aaronshi32.github.io/2018/04/02/读书笔记-Redis 单机数据库的实现/","excerpt":"","text":"Redis：单机数据库的实现1. 数据库通过 SELECT num 命令可以切换客户端使用的数据库, 即让客户端 db 指针指向服务端的某个 db 元素 1234567891011121314151617181920212223242526272829struct redisServer &#123; // 一个数组, 保存着服务器中的所有数据库 redisDb *db; // 服务器的数据库数量: 默认16个 int dbnum; // AOF 缓冲区: 记录写操作 sds aof_buf; // AOF 重写缓冲区: 记录在 AOF 重写机制期间的所有写操作, 用于完成数据一致性 sds aof_rewrite_buf; // 一个链表, 保存了所有客户端状态 list *clients;&#125;struct redisClient &#123; // 记录客户端当前正在使用的数据库 redisDb *db; // 输入缓冲区: 用于保存客户端发送的命令请求 sds querybuf; // 命令的实现函数 struct redisCommand *cmd;&#125; Redis 是一个键值对(key-value pair)数据库服务器, 服务器中的每个数据库都由一个 redisDb 结构表示 12345678struct redisDb &#123; // 数据库键空间, 保存着数据库中的所有键值对 dict *dict; // 过期字典, 保存着键的过期时间, EXPIRE 实际上是操作该键值 dict *expires;&#125; 主要由 dict 和 expires 两个字典构成, 其中 dict 字典负责保存键值对, 而 expires 字典则负责保存键的过期时间 dict 内容是 &lt; StringObject - 对象 &gt; 的键值对, 当使用 Redis 命令对数据库进行读写时, 服务器不仅会对键空间执行指定的读写操作, 还会执行一些额外的维护操作, 其中包括: 在读取一个键之后, 服务器会根据键是否存在来更新服务器的键空间命中(hit)次数或不命中(miss)次数, 这两个值可以在 INFO stats 命令的 keyspace_hits 属性和 keyspace_misses 属性中查看 在读取一个键之后, 服务器会更新键的 LRU 时间, 用于计算键的闲置时间, 使用 OBJECT idletime 命令可以查看 key 的闲置时间 如果服务器在读取一个键时发现该键已经过期, 那么服务器会先删除这个过期键, 然后才执行余下的其他操作 如果有客户端使用 WATCH 命令监视了某个键, 那么服务器在对被监视的键进行修改之后, 会将这个键标记为 dirty, 从而让事务程序注意到这个键已经被修改过 服务器每次修改一个件之后, 都会对脏键计数器的值增 1, 这个计数器会触发服务器的持久化以及复制操作 如果服务器开启了数据库通知功能, 那么在对键进行修改之后, 服务器将按配置发送相应的数据库通知 当键过期时, Redis 提供了三种删除策略 (主动)定时删除: 在设置键的过期时间的同时, 创建一个定时器(timer), 让定时器在键的过期时间来临时, 立即执行对键的删除操作, 省内存, 费CPU (Redis使用)(被动)惰性删除: 放任键过期不管, 当获取的时候, 过期就删除, 未过期就返回, 省CPU, 费内存(expireIfNeeded) (Redis使用)(主动)定期删除: 每隔一段时间, 程序就对数据库进行一次检查, 删除里面的过期键(算法), 折中(serverCron - activeExpireCycle) AOF 和 RDB 是如何删除过期键的 执行 SAVE 命令或者 BGSAVE 命令所产生的新 RDB 文件不会包含已经过期的键 执行 BGREWRITEAOF 命令所产生的重写 AOF 文件不会包含已经过期的键 当一个过期键被删除之后, 服务器会追加一条 DEL 命令到现有 AOF 文件的末尾, 显式的删除过期键 当主服务器删除一个过期键之后, 它会向所有从服务器发送一条 DEL 命令, 显式的删除过期键 从服务器即使发现过期键也不会自作主张的删除它, 而是等待主节点发来 DEL 命令, 从而保证主从服务器数据一致性 2. RDB 持久化RDB 持久化操作, 即执行 SAVE/BGSAVE 命令, 是由配置文件中的 save 选项生效的, 由周期性操作函数 serverCron 每个 100ms 检查是否满足持久化条件 1234// defaultsave 900 1 900秒内对数据库至少1次修改save 300 10 300秒内对数据库至少10次修改save 60 10000 60秒内对数据库至少10000次修改 使用 od 命令工具可以查看 RDB 内容 SAVE 命令由服务器进程直接执行保存操作, 会阻塞服务器 BGSAVE 命令由子进行执行保存操作, 不会阻塞服务器执行, 但是会拒绝再次服务器进行再次接收的 SAVE, BGSAVE, BGREWRITEAOF 命令防止竞争 服务器状态中会保存所有用 save 选项设置的保存条件, 当任意一个保存条件被满足时, 服务器会自动执行 BGSAVE 命令 RDB 是一个经过压缩的二进制文件, 有多个部分组成 对于不同类型的键值对, RDB 文件会使用不同的方式来保存它们 3. AOF 持久化 AOF 持久化是通过保存 Redis 服务器所执行的写命令(SET、SADD、RPUSH)来记录数据库状态的, 这个保存的过程是由 serverCron 函数中的 flushAppendOnlyFile 来完成的, 也是由配置文件中的 appendfsync 选项来设置的 随着服务器时间的运行, AOF 中记录的写操作越来越多, 文件体积也越来越大, 使用 AOF 文件来进行数据还原所需的时间也就越多, 因此 AOF 文件重写机制: 通过读取服务器当前的数据库状态, 然后用一条命令去记录键值对, 代替之前记录这个键值对的多条命令 AOF 重写机制采用后台子进程的方式来实现, 虽然不阻塞服务器继续处理命令请求, 但随之而来带来了数据同步不一致的问题, 因此 Redis 服务器设置了一个 AOF 重写缓存区, 每次子进程重写完成时, 服务器进程将 AOF 缓冲区的记录追加到新的 AOF 文件末尾, 这也是 BGREWRITEAOF 命令的实现原理 4. 事件 Redis 服务器是一个事件驱动程序, 服务器处理的事件分为时间事件和文件事件两类 文件事件处理器是基于 Reactor 模式实现的网络通信程序 文件事件是对套接字操作的抽象: 每次套接字变为可应答(acceptable), 可写(writable)或者可读(readable)时, 相应的文件事件就会产生 文件事件分为 AE_READABLE 和 AE_WRITABLE 两类事件 时间事件分为定时事件和周期性事件 服务器一般情况下只执行 serverCron 函数的一个时间事件, 并且这个事件是周期性事件 时间事件的实际处理时间通常比设定的到达时间晚一些 5. 客户端通过使用由 I/O 多路复用技术实现的文件处理器, Redis 服务器使用单线程单进程的方式来处理命令请求, 并与多个客户端进行网络通信 12127.0.0.1:6379&gt; CLIENT listid=5 addr=127.0.0.1:58793 fd=8 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client 通过客户端的标志属性 flags 可以查明客户端的角色, 以及目前客户端所处的状态, 例如 123456789# 客户端是一个主服务器REDIS_MASTER# 客户端正在被列表命令阻塞REDIS_BLOCKED# 客户端正在执行事务, 但事务的安全性已被破坏REDIS_MULTI | REDIS_DIRTY_CAS# 客户端是一个从服务器, 并且版本低于 Redis 2.8REDIS_SLAVE | REDIS_PRE_PSYNC... 服务器状态结构使用 clients 链表连接多个客户端状态, 新添加的客户端状态会被放到链表的末尾 客户端状态的 flags 属性使用不同标志来表示客户端的角色, 以及客户端当前所处的状态 输入缓冲区记录了客户端发送的命令请求, 这个缓冲区的大小不能超过 1GB 命令参数和参数个数会被记录在客户端状态的 argv 和 argc 属性里面, 而 cmd 属性则记录对了客户端要执行命令是实现函数 客户端有固定大小缓冲区和可变大小缓冲区两种缓冲区可用, 其中固定大小缓冲区的最大大小为 16KB, 而可变大小缓冲区的最大大小不能超过服务器设置的硬性限制 输出缓冲区限制值有两种, 如果输出缓冲区的大小超过了服务器设置的硬性限制, 那么客户端会被立即关闭; 除此之外, 如果客户端在一定时间内, 一直超过服务器设置的软性限制, 那么客户端也会被关闭 当一个客户端通过网络连接上服务器时, 服务器会为这个客户端创建相应的客户端状态, 网络连接关闭、发送了不合协议格式的命令请求、成为 CLIENT KILL 命令的目标、空转时间超时、输出缓冲区的大小超出限制, 以上这些原因都会造客户端被关闭 处理 LUA 脚本的伪客户端在服务器初始化时创建, 这个客户端会一直存在, 直到服务器关闭 载入 AOF 文件时使用的伪客户端在载入工作开始时动态创建, 载入工作完毕之后关闭 6. 服务器 一个命令请求从发送到完成主要包括以下几个步骤: 1) 客户端将命令请求发送给服务器; 2) 服务器读取命令请求; 3） 命令执行器根据参数查找命令的实现函数; 4) 服务器将命令回复给客户端 serverCron 函数默认每隔 100ms 执行一次, 它的主要工作包括更新服务器状态信息, 处理服务器接收的 SIGTERM 信号, 管理客户端资源和数据库状态, 检查并执行持久化操作 服务器从启动到能够处理客户端的命令请求需要执行以下步骤: 1) 初始化服务器状态; 2) 载入服务器配置; 3) 初始化服务器数据结构; 4) 还原数据库状态; 5) 执行事件循环","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"Java基础知识索引","slug":"Java基础知识索引","date":"2018-03-28T03:46:16.000Z","updated":"2018-05-23T10:50:59.597Z","comments":true,"path":"2018/03/28/Java基础知识索引/","link":"","permalink":"https://aaronshi32.github.io/2018/03/28/Java基础知识索引/","excerpt":"","text":"Java基础知识索引记录在日常工作中看到的一些浅显易懂的精华帖 ClassLoader 双亲委派模型, 隔离性 https://www.cnblogs.com/wxd0108/p/6681618.html CNAME, A, 重定向的区别 A记录 —— 映射域名到一个或多个IP, 适应于独立主机、有固定IP地址CNAME——映射域名到另一个域名（子域名）, 适应于虚拟主机、变动IP地址主机URL转发——重定向一个域名到另一个URL地址，使用HTTP 301状态码, 适应于更换域名又不想抛弃老用户 记一次排查JVM OOM的经历 A:","categories":[],"tags":[{"name":"基础知识","slug":"基础知识","permalink":"https://aaronshi32.github.io/tags/基础知识/"}],"keywords":[]},{"title":"2018书单","slug":"2018书单","date":"2018-03-28T01:27:38.000Z","updated":"2018-08-21T07:21:09.858Z","comments":true,"path":"2018/03/28/2018书单/","link":"","permalink":"https://aaronshi32.github.io/2018/03/28/2018书单/","excerpt":"","text":"思想类 《激荡十年，水大鱼大》 吴晓波 《原则》 Roy Dalio 小说类技术类 《Redis 设计与现实》 黄健宏 《深入分析 Java Web》 许令波 《图解 Http》于均良 《图解 Tcp/Ip》于均良 《分布式服务框架 原理与实践》 李林峰 《Java 并发编程实践》 Doug Lea","categories":[],"tags":[{"name":"日常","slug":"日常","permalink":"https://aaronshi32.github.io/tags/日常/"},{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-Redis 数据结构与对象","slug":"读书笔记-Redis 数据结构与对象","date":"2018-02-02T02:49:06.000Z","updated":"2018-04-02T05:55:13.180Z","comments":true,"path":"2018/02/02/读书笔记-Redis 数据结构与对象/","link":"","permalink":"https://aaronshi32.github.io/2018/02/02/读书笔记-Redis 数据结构与对象/","excerpt":"","text":"Redis：数据结构与对象1. SDS：简单动态字符串（Simple Dynamic String）在 Redis 里面，C 语言传统的字符只会作为常量，用在一些无须修改的字符串，其他字符串都用 SDS 123456789struct sdshdr &#123; // 记录 buf 数组中已使用字节的数量 // 等于 SDS 所保存字符串的长度 int len; // 记录buf 数组中未使用字节的数量 int free; // 字节数组，用于保存字符串 char buf[];&#125; SDS 与 C 字符串的区别 获取字符串长度：O(1)， len STRLEN key 命令复杂度仅为 O(1) 杜绝缓冲区溢出：buffer overflow SDSCAT（SDS API） 在拼接字符串之前会先检查空间是否足够(free)，不够的话先用 SDS 空间分配策略进行分配，再进行拼接 APPEND key 永远不会抹掉其他字符串的内存空间 减少修改字符串时带来的内存重分配次数 对于修改 C 字符串而言，拼接操作(append)会扩展底层数组的空间大小，截断操作(trim)会释放不再使用的空间，这两种内存重分配操作前者容易引起缓冲区溢出，后者容易引起内存泄漏，为了解决这两个问题，SDS 采用 空间预分配 和 惰性空间释放 两种优化策略 a. 空间预分配：当 SDS 需要修改并进行空间扩展时，程序不仅会为 SDS 分配修改所必须的空间，还会为 SDS 分配额外的未使用空间(free) a.1 分配后 len &gt; 1M，则给 free 预分配 1M 空间 a.2 分配后 len &lt; 1M，则给 free 预分配 len 空间 通过这种预分配策略，SDS 将连续增长 N 次字符串所需要的内存重分配次数从必定 N 次降低为 最多 N 次 b. 惰性空间释放：当 SDS 需要缩短保存的字符串时，程序不会立即回收多出来的字节，而是使用 free 属性将这些字节数量记录起来，等待再次使用 二进制安全 buf 长度是 len，决定了结尾，即使 buf 中存在 ‘\\0’，也不会被认为是结束 兼容部分 C 字符串函数 strcasecmp(sds-&gt;buf，”hello world”) strcat(c_string，sds-&gt;buf) 2. LinkedList：链表在 Redis 中，链表建的操作，底层实现之一的数据结构就是链表，除此之外，发布与订阅、慢查询、监视器等功能也用的是链表 链表节点（listNode）： 123456789101112typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点值 void *value;&#125; listNode; 链表（list）： 123456789101112131415161718192021typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数 unsigned long len; // 节点值复制函数 void *(*dup) (void *ptr); // 节点值释放函数 void (*free) (void *ptr); // 节点值对比函数 int (*match) (void *ptr);&#125; list; Redis 使用 list 结构持有链表，是 双端、无环、带表头指针和表尾指针，带链表长度计数器、多态（void * 接收各种类型的值） 性质的链表 3. Map：字典在 Redis 中，字典是哈希键的底层实现之一，字典由哈希表实现，哈希表由哈希表节点实现，哈希表节点就是键值对 哈希表节点 12345678910111213141516typedef struct dictEntry&#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个了哈希表节点，形成链表：解决哈希冲突 struct dictEntry *next;&#125; dictEntry; 哈希表 123456789101112131415typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算哈希值：等于 size - 1 unsigned long sizemask; // 已有节点数量 unsigned long used; &#125; dictht; 字典 123456789101112131415typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表：每个字典由两个哈希表，一个平时使用，一个仅在 rehash 时使用 dictht ht[2]; // rehash索引 int trehashidx;&#125; dict; 其中，type 和 privdata 是针对不同类型的键值对，为创建多态字典而设置，type 是指向 dictType 结构的指针，每个结构保存了一簇用于操作特定类型键值对的函数，比如：不同类型的哈希函数 ht 属性是一个包含两个项的数组，一般情况下，字典只使用 ht[0] 哈希表，ht[1] 当进行 rehash 时使用 Redis 的哈希算法使用的是 MurmurHash hash = dict -&gt; type -&gt; hashFunction(key) index = hash &amp; dict -&gt; ht[x].sizemask Redis 使用链表地址发来解决冲突，新节点将插入链表表头（O(1)） 重新哈希（rehash）操作对哈希表的大小进行相应的扩展或者收缩，维持负载因子（load factor）在一个合理的范围之内，实际上就是 ht[0] 和 ht[1] 相互替代（比如：扩展 ht[1] - 迁移 0-&gt;1 - 释放 ht[0]） load factor = ht[0].used / ht[0].size 哈希表的扩展与收缩的时机 服务器目前没有执行 BGSAVE 或者 BGREWRITEAOF 命令，并且哈希表的负载因子 &gt;= 1; 服务器目前正在执行 BGSAVE 或者 BGREWRITEAOF 命令，并且哈希表的负载因子 &gt;= 5; 渐进 rehash：为了避免 rehash 对服务器性能造成影响，服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1]，而是分多次、渐进式的将 ht[0] 里面的键值对慢慢 rehash 到 ht[1]，采用分治的思想，随着操作，随着 rehash，期间新加入的值会直接进入 ht[1]，如果 ht[0] 中 get 不到，会自动去 ht[1] 里面查找 4. Skip List：跳表 跳跃表是有序集合的底层实现之一 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成, 其中 zskiplist 用于保存跳跃表信息（比如表头、表尾节点、长度）, 而 zskiplistNode 则用于表示跳跃表节点 每个跳跃表节点的层高都是 1 至 32 之间的随机数 在同一个跳跃表中, 多个节点可以包含相同的分值, 但每个节点的成员对象必须是唯一的 跳跃表中的节点按照分值大小进行排序, 当分值相同时, 节点按照成员对象的大小进行排序 5. IntSet: 整数集合123456789101112typedef struct intset &#123; // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[];&#125; intset; 每次加入新的元素，都有可能引发升级: 1. 扩展空间, 2: 类型统一化, 3: 将新元素和老元素统一加入到扩展后的空间, 需要注意的是: 不支持降级 整数集合是集合键的底层实现之一 整数集合的底层实现为数组, 这个数组以有序、无重复的方式保存集合元素, 在有需要时, 程序会根据新添加元素的类型, 改变这个数组的类型 升级操作为整数集合带来了操作上的灵活性, 并且尽可能的节约了内存 整数集合只支持升级操作, 不支持降级操作 6. Zip List: 压缩列表 压缩列表是一种为节约内存而开发的顺序型数据结构 压缩列表被用作列表键和哈希键的底层实现之一 压缩列表可以包含多个节点, 每个节点可以保存一个字节数组或者整数值 添加新节点到压缩列表, 或者从压缩列表中删除节点, 可能会引发连锁更新操作, 但这种操作出现的几率并不高 7. Object: 对象Redis 并没有直接使用这些数据结构来实现键值对数据库, 而是基于这些数据结构创建了一个对象系统, 这个系统包含 字符串对象、列表对象、哈希对象、集合对象和有序集合对象 五种, 根据命令, 判断哪种对象适合就用于执行 Redis 数据库中的每个键值对的键和值都是一个对象 Redis 共有字符串、列表、哈希、集合、有序集合五种类型的对象, 每种类型的对象至少都要两种或以上的编码方式, 不同的编码可以在不同的使用场景上优化对象的使用效率 服务器在执行某些命令之前, 会先检查给定键的类型能否执行指定的命令, 而检查一个键的类型就是检查键的值对象的类型 Redis 的对象系统带有引用计数实现的内存回收机制 Redis 会共享值为 0 到 9999 的字符串对象 对象会记录自己的最后一次被访问的时间, 这个时间可以用于计算对象的空转时间","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"加密算法","slug":"加密算法","date":"2018-01-16T06:02:03.000Z","updated":"2018-04-03T03:34:08.634Z","comments":true,"path":"2018/01/16/加密算法/","link":"","permalink":"https://aaronshi32.github.io/2018/01/16/加密算法/","excerpt":"","text":"工作中需要使用一种加密算法，来验证消息的来源，故借此了解下这个专题 常见的加密算法可以分为三类：对称加密，非对称加密，哈希 首先是对称加密，即加密和解密使用相同的秘钥（K），优势在于速度快，当 K 很长时候，破解难度异常大，缺点是加密通路有 N 条，比如 Encrypt（A，B）和 Encrypt（B，A）是两条加密通路，则需要 N 个秘钥，其中 A，B 分别获有自己的和对方的秘钥，这就带来了秘钥泄露的风险，如果多个通路公用一个秘钥的话，一旦泄露，保密性也就无从谈起 常见的加密算法有：AES，DES，3DES，Blowfish，IDEA，RC4，RC5，RC6 接下来是非对称加密，即加密和解密使用不同的秘钥（K，K’），又称为公钥和私钥，使用公钥进行加密，使用私钥进行解密，优势在于私钥是唯一的，其他用户除了可以可以通过信息发送者的公钥来验证信息的来源是否真实，还可以确保发送者无法否认曾发送过该信息，缺点在于速度慢 常见的非对称加密算法有：RSA，ECC，DSA，Diffie-Hellman，EI Gamal 最后是哈希算法，它是一种单向的算法，即可以将目标信息通过 Hash 算法生成具有一定长度的 Hash 值，却不能通过这个 Hash 值从新获得目标信息，因此常用于不可还原的密码存储，信息完整性校验等 常见的哈希算法有： MD5，SHA，MD2，MD4，HAVAL 由于项目的本身需要，加密端在 PC，解密端在移动设备，且需要根据加密消息进行相关有效性验证，故最终选择 ECC 算法来","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"基础知识-HashMap实现与原理","slug":"基础知识-HashMap实现与原理","date":"2017-12-25T08:42:04.000Z","updated":"2018-08-09T00:57:26.911Z","comments":true,"path":"2017/12/25/基础知识-HashMap实现与原理/","link":"","permalink":"https://aaronshi32.github.io/2017/12/25/基础知识-HashMap实现与原理/","excerpt":"","text":"哈希表又称为散列表（HashTable），应用场景诸如缓存技术（memcached），xxx 等 几种数据机构在新增，查找等基础操作的执行性能 数组，指定下标查找 O(1)，指定值查找 O(n)（有序的情况下二分，插值可以达到 O(logn)），新增 O(n) 线性链表，查找 O(n)，新增 O(1) 二叉树，对一棵相对平衡的有序二叉树，新增和查找操作平均 O(logn) 哈希表，不考虑哈希冲突的情况下，新增和查找平均为 O(1) 哈希表的实质：数组 A + 哈希函数 + 解决哈希冲突的方案 而对于数组的下表，即元素的存储位置是通过哈希函数计算出来的，这个哈希函数就是 f，有以下公式 存储位置 = f(关键字)，f 为哈希函数 哈希表的一切操作前提是通过这个哈希函数，找到元素的存储位置，继而进行操作，可在找存储位置的时候，一旦发现已经先有元素占用了，这就引发了哈希冲突问题 哈希冲突： f(i) = f(j)，i ≠ j 没有完美的哈希函数存在，好的哈希函数会尽可能地保证 计算简单 和 散列地址分布均匀，一旦冲突发生，有以下解决方案： 开放定址法（发生冲突，继续寻找下一块未被占用的存储地址）， 二次散列函数法 链地址法（HashMap）也就是数组+链表的方式 HashMap 实现原理：数组 + 链表数组 1234567891011121314151617181920212223242526272829303132333435363738static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + \"=\" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; HashMap 其实就是一个 以上 Entry 数组，Entry 对象中包含了键和值，其中 next 也是一个 Entry 对象，它就是用来处理 hash 冲突的，形成一个链表 几个重要的因素 12345678//实际存储的key-value键值对的个数transient int size;//阈值，当table == &#123;&#125;时，该值为初始容量（初始容量默认为16）；当table被填充了，也就是为table分配内存空间后，threshold一般为 capacity*loadFactory。HashMap在进行扩容时需要参考threshold，后面会详细谈到int threshold;//负载因子，代表了table的填充度有多少，默认是0.75final float loadFactor;//用于快速失败，由于HashMap非线程安全，在对HashMap进行迭代时，如果期间其他线程的参与导致HashMap的结构发生变化了（比如put，remove等操作），需要抛出异常ConcurrentModificationExceptiontransient int modCount; 哈希函数 123456789101112//这是一个神奇的函数，用了很多的异或，移位等运算，对key的hashcode进一步进行计算以及二进制位的调整等来保证最终获取的存储位置尽量分布均匀final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 注意： 重写 equals 方法需同时重写 hashCode 方法，因为 HashMap 是通过 hashCode 进行映射的，如果不重写则映射不到 value putIfAbsent 是线程安全的，这个方法等价于在key不存在的时候加入一个值，如果key存在就不放入，记得判断返回值","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"基础知识-Java 语法","slug":"基础知识-Java语法","date":"2017-12-11T12:11:30.000Z","updated":"2018-10-25T10:18:13.637Z","comments":true,"path":"2017/12/11/基础知识-Java语法/","link":"","permalink":"https://aaronshi32.github.io/2017/12/11/基础知识-Java语法/","excerpt":"","text":"Q：Transient 作用 A：1）一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问 2）transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口 3）一个静态变量不管是否被transient修饰，均不能被序列化 4）若实现的是Externalizable接口，则没有任何东西可以自动序列化，需要在writeExternal方法中进行手工指定所要序列化的变量，这与是否被transient修饰无关 Q：接口和抽象类的区别 A：抽象类： 1）抽象方法必须为public或者protected（因为如果为private，则不能被子类继承，子类便无法实现该方法），缺省情况下默认为public。 2）抽象类不能用来创建对象； 3）如果一个类继承于一个抽象类，则子类必须实现父类的抽象方法。如果子类没有实现父类的抽象方法，则必须将子类也定义为为abstract类 接口： 1）接口中的变量会被隐式地指定为public static final变量 2）方法会被隐式地指定为public abstract方法且只能是public abstract方法 继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系，一个类只能继承一个抽象类，而一个类却可以实现多个接口 Q：serialVersionUID 作用 A：Java 的序列化机制是通过在运行时判断类的 serialVersionUID 来验证版本一致性的。在进行反序列化时，JVM 会把传来的字节流中的serialVersionUID 与本地相应实体（类）的 serialVersionUID 进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常。(InvalidCastException) 1）两端不一致，无法序列化 2）两端一致，增的字段会序列化成默认值 3）两端一致，减的字段会序列化丢失 Q：多线程相关 A： 1） 并行：多个cpu实例或者多台机器同时执行一段处理逻辑，是真正的同时；并发：通过cpu调度算法，让用户看上去同时执行，实际上从cpu操作层面不是真正的同时 2）线程安全：代码在多线程中使用，线程的调度顺序不影响结果 3）线程状态：NEW，RUNNABLE，WAITING，BLOCKED，TERMINATED，TIMED_WAITING 4）sleep 和 wait 区别：前者不会释放锁，后者会 5）三种线程实现方式：Thread，Runnable，Callable（Future），后者可以返回线程执行结果 6）容器类：BlockingQueue、ConcurrentHashMap 7）ConcurrentHashMap 是用 lock 实现并发（锁的方式是稍微细粒度的。将hash表分为16个桶（默认值），诸如get, put, remove等常用操作只锁当前需要用到的桶）而 HashTable 是用 synchronized 实现并发（对整个集合加锁，加锁期间集合不可访问） Q：String 类是不可变的吗 A：从语法上来讲，是的，因为 String 类是用 final 修饰的，绝对吗？不绝对，因为可以用 UnSafe 来修改 String 值；多个相同值的 String，可以用 intern（） 来返回存储在字符串池的值，需要注意：采用new 创建的字符串对象不进入字符串池，见以下代码 1234567891011String str1 = \"a\"; String str2 = \"b\"; String str3 = \"ab\"; String str4 = str1 + str2; String str5 = new String(\"ab\"); System.out.println(str5.equals(str3)); // true System.out.println(str5 == str3); // false System.out.println(str5.intern() == str3); // true: str3 是直接声明的，进入字符串池 System.out.println(str5.intern() == str4); // false: str5 是 new 声明的，不进入字符串池","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"基础知识-Http 和 Https","slug":"基础知识-Http 和 Https","date":"2017-11-25T08:42:04.000Z","updated":"2018-08-22T09:10:40.243Z","comments":true,"path":"2017/11/25/基础知识-Http 和 Https/","link":"","permalink":"https://aaronshi32.github.io/2017/11/25/基础知识-Http 和 Https/","excerpt":"","text":"TCP 三次握手 和 四次挥手三次握手 Client 发送 SYN 给 Server，Client 状态置为 SYN-SEND Server 发送 SYN + ACK 给 Client，Server 状态置为 SYN-RCVD Client 发送 ACK 给 Server，Server 状态置为 ESTABLISED 四次挥手 Client 发送 FIN 给 Server，Client 状态置为 FIN-WAIT-1 Server 发送 ACK 给 Client，Server 状态置为 CLOSE-WAIT，Client 收到后置为 FIN-WAIT2 Server 发送 FIN 给 Client，Sever 状态置为 LAST-ACK Client 发送 ACK 给 Server，Client 状态置为 TIME-WAIT，Server 收到后状态置为 CLOSE 为什么四次挥手是由客户端发起的 https://blog.csdn.net/Daputao_net/article/details/81255499 Http 和 HttpsHTTP是应用层协议，位于HTTP协议之下是传输协议TCP。TCP负责传输，HTTP则定义了数据如何进行包装 HTTP - TCP（明文传输） HTTPS - TLS/SSL - TCP（密文传输）","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://aaronshi32.github.io/tags/面试/"}],"keywords":[]},{"title":"SpringBoot 问题汇总","slug":"SpringBoot 问题汇总","date":"2017-10-16T03:22:52.000Z","updated":"2018-10-25T02:39:12.592Z","comments":true,"path":"2017/10/16/SpringBoot 问题汇总/","link":"","permalink":"https://aaronshi32.github.io/2017/10/16/SpringBoot 问题汇总/","excerpt":"","text":"简介最近上手项目, 开始接触了 SpringBoot, 相比较 SpringMVC 而言, 省去了繁琐的配置, 秉承默认大于配置的原则, 使用起来更加简单方便。但越是黑盒(简单), 使用起来越不安, 这里将学习过程中遇到的问题记录下来, 慢慢地打开这个黑盒 初始化工程问题 使用 Maven 初始化, 有两种方式, 以此解决 Maven 不支持多 parent 的问题 继承默认的 parent 123456&lt;!-- Inherit defaults from Spring Boot --&gt;&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;&lt;/parent&gt; - 将 SpringBoot 的依赖添加到 dependencyManagement 中，并且设置 scope=import 1234567891011121314151617181920&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Override Spring Data release train provided by Spring Boot --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-releasetrain&lt;/artifactId&gt; &lt;version&gt;Fowler-SR2&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;!-- Spring Boot basic dependencies --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 2. Maven用户可以继承 spring-boot-starter-parent 项目来获取合适的默认设置。该parent项目提供以下特性 12345678- 默认编译级别为Java 1.6- 源码编码为 UTF-8- 一个 dependency management 节点，允许你省略常见依赖的 &lt;version&gt; 标签，继承自 spring-boot-dependencies POM。- 恰到好处的资源过滤- 恰到好处的插件配置（exec插件，surefire，Git commit ID，shade）- 恰到好处的对 application.properties 和 application.yml 进行筛选，- 包括特定 profile（profile-specific）的文件，比如 applicationfoo.- properties 和 application-foo.yml 3. Spring Boot Starters 列表 官方：https://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#using-boot-starter 中文：http://blog.csdn.net/chszs/article/details/50610474 4. 配置工程自检功能：`spring-boot-starter-actuator` 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件中增加：management.security.enabled=false 访问以下网站： 123456http://localhost:8080/beanshttp://localhost:8080/envhttp://localhost:8080/healthhttp://localhost:8080/metricshttp://localhost:8080/tracehttp://localhost:8080/mappings 默认 的奥秘，来源于：spring-boot-autoconfigure 的 JAR 文件 @SpringBootApplication 注解等价于以默认属性使用 @Configuration ， @EnableAutoConfiguration 和 @ComponentScan，即开启组件扫描和自动配置 application.properties（yml） 默认加载路径：当前目录下 /config 子目录，当前目录，classpath 下的 /config，classpath 根路径。如果不喜欢将 application.properties 作为配置文件名，你可以通过指定 spring.config.name 环境属性来切换其他的名称，也可以使用spring.config.location 环境属性引用一个明确的路径（目录位置或文件路径列表以逗号分割） Spring 4.0 的条件化配置是自动配置的基础，当满足一定条件时才会执行某段程序，比如在 classpath 中发现了 JdbcTemplate 时才自动注入，这使得众多 autoconfig 选项得以简化 配置的加载顺序 Spring Boot设计了一个非常特别的 PropertySource 顺序，以允许对属性值进行合理的覆盖，属性会以如下的顺序进行设值： home目录下的devtools全局设置属性（ ~/.spring-bootdevtools.properties ，如果devtools激活） 测试用例上的@TestPropertySource注解 测试用例上的@SpringBootTest#properties注解 命令行参数 来自 SPRING_APPLICATION_JSON 的属性（环境变量或系统属性中内嵌的内联JSON） ServletConfig 初始化参数 ServletContext 初始化参数 来自于 java:comp/env 的JNDI属性 Java系统属性（System.getProperties()） 操作系统环境变量 RandomValuePropertySource，只包含 random.* 中的属性 没有打进jar包的Profile-specific应用属性（ application-{profile}.properties 和YAML变量） 打进jar包中的Profile-specific应用属性（ application-{profile}.properties 和YAML变量） 没有打进jar包的应用配置（ application.properties 和YAML变量） 打进jar包中的应用配置（ application.properties 和YAML变量） @Configuration 类上的 @PropertySource 注解 默认属性（使用 SpringApplication.setDefaultProperties 指定） 开发调试问题 使用 spring-boot-devtools 来实现热部署（TODO） Maven 构建插件的主要功能是把项目打包成一个可执行的超级 JAR（uber-JAR），包括把应用程序的所有依赖打入 JAR 文件内，并为 JAR 添加一个描述文件，其中的内容能够通过 java -jar 来运行应用程序 Spring 启动问题 精辟：读取配置说明（xml，java配置，Groovy配置，其他类型配置），再应用程序上下文里初始化 Bean，将 Bean 注入依赖他们的其他 Bean 中，Spring 帮你通过 组件扫描，自动织入和生命切面等额外辅助功能，帮你简单的做了初始化的事情 导入的 starter 是如何 在 application.properties 中给出配置提示的 starter 的包 /META-INF/spring.factories 中会有 org.springframework.boot.autoconfigure.EnableAutoConfiguration 自动配置的实现类，打开这个类可以看到实现了 ImportBeanDefinitionRegistrar 接口的 registerBeanDefinitions 方法，如此一来，配置文件中就会给出提示啦 Spring Cloud 大家族 1、Spring Cloud Config 配置中心，利用git集中管理程序的配置。 2、Spring Cloud Netflix 集成众多Netflix的开源软件 3、Spring Cloud Bus 消息总线，利用分布式消息将服务和服务实例连接在一起，用于在一个集群中传播状态的变化 4、Spring Cloud for Cloud Foundry 利用Pivotal Cloudfoundry集成你的应用程序 5、Spring Cloud Cloud Foundry Service Broker 为建立管理云托管服务的服务代理提供了一个起点。 6、Spring Cloud Cluster 基于Zookeeper, Redis, Hazelcast, Consul实现的领导选举和平民状态模式的抽象和实现。 7、Spring Cloud Consul 基于Hashicorp Consul实现的服务发现和配置管理。 8、Spring Cloud Security 在Zuul代理中为OAuth2 rest客户端和认证头转发提供负载均衡 9、Spring Cloud Sleuth SpringCloud应用的分布式追踪系统，和Zipkin，HTrace，ELK兼容。 10、Spring Cloud Data Flow 一个云本地程序和操作模型，组成数据微服务在一个结构化的平台上。 11、Spring Cloud Stream 基于Redis,Rabbit,Kafka实现的消息微服务，简单声明模型用以在Spring Cloud应用中收发消息。 12、Spring Cloud Stream App Starters 基于Spring Boot为外部系统提供spring的集成 13、Spring Cloud Task 短生命周期的微服务，为SpringBooot应用简单声明添加功能和非功能特性。 14、Spring Cloud Task App Starters 15、Spring Cloud Zookeeper 服务发现和配置管理基于Apache Zookeeper。 16、Spring Cloud for Amazon Web Services 快速和亚马逊网络服务集成。 17、Spring Cloud Connectors 便于PaaS应用在各种平台上连接到后端像数据库和消息经纪服务。 18、Spring Cloud Starters （项目已经终止并且在Angel.SR2后的版本和其他项目合并） 19、Spring Cloud CLI 插件用Groovy快速的创建Spring Cloud组件应用。","categories":[],"tags":[{"name":"框架","slug":"框架","permalink":"https://aaronshi32.github.io/tags/框架/"}],"keywords":[]},{"title":"NodeJS - 理解 内存控制","slug":"NodeJS - 理解 内存控制","date":"2017-08-29T03:22:52.000Z","updated":"2017-08-29T03:32:24.000Z","comments":true,"path":"2017/08/29/NodeJS - 理解 内存控制/","link":"","permalink":"https://aaronshi32.github.io/2017/08/29/NodeJS - 理解 内存控制/","excerpt":"","text":"简介V8 具有内存限制，所有的 JavaScript 对象都是通过堆来进行分配的，可以通过 process.memoryUsage() 来查看内存信息，其中 heapTotal 和 heapUsed 分别代表了已申请到的对内存和当前使用的量。如果当前代码已申请的堆空闲内存不够分配新的对象，将就申请堆内存，知道堆的大小超过 V8 的内存限制为止 为何采取 V8 内存限制：一次小的垃圾回收需要 50ms 以上，一次非增量式的垃圾回收甚至需要 1s 以上，这是垃圾回收引起的 JS 线程暂停执行的时间，应用的性能和响应时间都会大幅度下降 V8 的垃圾回收机制采用 node –trace-gc xxx 命令行参数来查看内存垃圾回收日志，使用 –prof 可以得到 V8 执行时的性能分析数据，配合 tick-processor 工具查看统计信息 垃圾回收算法 基于分代式垃圾回收（解决不同的对象生命周期不同） --max-old-space-size 指定老生代内存空间的最大值 + --max-new-space-size 指定新生代内存空间的最大值 = V8 堆的整体大小 新生代的回收算法：Scavenge（base on Cheney） 核心思想：空间一分为二，从 From 空间 复制 到 To 空间，缺点是只能使用堆内存的一般，好处是以空间换取时间（块），适合新生代频繁的内存回收 对象晋升两个条件：是否经历过 Scavenge 回收，To空间的内存占用是否超过了比例（25%） 老生代的回收算法：Mark-Sweep &amp; Mark-Compact 核心思想：标记 - 清楚 - 整理内存碎片 回收策略：延迟清理（lazy sweeping）和 增量式整理（incremental compaction） 为了降低全堆垃圾回收带来的线程停顿问题，采用增量标记来减少由于垃圾回收造成的停顿时间 高效使用内存在正常的 JavaScript 执行中，无法立即回收的内存有必报和全局变量引用这两种情况，由于 V8 的内存限制，要十分小心此类变量是否无限制的增加，因为他会导致老生代的对象增多 内存指标 使用 process.memoryUsage() 查看 Node 进程的内存占用情况 12345678910111213141516171819202122232425// outofmemory.jsfunction showMem()&#123; var mem = process.memoryUsage(); var format = function(bytes)&#123; return (bytes / 1024 / 1024).toFixed(2) + 'MB'; &#125; console.log(`Process: heapTotal $&#123;format(mem.heapTotal)&#125; heapUsed $&#123;format(mem.heapUsed)&#125; rss $&#123;mem.rss&#125;`)；&#125;var useMem = function()&#123; var size = 20 * 1024 * 1024; var arr = new Array(size); for(var i = 0; i &lt; size; i++)&#123; arr[i] = 0; &#125; return arr;&#125;var total = [];for(var j = 0; j &lt; 15； j++)&#123; showMem(); total.push(useMem());&#125;showMem(); 使用 os.totalmem() 和 os.freemem() 查看 操作系统 的内存情况 堆外内存：那些不是通过 V8 分配的内存 - Buffer 对象","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"NodeJS - 理解 Buffer","slug":"NodeJS - 理解 Buffer","date":"2017-08-29T03:22:52.000Z","updated":"2017-08-29T03:31:12.000Z","comments":true,"path":"2017/08/29/NodeJS - 理解 Buffer/","link":"","permalink":"https://aaronshi32.github.io/2017/08/29/NodeJS - 理解 Buffer/","excerpt":"","text":"简介Buffer 是一个像 Array 的对象，主要用于操作字节，数组元素为 16 进制的两位数，即 0 到 255的数值，如果超出或者不足，小数等情况，会使用叠加，递减，省略小数部分的措施保证元素数值合法性，此外，采用 JavaScript 和 C++ 相结合的模式，将性能部分用 C++ 来实现，非性能相关的部分用 JavaScript 来实现 内存分配机制Buffer 对象的内存分配不是在 V8 的堆内存中，而是在 Node 的 C++ 层面实现内存的申请，采用 slab 动态内存管理机制（先申请，后分配），以 4KB 作为界限来区分 Buffer 是大对象还是小对象，针对大对象，每次都 alloc 一个足够长的 SlowBuffer 对象（C++层）作为 slab 单元，针对小对象，则共用一个 slab，当不足以分配时（可分配小于 4KB），才会 alloc 一个新的 slab 内存进行再分配。需要注意的是当且仅当一个 slab 上面的所有小对象在作用域释放并都可以回收时，slab 的 8KB 才会被回收，此处会存在由于编码不当导致的内存泄漏，浪费问题 在 Buffer 中创建一个数组，需要注意以下规则： Buffer 是内存拷贝，而不是内存共享 Buffer 占用内存被解释为一个数组，而不是字节数组。比如，new Uint32Array(new Buffer([1,2,3,4])) 创建了 4 个 Uint32Array，它的成员为 [1,2,3,4]，而不是 [0x1020304] 或 [0x4030201] 1234567891011121314151617// 4KB 作为界限的由来：createPool 判断条件Buffer.poolSize = 8 * 1024; function allocate(size) &#123; if (size &lt;= 0) &#123; return new FastBuffer(); &#125; if (size &lt; (Buffer.poolSize &gt;&gt;&gt; 1)) &#123; if (size &gt; (poolSize - poolOffset)) createPool(); var b = new FastBuffer(allocPool, poolOffset, size); poolOffset += size; alignPool(); return b; &#125; else &#123; return createUnsafeBuffer(size); &#125;&#125; 四种 内存分配的 API Buffer.from Buffer.alloc Buffer.allocUnSafe Buffer.allocUnSafeSlow 支持与字符串相互转换目前支持的字符串编码类型有：ASCII，UTF-8，UTF-16LE/UCS-2，Base64，Binary，Hex，可以用 isEncoding() 来判断是否支持编码 对于不支持的编码类型，可以通过 iconv 和 iconv-lite 两个模块来支持更多编码类型转换 转换成 Buffer：new Buffer(str，[encoding]) 和 buf.write(string，[offset]，[length]，[encoding]) 转换成 String：buf.toString([encoding]，[start]，[end]) 正确的拼接方式用一个数组来存储接收到的所有 Buffer 片段并记录下所有片段的总长度，然后调用 Buffer.concat() 方法生成一个合并的 Buffer 对象 123456789101112131415161718192021222324252627282930313233343536Buffer.concat = function concat(list, length) &#123; var i; if (!Array.isArray(list)) throw kConcatErr; if (list.length === 0) return new FastBuffer(); if (length === undefined) &#123; length = 0; for (i = 0; i &lt; list.length; i++) length += list[i].length; &#125; else &#123; length = length &gt;&gt;&gt; 0; &#125; var buffer = Buffer.allocUnsafe(length); var pos = 0; for (i = 0; i &lt; list.length; i++) &#123; var buf = list[i]; if (!isUint8Array(buf)) throw kConcatErr; _copy(buf, buffer, pos); pos += buf.length; &#125; // Note: `length` is always equal to `buffer.length` at this point if (pos &lt; length) &#123; // Zero-fill the remaining bytes if the specified `length` was more than // the actual total length, i.e. if we have some remaining allocated bytes // there were not initialized. buffer.fill(0, pos, length); &#125; return buffer;&#125;; Buffer 与性能 网络传输用 Buffer 比直接传字符串要快很多 通过预先转换静态内容为 Buffer 对象，可以有效地减少 CPU 的重复使用，节省服务器资源。在构建 Web 应用中，可以选择将页面中的动态内容和静态内容分离，静态内容部分可以通过预先转换为 Buffer 的方式，是性能得到提升。由于文件自身是二进制数据，所以在不需要改变内容的场景下，尽量只读取 Buffer，然后直接传输，不做额外的转换，避免损耗 文件读取速度与 highWaterMark 有关 这个值代表了每次读取的长度，对 Buffer 内存的分配和使用有一定影响，设置过小，可能导致系统调用的次数过多，在读取一个相同的大文件时，该值越大，读取的速度越快","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（十一）","slug":"读书笔记-程序员的自我修养（十一）","date":"2017-07-05T02:51:46.000Z","updated":"2017-07-05T02:53:28.000Z","comments":true,"path":"2017/07/05/读书笔记-程序员的自我修养（十一）/","link":"","permalink":"https://aaronshi32.github.io/2017/07/05/读书笔记-程序员的自我修养（十一）/","excerpt":"","text":"系统调用与API 普及：现代操作系统中程序本身是 没有权利访问过多系统资源 的，为了防止程序访问冲突，操作系统将可能产生冲突的系统资源给 保护起来 普及：每个操作系统都会提供 一套接口，以供应用程序使用，这些接口往往通过 中断 来实现，比如 Linux 使用 0x80 号中断作为系统调用的入口，Windows 采用 0x2E 号中断作为系统调用入口 系统调用的弊端 使用不便，操作系统提供的系统调用接口往往过于原始，程序员需要了解很多与操作系统相关的细节 各个操作系统之间系统调用不兼容，定义和实现都不大一样 为此：运行时库将不同的操作系统的系统调用包装为统一固定的接口，使得童颜的代码，在不同的操作系统下都可以直接编译，并产生一致的效果，这就是源码级上的可移植性 系统调用的原理 用户态 和 内核态：现代操作系统的CPU在不同特权级别下执行不同的指令，称之为 用户模式（User Mode） 和 内核模式（Kernel Mode） 系统调用 是运行在内核态的，而 应用程序 基本都是运行在用户态的 操作系统一般通过 中断 来从用户态 切换 到内核态，中断是一个硬件或软件发出的 请求，要求 CPU 暂停当前的工作 转手去处理 更加重要的事情 中断的两个属性：中断号（Interrupt Number）（中断类型） 和 中断处理程序（Interrupt Service Routine）（既然中断了就要告诉CPU去干什么） 中断向量表（Interrupt Vector Table）：是个数组，第 n 项包含了指向第 n 号中断的中断处理程序的 指针 中断的流程：当中断到来时，CPU 会暂停当前执行的代码，根据中断号，在中断向量表中找到对应的中断处理程序，并调用它。中断处理程序执行完成之后，CPU 会继续执行之前的代码 硬件中断 和 软件中断：硬件中断通常来源于硬件的异常或其他事情的发生，软件中断通常是一条 指令（i386 下是 int），带有一个参数记录中断号，使用这条指令用户可以手动触发某个中断并执行其终端处理程序，比如 int 0x80 会调用第 0x80 号中断的处理程序 Windows API Windows API 是以 DLL 导出函数的形式暴露给应用开发者的，规模上非常庞大，其中一个头文件 “Windows.h” 包含了核心部分 基本服务：kernel32.dll、图形设备接口：gdi32.dll、用户接口：user32.dll、高级服务：advapi32.dll、通用对话框：comdlg32.dll、通用控件：comctl32.dll、Shell：shell32.dll、网络服务：ws2_32.dll","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（十）","slug":"读书笔记-程序员的自我修养（十）","date":"2017-06-26T03:03:20.000Z","updated":"2017-06-26T03:07:18.000Z","comments":true,"path":"2017/06/26/读书笔记-程序员的自我修养（十）/","link":"","permalink":"https://aaronshi32.github.io/2017/06/26/读书笔记-程序员的自我修养（十）/","excerpt":"","text":"运行库 入口函数和程序初始化 普及：操作系统装载程序之后，首先运行的代码并不是 main 的第一行，而是 某些别的代码，这写代码负责准备好 main 函数执行所需要的环境，并且负责调用 main 函数，我们称这写代码为 入口函数 或 入口点（Entry Point） 程序初始化：Main函数之前有个入口函数 操作系统在创建进程后，把控制权交到了程序的入口，这个入口往往是运行库中的某个入口函数 入口函数对运行库和程序运行环境进行初始化，包括堆、I/O、线程、全局变量构造等 入口函数在完成初始化之后，调用 main 函数，正式开始执行程序主体部分 main 函数执行完毕以后，返回到入口函数，入口函数进行清理工作，然后进行系统调用结束进程 入口函数示例 GLIBC（Linux）：__start -&gt; __libc_start_main -&gt; exit -&gt; _exit，即在 __libc_start_main 执行之前，做了参数压入栈，寄存器初始化，全局变量赋值等操作 MSVC CRT（Windows）：初始化和 OS 版本有关的全局变量，初始化堆，初始化 I/O，获取命令行参数和环境变量，初始化C库的一些数据，调用 main 并记录返回值，检查错误并将 main 的返回值返回 堆 初始化：mainCRTStartup –&gt; heap_init() –&gt; HeapCreate 方法 I/O 初始化：建立 打开文件表，如果能够继承自父进程，那么从父进程 获取 继承的句柄，初始化 标准输入输出 运行库与 I/O 对于程序来说，I/O 涵盖的范围还要广一些，一个程序的 I/O 指代了程序与外界的交互，包括文件、管道、网络、命令行、信号等 I/O 初始化函数需要在用户空间中建立 stdin、stdout、stderr 及其 对应的 FILE 结构，是的程序进入 main 之后可以直接使用 printf，scanf 等函数 C++运行库 普及：任何一个 C 程序，它的背后都有一套庞大的代码来进行支撑，以使得该程序能够正常运行。这套代码至少包括入口函数，及其所以来的函数所构成的函数集合，当然，他还理应包括各种标砖函数的实现，我们称之为 运行时库（Runtime Library），亦或 C运行库（CRT） CRT大致包含了如下功能： 启动与退出：包括入口函数及入口函数所依赖的其他函数等 标准函数：由C语言标准规定的 C语言标准库 所拥有的函数实现 I/O：I/O功能的封装和实现 堆：堆的封装和实现 语言实现：语言中一些特殊功能的实现 调试：实现调试功能的代码 C语言标准库由 24 个 C头文件 组成：标准输入输出（stdio.h），文件操作（stdio.h），字符操作（ctype.h），字符串操作（string.h），数学函数（math.h），资源管理（stdlib.h），格式转换（stdlib.h），时间/日期（time.h），断言（assert.h），各种类型上的常熟（limits.h &amp; float.h），变长参数（stdarg.h），非局部跳转（setjump.h） 运行库是和平台（操作系统）强相关 的，提供了不同操作系统平台的底层抽象，Linux 和 Window 平台下的两个主要 C 语言运行库分别为： glibc （GNC C Library） 和 MSVCRT（Microsoft Visual C Run-time） 运行库与多线程 Window API ：CreateThread() 和 ExitThread() MSVCRT：_beginThread() / _beginthreadex 和 _endthread() Glibc：pthread_create() 和 pthread_exit()","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（九）","slug":"读书笔记-程序员的自我修养（九）","date":"2017-06-20T02:06:20.000Z","updated":"2017-06-20T02:08:04.000Z","comments":true,"path":"2017/06/20/读书笔记-程序员的自我修养（九）/","link":"","permalink":"https://aaronshi32.github.io/2017/06/20/读书笔记-程序员的自我修养（九）/","excerpt":"","text":"内存：一个承载程序运行的介质，也是程序进行各种运算和表达的场所 程序的内存（进程的地址空间）布局 栈：用于维护函数调用的上下文，执行 函数调用 的功能 堆：用于程序 动态分配 的内存区域，也是 malloc 或 new 分配的内存区域 可执行文件映像：存储着可执行文件在内存里的映像，装载器 保留区：保留区并不是一个单一的内存区域，而是对内存中受到保护而禁止访问的内存区域的总称，比如有些地址不允许访问等等 栈与调用惯例 作用：保存了一个函数调用所需要的 维护信息（函数返回地址和参数，临时变量，保存的上下文），常称为 堆栈帧（Stack Frame） 函数的调用方和被调用方对于函数如何调用，遵循 惯用惯例，包括：函数参数的传递顺序和方式，栈的维护方式，名字修饰策略。常用的管理模式有：cdecl（函数调用方），stdcall（函数本身），fastcall（函数本身），pascal（函数本身） 堆与内存管理 普及：全局变量没有办法动态地产生，只能在编译的时候定义 形象解释：运行库相当于是向操作系统 “批发” 了一块较大的对控件，然后 “零售” 给程序用。当全部 “售完” 或程序有大量的内存需求时，再根据实际需求向操作系统 “进货” Linux 提供两种系统调用：brk 和 mmap，Window 提供四种系统调用：HeapCreate，HeapAlloc，HeapFree，HeapDestroy，对向上封装的函数就是著名的 malloc 要点： 堆里的同一片内存不能重复释放两次 malloc 申请的内存，进程结束后，所有资源都会回收，因此不存在了 malloc 申请的内存，逻辑上是连贯的，物理上不一定 堆分配算法：如何管理一大块连续的内存空间，能够按照需求分配、释放其中的空间 空闲链表 核心原理：在堆里的每一个空闲空间的开头（或结尾）有一个头（header），头结构里记录了上一个（prev）和下一个（next）空间块的地址，这样所有空闲块形成了一个链表，申请的时候，查找符合大小的空闲块，然后将这块空闲空间从链表中”删除”，供使用，回收的时候，需要知道头指针和空间大小，因此在申请的时候，往往申请 K 空间，分配 K + 4 的空间，多余的 4 个记录大小，但这样一旦堆操作越界，破坏了这 4 个里面的数据，整个对就无法正常工作了 位图 将整个堆划分为大量的 块（Block），每个块大小相同，申请的时候，总是分配整数个块的空间，且称已经分配的第一块为 头（head），其余的称为 主体（Body），优点：速度快，整个堆的空闲信息存储在一个数组中，因此访问该数组时cache容易命中，稳定性好：为了避免用户越界读写破坏，只需要简单的备份下位图即可，缺点：分配容易产生碎片（平均划分），位图很大，cache命中率降低 对象池 以申请空间的大小作为分水岭，综合应用上述两种算法","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（八）","slug":"读书笔记-程序员的自我修养（八）","date":"2017-06-14T02:45:07.000Z","updated":"2017-06-14T02:46:54.000Z","comments":true,"path":"2017/06/14/读书笔记-程序员的自我修养（八）/","link":"","permalink":"https://aaronshi32.github.io/2017/06/14/读书笔记-程序员的自我修养（八）/","excerpt":"","text":"Windows 下的动态链接 普及：Windows 下的 DLL 文件和 EXE 文件实际上是一个概念，都是 PE 格式 的二进制文件，扩展名并不唯一：.dll 和 .ocx 或是 .cpl 都是 dll 文件，软件更新包（Service Packs）机制就是通过升级 DLL 的形式进行自我完善 普及：当一个 dll 被装载到内存中之后，通常有两个地址概念分别是：基地址（Base Address） 和 相对地址（RVA, Relative Virtual Address），基地址就是被装载的起始地址，RVA地址就是 地址 + 基地址 普及：dll 实现进程共享数据段，不推荐，存在一定的安全风险，应该尽量 避免 这种 dll 共享数据段来实现进程间通信 普及：声明 dll 中的某个函数为 导出函数 的办法有两种：一种是使用 __declspec(dllexport) 扩展，另一种是采用模块定义（.def）文件声明 普及：dll 支持显示运行时链接，提供了 3 个 API 分别是：LoadLibrary，GerProcAddress，FreeLibrary 符号导入导出表 导出表，EXP文件，导出重定向，导入表，导入函数调用，这些都是 dll 中存在的结构和方式，均与符号和函数相关 DLL优化 应用程序在加载 dll 导致启动速度变慢，这里主要有两个原因：1. dll 的代码段和数据段本身并不是地址无关的，当被装载的目标地址被占用时，整个 dll 便会引起 rebase，频繁的 rebase 情况更加糟糕； 2. 导入函数的符号在运行时需要被 逐个查找解析，这里用的是二分查找，整个过程随着 dll 数量的增加，也是非常耗时的 重定基地址（Rebasing）：只需要重新定位基地址即可，其余的 RVA 只是的偏移量，不受影响，当且仅当 dll 被装载时，基地址被占用的情况下，才会发生 重定位，所以 windows 系统为本身自带的许多库单独划分了一段空间区域（0x70000000 ~ 0x80000000），用于映射这些系统常用的 dll dll 绑定：把导出函数的地址保存到模块的导入表中，省去了每次启动时符号解析的过程，根本优化在于：当程序每次运行时，所有被依赖的 dll 都会装载，并且一些列的导入导出符号依赖关系都会 被重新解析，大多数情况下，这些 dll 都会 以同样的顺序 被装载到 同样的内存地址，所以他们的到处符号的地址都是不变的 在使用 C++ 时需要注意的动态链接事项 所有接口函数都应该是抽象的，所有的方法都应该是纯虚的 所有的全局函数都应该是用 extern “C” 来防止名字修饰的不兼容，并且导出函数都是应该是 __stdcall 调用规范的 不要使用 C++ 标准的 STL 不要使用异常 不要使用虚析构函数 不要在 dll 里面申请内存，而且在 dll 外释放（或者相反） 不要再接口中使用重载方法 DLL HELL：主要是更新过程中的兼容性问题，解决方法如下： 静态链接 防止 dll 覆盖（DLL Stomping） 避免 dll 冲突（Conflicting DLLs） 应用程序使用 Manifest 文件来打包，控制自身依赖的 dll","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（七）","slug":"读书笔记-程序员的自我修养（七）","date":"2017-06-06T03:03:25.000Z","updated":"2017-06-06T03:07:12.000Z","comments":true,"path":"2017/06/06/读书笔记-程序员的自我修养（七）/","link":"","permalink":"https://aaronshi32.github.io/2017/06/06/读书笔记-程序员的自我修养（七）/","excerpt":"","text":"动态链接（Dynamic Linking）：一个单一个可执行文件模块被拆分成若干模块，在程序运行时进行链接的一种方式 欲扬先抑：静态链接饱受 内存磁盘空间浪费（同一个目标文件存在多份供使用），更新发布需要重新链接 等问题，而动态链接解决了上面的问题，提供了 可扩展性（动态选择加载），但饱受 兼容性 （如不兼容，程序将崩溃无法运行），性能损耗（装载时重新链接）困扰 基本思想：把程序按照模块 拆分 成各个相对 独立 部分，在程序 运行时 才将它们链接在一起形成一个 完整 的程序，而不是像静态链接一样把所有的程序模块都链接成一个单独的可执行文件 普及： Linux 下，ELF 动态链接文件叫 动态共享对象（DSO，Dynamic Shared Objects），以 .so 结尾；Windows 下，动态链接文件叫 动态链接库（Dynamical Linking Library），以 .dll 结尾 普及：动态链接器 才完成以上操作，策略：延迟绑定（Lazy Binding） 普及：PIC（Position-independent Code）地址无关代码 技术，GOT（Global Offset Table）全局偏移量表 动态链接的地址空间分配 奥秘 共享对象的 最终装载地址 在编译时是 不确定 的，而是在 装载 时，装载器根据当前地址空间的空闲情况，动态分配一块 足够大小 的虚拟地址空间给相应的共享对象 普及：静态链接时提到的重定位叫做 链接时重定位（Link Time Relocation），而动态链接提到的重定位叫做 装载时重定位（Load Time Relocation），也成 基址重置（Rebasing） 共享对象的虚拟地址空间分配技术离不开：地址无关代码（PIC机制），共享模块的全局变量（线程私有存储 Thread Local Storage），数据段地址无关性，也正是这些无关性技术，才保证了共享对象被多个程序引用时，能够发挥各自的作用，确保程序不出现崩溃 延迟绑定（Lazy Binding）的 艺术 性能的事实：动态链接是以 牺牲性能 为代价的，具体表现在：一方面 对于全局和静态数据的访问、模块间的调用都要进行复杂的 GOT定位，然后 间接寻址，另一方面动态链接的工作是在 运行时 完成的，不是事先链接好的 优化的手段：延迟绑定，理解为当函数第一次被用到时才进行绑定（符号查找、重定位等），使用 PLT （Procedure Linkage Table） 来实现，通常以 .plt 作为段名，保存在 ELF 文件中 实现原理：通过 PLT 中的待跳转指令，将各个函数的待跳转偏移量链接到地址栏，等运行时使用的时候，直接读取偏移量进行跳转，这里不是直接读取GOT链接到真正的目标函数地址，而是读取偏移量，进行jump，再到目标函数 原话：而是将上面代码中第二条指令&quot;push n&quot;的地址填入到 bar@GOT 中，这个步骤不需要查找任何符号 动态链接 相关结构 .interp 段：保存可执行文件需要的 动态链接器 的路径，内容就是个字符串 .dymanic 段：保存了动态链接器所需要的 基本信息：依赖于哪些共享对象、动态链接符号表的位置、动态链接重定位表的位置、共享对象初始化代码的地址等，位于经典的头文件 elf.h （Elf32_Dyn） 中 动态符号表（Dynamic Symbol Table）[.dynsym 段]：只保留动态链接这些模块之间的符号导入导出关系 动态链接重定位表：.rel.dyn 是对数据引用的重定位修正，它所修正的位置位于 .got 及 数据段，.rel.plt 是对函数引用的修正，它所修正的位置位于 .rel.plt 动态链接时进程 堆栈初始化信息 堆栈里保存了关于进程 执行环境：程序执行入口 和 命令行参数 等信息，还保存了动态链接器所需要的一些 辅助信息数组（Auxiliary Vector） int main(int argc, char argv[]) ，argc 表示参数的个数，*argv数组 是参数 动态链接器的 步骤 和 实现 步骤： 启动动态链接器本身 动态链接器本身也是一个共享对象，因此它的编写除了不能依赖于其他任何共享对象，而且本身所需要的全局和静态变量的重定位工作也由它本身来完成，因此有一段精致的启动代码称之为 自举（Bootstrap） 装载所有需要的共享对象 动态链接器会根据 .dynamic 段中所依赖的共享对象，使用 广度优先的图遍历 算法，来按顺序状态共享对象；全局符号表（Global Symbol Table） 的介入，为了解决相同符号名的链接冲突，如已存在，则后加入的符号被忽略 重定位和初始化 动态链接器根据进程的全局符号表，对需要重定位的位置进行修正，如果某个共享对象有 .init 段，那么动态链接器会执行，实现共享对象 特有的 初始化过程（常见的，C++全局/静态对象的构造就在此初始化） 实现： 普及：内核在装载完 ELF 可执行文件以后，就返回到用户空间，将控制权交给程序的入口。对于静态链接，入口地址是 e_entry 制定的地址，对于动态链接，入口地址是将动态链接器映射至进程地址空间，然后把控制权交给动态链接器 普及：Linux 动态链接器：/lib/ld-linux.so.2 -&gt; /lib/ld-x.y.z.so 动态链接器在实现中的 几个问题 动态链接器本身是动态链接还是静态链接？ 答：静态链接，不依赖于任何共享对象 动态链接器本身必须是 PIC 吗？答：不一定，如果是 PIC 会简单些 动态链接器可以被当作可执行文件运行，装载地址是多少？答：0x00000000 无效的，内核 会装载它时 分配 一个 有效 的 灵活技术：显示运行时链接（Explicit Run-time Linking） 顾名思义：让程序自己在运行时控制加载制定的模块，并且可以在不需要该模块时将其卸载，操作对象是 DLL 动态链接库提供了 4 个 API 来实现，它们分别是：dlopen()，dlsym()，dlerror()，dlclose() 总结一波： 动态链接可以更加有效地利用内存和磁盘资源，可以更加方便的维护升级程序，可以让程序的重用变得更加有效和可行 装载时重定位和地址无关代码是解决绝对地址引用的两个方法，装载时重定位的缺点是无法共享代码段，但是它的运行速度较快；而地址无关代码的缺点是运行速度较慢，但它可以实现代码段在各个进程之间的共享，还介绍了 ELF 的延迟绑定 PLT 技术 .interp、.dynamic、动态符号表、重定位表等接口，它们是实现 ELF 动态链接的关键结构。动态链接器实现自举，装载共享对象，实现重定位和初始化的过程，实现动态链接，最后关键技术：显示运行时链接","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（六）","slug":"读书笔记-程序员的自我修养（六）","date":"2017-05-24T02:24:44.000Z","updated":"2017-05-24T02:27:54.000Z","comments":true,"path":"2017/05/24/读书笔记-程序员的自我修养（六）/","link":"","permalink":"https://aaronshi32.github.io/2017/05/24/读书笔记-程序员的自我修养（六）/","excerpt":"","text":"可执行文件的装载与进程前言：源文件经过编译，链接过程，生成了目标可执行文件，可执行文件只有装载到内存以后才能被CPU执行 程序 和 进程：前者理解为一些预先编译好的指令和数据集合的一个 文件，是一个 静态 的概念，后者理解为程序运行时的一个 过程，是一个 动态 的概念 每个进程都被操作系统分配以 虚拟的地址空间（Virtual Address Space） 来供运行时分配使用，如果被操作系统捕捉到进程非法访问了额外控件，将当做 非法操作强制结束进程，常见的 Windows：”进程因非法操作需要关闭” 和 Linux：Segmentation fault 限制： 对于32位系统而言，允许分配给进程最大为：4GB的地址空间使用，而64位系统，则17179869184GB，即使 4GB，操作系统也会分配占用之后，最终留给进程最大3GB的地址空间使用，再通俗一点：整个进程在执行的时候，所有代码、数据包括通过 C 语言 malloc() 等方法申请的虚拟空间之和不得超过 3GB PAE（Physical Address Extension）和 AWE（Address Windowing Extension）：Intel 扩展了地址线，由 对应的32根（32位）扩展到了 36根 地址线，为此 多出来了256MB的物理空间，使用 窗口映射 技术，变向的增大了内存空间，进而也打破了 3GB 的限制，作为一种补救地址空间不够大的非常规手段 装载的方式 原理：程序执行时所需要的指令和数据必须 在内存中 才能够正常运行 装载的方法：都是利用了 程序局部性 的原理，即某一时刻程序只用到了局部的一些执行和数据，并非全局 以时间换取空间（淘汰）：覆盖装入（Overlay） 页映射（Paging），通常32位的Intel都是用4096字节的页作为页长，假设内存为16K，那么会被分为4个页，假设程序需要32K的内存，即需要8个页的空间，那么如何在4个页中不影响程序执行的情况下，自如装载8个页呢？ 内存页的分配算法 先进先出算法（FIFO） 最少使用算法（LUR） 从 操作系统角度 看可执行文件的装载，干了三件事 创建 一个独立的虚拟地址空间（这也是进程最关键的特征，实质上是创建映射函数所需要的相应的数据结构） 读取 可执行文件头，并建立虚拟空间与可执行文件的映射关系 将CPU的指令寄存器设置成可执行文件的入口地址，启动运行 操作系统通过不断的 页错误 来不停地将程序通过虚拟地址管理器装载到物理内存，进程才得以正常运行 进程虚拟空间 分布 区分概念 段 和 页 的关系：ELF段在映射时长度应该都是系统页长度的 整数倍，如果不是，那么多余部分也会占用一个页，进而造成 浪费，实际上，操作系统装载可执行文件时，只会根据段的 权限 来区分，对于相同权限的段，把它们合并到一起当做一个段来映射 普及概念 Linux 中将进程虚拟空间中的一个段叫做 虚拟内存区域（VMA，Virtual Memory Area），Windows 叫做 虚拟段（Virtual Section） 两次合并：第一次是链接器把各个目标文件中的相同段（Section，链接视图）合并，统一存放于可执行文件中，第二次是操作系统按照权限把可执行文件中的各个段（Segement，执行视图）合并，映射到虚拟内存区域中 核心：操作系统 通过给进程空间划分出一个个 VMA 来 管理 进程的虚拟空间，基本原则是将相同 权限 属性的、有相同 映像文件 的映射成一个VMA，一个进程基本上可以分为如下几 种 VMA区域： 代码VMA，权限只读、可执行；有映像文件 数据VMA，权限可读写，可执行；有映像文件 堆VMA，权限可读写、可执行；无映像文件，匿名，可向上扩展 栈VMA，权限可读写、不可执行；无映像文件，匿名，可向下扩展 一些限制 堆的 最大 申请数量：受操作系统版本、程序本身大小、用到的动态/共享库数量、大小、程序栈数量、大小等因素影响（随机地址空间分布技术） 段地址 对齐：受页和段的长度 整数倍映射 问题，Unix采取了 段合并 映射的方法：各个段接壤部分共享一个物理页面 进程栈 初始化：进程初始化启动之前，一些系统环境变量和进程的运行 参数 会提前保存到 Stack VMA 中，程序的库部分会把堆栈里面的初始化信息中的参数信息传递给 main() 的 argc 和 argv 两个参数 大致了解：Linux 内核装载 ELF 过程简介 bash 进程会调用 fork() 系统调用创建一个新的进程，然后新的进程调用 execve() 系统调用加载执行指定的 ELF 文件 检查 ELF 可执行文件格式的有效性，比如魔数、程序头表中段的数量 寻找动态链接的 .interp 段，设置动态链接器路径 根据 ELF 可执行文件的程序头表的描述，对 ELF 文件进行映射，比如代码、数据、只读数据 初始化 ELF 进程环境，动态链接准备 将系统调用的返回地址修改成 ELF 可执行文件的入口点 大致了解：Windows PE 的装载 过程简介 先读取文件的第一个页，包含了 DOS 头、PE文件头 和 段表 检查进程地址空间中，目标地址是否可用 使用段表中提供的信息，将 PE 文件中所有的段一一映射到地址空间中相应的位置 如果装载地址不是目标地址，则进行 Rebasing 装载所有 PE 文件所需要的 DLL 文件 对 PE 文件中的所有导入符号进行解析 根据 PE 头中制定的参数，建立初始化栈和堆 建立主线程并启动进程","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（五）","slug":"读书笔记-程序员的自我修养（五）","date":"2017-05-18T02:37:18.000Z","updated":"2017-05-18T02:39:00.000Z","comments":true,"path":"2017/05/18/读书笔记-程序员的自我修养（五）/","link":"","permalink":"https://aaronshi32.github.io/2017/05/18/读书笔记-程序员的自我修养（五）/","excerpt":"","text":"静态链接 到底做了什么？ 模块间符号的引用，其实就是链接器的主要职责，即把各个模块之间相互引用的部分都处理好，使得各个模块之间能够正确地衔接，通俗一点来讲：两个目标文件如何链接成一个可执行文件。实质 上是将各个 目标文件 中的 地址，结合其他目标文件，包括了 运行时库（Runtime Library），确定其正确的地址，以下： 任务一： 空间与地址分配（Address and Storage）：合并-重组各个目标文件的段，以完成空间和地址的分配 共识：可执行文件中的代码段和数据段都是由输入的目标文件中合并而来的 【摒弃】 方案一：按序叠加：直接将各个目标文件中的相应段依次叠加到一起，缺点就是：内存空间大量碎片化。 方案二：相似段合并：1. 空间与地址分配：先扫描所有输入的目标文件，获取各个段的长度，属性，位置，将输入目标文件中的符号表中所有的符号定义和引用统一放到一个 全局符号表 中。2. 符号解析与重定位：根据第一部收集的所有信息，进行符号解析、重定位、调整代码中的地址（增加偏移量） 等 任务二： 符号解析与重定位（Resolution and Relocation） 重定位 ：输入目标文件中，凡是引用了 外部符号，全用 0x0000000 这 4 个字节来代替，凡是引用了 外部函数，全用 0xFFFFFFFC 这四个字节来代替。链接器负责把目标文件被外部引用的函数符号的 偏移量，来 替换 这些初始代替量，链接写入输出 可执行文件 中 重定位表 ：链接器用于识别哪些引用的符号或者函数需要重定位，使用 objdump -r a.o 来查看 符号解析 ：之所以链接是因为目标文件中用到的符号被定义在其他目标文件中，使用 objdump -s a.o 来查看符号表，其中 UND 的符号就是未定义类型，就是 待链接对象，即链接的过程经历了符号解析 指令修正方式：由于不同处理器的指令千差万别，寻址的方式也不尽相同，因此在链接过程中替换目标文件地址，需要经历指令修正来确保地址有效性，其中 绝对寻址修正 后的地址为该符号的实际地址，相对寻址修成 后的地址为符号距离被修正位置的地址差 番外介绍：特殊模块：COMMON块 作用：用于链接时存放待链接的 多个命名相同的弱符号，作为储备池，因为链接器无法区分符号类型，故无法判断是否一致，因此链接器链接的选择原则是： 取占用空间最大的那个弱符号为准 任务三： 静态库链接（.a/.lib） 静态库可以简单的看成 一组目标文件的集合，用 ar lib 等工具可以看静态库的内容 静态库里面的一个目标文件只包含一个函数，避免链接函数过多导致程序臃肿，空间浪费 使用 gcc -static --verbose -fno-builtin hello.c 查看详细的链接过程，涉及到的目标文件非常多，过程也相当复杂 综上：链接的 过程控制 链接过程要确定的内容有：使用哪些目标文件？使用哪些库文件？是否在最终可执行文件中保留调试信息？输出文件格式是什么？是否导出符号表等 链接器支持 使用命令行参数控制、将链接指令存放在目标文件里、使用链接控制脚本 来完成以上内容 ld 链接器的链接脚本语法继承于 AT&amp;T 链接器命令语言的语法，风格有点像 C 语言，分为 命令语句 和 赋值语句 BFD库 的出现，旨在解决不同软硬件平台上，通过一种 统一 的接口来处理不同的目标文件格式","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（四）","slug":"读书笔记-程序员的自我修养（四）","date":"2017-05-13T02:50:07.000Z","updated":"2017-05-13T02:52:40.000Z","comments":true,"path":"2017/05/13/读书笔记-程序员的自我修养（四）/","link":"","permalink":"https://aaronshi32.github.io/2017/05/13/读书笔记-程序员的自我修养（四）/","excerpt":"","text":"链接的接口：符号 普及 : 链接的过程 实际上 就是 相互拼合 的过程，即目标文件中的（函数和变量） 地址引用。符号 是拼合剂 普及 : 我们将函数和变量统称为 符号（Symbol），函数名货变量名就是 符号名（Symbol Name） 普及：符号统一存放在目标文件的 符号表（Symbol Table） 中，每个定义的符号有一个对应的 符号值（Symbol Value） 主要关注的 链接对象 有：1. 定义在本目标文件的 全局符号，可以被其他目标文件引用 2. 在本目标文件中引用的 全局符号，却没有定义在本目标文件中，这一般叫做外部符号 ELF符号表 [.symtab] 结构详解： 逻辑结构：Elf32_Sym 结构体 对应一个符号，符号表是一个 数组，其中 符号类型和绑定信息（st_info）：类型：局部，全局，弱引用，绑定信息：位置，对象，函数，段，文件 符号所在段（st_shndx）：符号定义在本目标文件中，即对应段的下表，如不在，则跟 深入静态链接 的 COMMON块 有关 符号值（st_value）：依据符号所在段不同，值也会有不同的类型 特殊符号：有些符号并没有在你的程序中定义，但是你可以直接声明并且引用它，称之为特殊符号，比如：__executable_start，__etext，__edata，__end 这些符号当且仅在 链接过程中控制 符号的管理 修饰与函数签名：为了防止程序中的符号冲突，先后发明了 名称空间（namespace），符号修饰（Name Decoration） 等机制，以 函数签名（Function Signature） 作为符号的唯一标识，避免多个符号冲突，链接器无法识别 exten “C”：C++中用来声明或定义一个 C 符号的语法，配合 __cplusplus 来定义变量，这样就可以在 C 或 C++ 中正确的定义符号，使得链接器能够找到目标符号进行链接 弱符号和强符号： 都是针对定义而言的，编译器默认函数和初始化了的全局变量为 强符号（Strong Symbol），未初始化的全局变量为 弱符号（Weak Symbol），链接器会在链接的时候，遵循3种选择规则，来选择强符号还是弱符号：1. 相同变量强弱都定义，选择强，2. 不允许强符号被多次定义，3. 都是弱，选择占用空间最大的 弱引用和强引用：当链接器在引用决议时未发现符号就报错时，就是强引用，相反，不报错的就是弱引用。之所以有弱引用的存在，是为了使得程序可以在未发现符号的时候，使用原来库的定义，即便去掉了某个模块，也不会报错，方便解耦 调式信息 DWARF（Debug With Arbitrary Record Format） 使用命令 strip 来去掉 ELF 文件中的调试信息， 大幅度减少文件大小","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（三）","slug":"读书笔记-程序员的自我修养（三）","date":"2017-05-09T02:16:33.000Z","updated":"2017-05-09T02:20:02.000Z","comments":true,"path":"2017/05/09/读书笔记-程序员的自我修养（三）/","link":"","permalink":"https://aaronshi32.github.io/2017/05/09/读书笔记-程序员的自我修养（三）/","excerpt":"","text":"目标文件（ELF） 详解 目标文件 里有什么？ 目标文件的格式，从 结构 上讲，它是已经 编译后的可执行文件，只是还 没有经过链接 的过程，其中可能有些符号或有些地址还没有被 调整，本身就是按照可执行文件格式存储的 普及 : 可执行文件格式（Executable） 主要是 Windows 下的 PE（Portable Executable） 和 Linux 下的 ELF（Executable Linkable Format），都隶属于 COFF（Common file format） 的变种 普及 : 动态链接库（DLL，Dynamic Linking Library） 即（Windows 上的 .dll 和 Linux 上 .so），静态链接库（Static Linking Library） 即（Windows 上的 .lib 和 Linux 上的 .a） 普及 : ELF 格式的文件归为四类：可重定位文件（Relocatable File）、可执行文件（Executable File）、共享目标文件（Shared Object File） 、核心转储文件（Coredump File），可以用 file 命令查看 概述 ELF 剖析 目标文件是什么样的 ? 包含的内容： 机器指令代码、数据、符号表、调试信息、字符串等 按照信息的不同属性，存储的单位：节（Section） 和 段（Segment） 结构详解：使用 binutils 的工具 objdump 可以查看 .o 文件内部的结构 工具命令 查看主要段的基本信息：objdump -h SimpleSection.o 查看各个段的信息：readelf -S SimpleSection.o 查看ELF文件的长度：size SimpleSection.o 查看段的内容：objdump -s -d SimpleSection.o 查看ELF文件头：readelf -h SimpleSection.o 逻辑划分 代码段[ .text ]：机器指令代码（一般是执行语句） 数据段和只读数据段[ .data 和 .rodata ]：已初始化 的全局变量和局部静态变量 BSS段 [ .bss ]： 未初始化 的全局变量和局部静态变量，只是预留位置而已，值得注意的是：即便初始化为 0，也会放到 bss 中，因为未初始化的都是 0 其他段：列举几个：.comment 存放编译器版本信息，.debug 存放调试信息，.dynamic 存放动态链接信息，.hash 存放哈希表，.symtab 存放符号表，.plt/.got 动态连接的跳转表和全局入口表 等等 自定义段：GCC提供了一个扩展机制，是的程序员可以制定变量所处的段：attribute((section(“name”))) int global = 42 ，global变量即会存放于 name 作为段名的段中 物理划分 文件头（File Header）：描述文件 属性，其中定义了 ELF魔数（e_ident）、文件机器字节长度、数据存储方式、版本、运行平台（e_machine）、ABI版本、ELF重定位类型（e_type）、硬件平台、硬件平台版本、入口地址、程序头入口地址、段表的位置和长度（e_shoff）、段的数量 等，常定义于：/usr/include/elf.h 中，根据版本分为：Elf32_Ehdr 和 Elf64_Ehdr 段表（Section Table）：描述包含的所有段信息，比如每个段名、段的长度、在文件中的偏移、读写权限及段的其他属性。每个段的结构是由 Elf32_Shdr 的结构体组成，对于编译器和链接器来说，主要决定段的属性是类型（sh_type）（12种）和标志位（sh_flags）（读写可分配），如果段的类型是与连接相关的，那么段的链接信息就是控制如此的（sh_link、sh_info） 重定位表（sh_type = SHT_RELA）[.rel]：重定位代码段和数据段中那些对绝对地址的引用的位置 字符串表（sh_type = SHT_STRTAB）[.symtab]：ELF文件中用到的很多字符串，比如段名和变量名等，比较聪明的做法是，具体字符串存放在字符串表中，段中直接引用 偏移量 即可","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（二）","slug":"读书笔记-程序员的自我修养（二）","date":"2017-05-08T02:24:26.000Z","updated":"2017-05-08T02:25:36.000Z","comments":true,"path":"2017/05/08/读书笔记-程序员的自我修养（二）/","link":"","permalink":"https://aaronshi32.github.io/2017/05/08/读书笔记-程序员的自我修养（二）/","excerpt":"","text":"编译和链接 编译和链接： 通常将编译和链接合并到一起的过程称为 构建（Build），可以分解成以下 4 个步骤 预编译（Propressing） 源文件 和 头文件 被预编译器预编译成一个 .i 或者 .ii（cpp）的文件 期间主要处理那些源代码文件中的以 # 开始的预编译命令，比如 #include，#define 等 经过预编译后的 .i 文件 不包含任何宏定义，并且引用的文件已经全部被插入到 .i 文件中，删除了所有注释，处理了所有条件预编译指令（#if，#ifdef，#elif，#else，#endif），保留了所有#progma编译器命令 为此，当需要判断 宏定义，包含的头文件是否正确时，可以查看预编译后的文件来确定问题（gcc -E hello.c -o hello.i） 编译（Compilation） 编译过程就是把预编译完的文件（.i / .ii）进行一些列 词法分析，语法分析，语义分析及优化 后生成相应的 汇编代码文件（.s） 如今GCC已经把预编译和编译两个步骤合二为一了，使用叫做 cc1 的程序来完成（gcc -S hello.c -o hello.s） 实际 gcc 这个命令只是这些后台程序的 包装，它会根据不同的参数要求去调用预编译编译程序 cc1，汇编器 as，链接器 Id 汇编（Assembly） 汇编过程只是根据汇编指令和机器指令的对照表一一翻译就可以了（gcc -c hello.s -o hello.o），生成 目标文件（Object File） 链接（Linking） 目标文件 到 可执行文件，需要链接许多内容（静态链接和动态链接），后文会详细介绍 ld -static crt1.o crti.o crtbeginT.o hello.o –start-group -lgcc -lgcc_eh -lc -end-group crtend.o crtn.o 编译的过程： 编译器 从源代码（Source Code）到最终目标代码（Final Target Code）过程，做了以下 6 个步骤 词法分析 通过扫描器，运用一种类似于 有限状态机（Final State Machine） 的算法，将源代码的字符序列分割成一系列的 记号（Token） 词法分析产生的记号一般分为以下几类：关键字、标识符、字面量（包含数字、字符串等）和 特殊符号（如加号、等号），并将它们放到对应的 表 中 lex 程序可以实现词法分析，支持自定义词法规则 语法分析 通过 语法分析器（Grammar Parser），采用 上下文无关语法（Context-free Grammar） 的分析手段，将由扫描器产生的记号生成 语法树（Syntax Tree） 语法树就是以 表达式（Expression） 为节点的树，如果出现了表达式不合法，编译器就会报告语法分析阶段的错误 yacc 程序可以实现语法分析，构建一颗语法树，支持自定义语法规则 语义分析 语义分析器（Semantic Analyzer） 只对表达式完成语法层面的分析，不关心是否真正有意义，通常可以分为以下两种 静态语义（Static Semantic） 通常包括声明和类型的匹配，类型的转换，最简单的例子：赋值类型转换 动态语义（Dynamic Semantic） 一般指在运行期出现的语义相关问题，比如将0作为除数 经过语义分析之后，整个语法树的表达式都被标识了 类型，如果有些类型需要做隐式转换，语义分析程序会在语法树中 插入 相应的转换节点 中间语言生成 现代的编译器都包含 源码级优化器（Source Code Optimizer），在源代码级别有一个优化过程（比如能在编译期间确定表达式值） 直接在语法树上做优化比较困难，所以将整个语法树转换成 中间代码（Intermediate Code），比较常见的有：三地址码（Three-address Code） 和 P-代码（P-Code） 跨平台原理：中间代码使得编译器可以被分为前端和后端。编译器前端负责产生机器无关的中间代码，编译器后端将中间代码转换成目标机器代码。这样对于一些可以跨平台的编译器而言，他们可以针对不同的平台使用同一个前端和针对不同机器平台的数个后端 目标代码生成与优化 源代码级优化器产出的结果：中间代码，标志着下面的过程都属于编译器后端，包括 代码生成器（Code Generator） 和 目标代码优化器（Target Code Optimizer） 代码生成器（Code Generator） 主要是将中间代码转换成目标机器代码，依赖于目标机器的环境 目标代码优化器（Target Code Optimizer） 主要针对目标代码，在合适的寻址方式，使用位移来代替乘法运算，删除多余的指令等方面，进行优化，涉及到的技术比如： 基址比例变址寻址（Base Index Scale Addressing） 问题：此时，目标机器代码（汇编）已经生成，但是目标代码中的变量地址还没有确定，这个地址可以是本程序内定义的，也可以是其他程序模块中的，那么运行时该如何寻址？在介绍链接器之前，先说明下链接器的作用对象：目标文件都包括什么","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"读书笔记-程序员的自我修养（一）","slug":"读书笔记-程序员的自我修养（一）","date":"2017-05-06T02:50:04.000Z","updated":"2017-05-06T02:51:28.000Z","comments":true,"path":"2017/05/06/读书笔记-程序员的自我修养（一）/","link":"","permalink":"https://aaronshi32.github.io/2017/05/06/读书笔记-程序员的自我修养（一）/","excerpt":"","text":"计算机基础知识回顾 SMP（Symmetrical Multi-Processing）对称多处理器：每个CPU在系统中所处的地位和所发挥的功能都是一样的。通常情况下的程序不可能分解成多个不相干的子问题，除非商用的计算环境（充分发挥每个CPU的能力），所以多核处理器（Mutli-core Processor）和 SMP 通常是一个概念。 层次结构，计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决。开发工具与应用程序属于同一个层次，因为它们都使用运行库提供的 应用程序编程接口（Application Programming Interface）。运行库使用操作系统提供的 系统调用接口（System call Interface），通常以软件中断方式实现提供。操作系统内核层使用硬件层提供的 硬件规格（Hardware Specification） 操作系统的一个功能是提供抽象的接口，另一个主要功能是管理硬件资源（CPU，存储器（内存和磁盘）和 I/O 设备） 不要让CPU打盹：早期的CPU是 单一 服务式，简直浪费，后来出现了分时系统（Time-Sharing System），以 协作 的方式主动让闲CPU，供其他程序使用，再到现在，操作系统接管了所有硬件资源，本身运行在一个受硬件保护的级别，所有程序都以进程的方式运转，并且允许 抢占式 的CPU分配方式 设备驱动（Device Driver），操作系统 将硬件逐渐抽象成一些列的概念。比如：UNIX 中硬件设备的访问形式跟访问普通的文件形式一样，Windows 中，图像硬件被抽象成 GDI，声音和多媒体设备被抽象成 DirectX 对象等，向上提供统一的接口供程序使用，设备驱动完成向下对硬件的适配 I/O 设备：内存和磁盘的读取和写入是通过 I/O 端口寄存器来实现的，提供读取和写入的地址，来完成磁盘数据操作 即操作系统以进程概念，使得每个进程从逻辑上来看都是独占计算机资源，实质上操作系统以抢占式的 CPU 策略，I/O 抽象模型的方式来管理这些资源 操作系统是如何管理内存的 解决的问题：如何将计算机 有限 的物理内存分配给 多个 程序使用 直接分配物理内存的弊端：地址空间不隔离（程序直接访问物理内存），内存使用效率低（缺乏有效的内存管理机制，使得频繁换入换出数据，内存的使用分配以程序为单位（粒度太大）），程序运行的地址不固定（程序每次装入内存执行时，数据和指令跳转涉及到重定位问题） 解决方法： 增加中间层来 隔离，把程序给出的地址看作是一种 虚拟地址（Virtual Address），通过某些映射方法，保证任意一个程序所能够访问的物理内存区域跟另外一个程序相互不重叠，以达到地址空间隔离的效果 分段（Segmentation）：段映射，以程序所使用的的内存为单位，从虚拟地址映射到物理地址，映射结果唯一，解决了隔离和程序地址不固定的问题 分页（Paging）：页分割，把常用的数据和代码页装载到内存中，把不常用的代码和数据保存在磁盘里，当用到的时候再取出来。若用到的页不在内存中，操作系统会受到页错误（Page Fault），然后将需要的页装载到内存中。硬件级别：MMU（Memory Management Unit）用来支持虚拟地址到物理地址的转换，已集成到CPU内部 程序执行流最小单元：线程（Thread），又称为轻量级进程（Lightweight Process，LWP） 基础概念 线程私有（局部变量，函数的参数，TLS数据），线程之间共享（全局变量，堆数据，函数里的静态变量，程序代码，打开的文件，这些就是进程所有的东西） 调度（线程数&gt;处理器时（或者单处理器应对多线程时），会模拟出来一种并发的状态，即切换不同的线程行为，称之为线程调度 至少三种状态：运行，就绪，等待，以时间片的改变作为区分 调度算法：优先级调度（Priority Schedule），轮转法（Round Robin），也区分出 IO密集型线程（IO Bound Thread） 和 CPU密集型线程（CPU Bound Thread），IO 总比 CPU 能获得更高的优先级 为了防止 饿死（Starvation） 现象： 用户指定优先级/根据进入等待状态的频繁程序提升或降低优先级/长时间得不到执行而被提升优先级，也出现了 可抢占式线程（Preemption） 线程安全 竞争与原子操作：多个线程同时访问一个共享数据，为此引出 原子性（Atomic），但原子性操作只适用于简单的操作，更复杂的数据结构访问，通常用 锁 的手段 同步与锁：同步（Synchronization） 是一种机制：保证线程访问数据的原子性。锁（Lock） 是一种手段，每个线程在访问数据或资源之前，首先试图 获取（Accquire） 锁，并在访问结束之后 释放（Release） 锁 锁的演进：二元信号量（Binary Semaphore），只允许一个线程独占，其状态有两个：占用 和 非占用，可以由一个线程获取，另一个线程释放；多元信号量（N Semaphore），允许 N 个线程并发访问共享资源；互斥量（Mutex），类似于二元信号量，但谁获得谁负责释放；临界区（Critical Section）：不同于之前的信号量，互斥量（作用域为进程内的所有线程），临界区的作用范围仅限于本线程，其他线程无法获取临界区的锁，因此也就无法访问；条件变量（Condition Variable），使用条件变量可以让许多线程一起等待某个事件的发生，当事件发生时，所有线程可以一起恢复执行 读写锁（Read-Write Lock）：三种状态分别是，自由（Free），共享（Shared）和独占（Exclusive），当处于自由状态时，试图以任何一种方式获取锁都能成功，并将锁至于对应的状态。如果锁处于共享状态，其他线程以共享的方式获取锁仍会成功（读），此时这个锁分配给了多个线程。如果其他线程试图以独占的方式获取已经处于共享状态的锁，那么他将必须等在锁被所有的线程释放后，才能将锁的状态置成独占（写） 可重入（Reentrant）：即幂等性的函数，可以称为可重入函数 过度优化：使用 volatile 关键字，解决 1. 阻止编译器为了提高速度将一个变量缓存到寄存器内而不写回，2. 阻止编译器调整操作 volatile 变量的质量顺序，但还是无法阻止CPU动态调度换序 线程内部三种模型 多线程库的实现方式：对用户来说如果有三个线程在同时执行（用户态线程（User Thread）），对内核来说很可能只有一个线程（内核线程（Kernel Thread）），下面提及的模型都是 UT：KT 一对一模型：真正意义上的并发调度，缺点是受内核线程数量限制，内核线程上下文切换调度开销较大，使得用户线程执行效率低下 多对一模型：高效的上下文切换和几乎无限制的线程数量，缺点是一个用户线程阻塞了，那么所有线程都将无法执行 多对多模型：多个用户线程映射到多个内核线程，切换方便且避免了一个阻塞全局的情况","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"SonarQube - 安装使用调试","slug":"SonarQube安装使用调试","date":"2017-03-14T02:57:08.000Z","updated":"2017-03-16T08:36:02.000Z","comments":true,"path":"2017/03/14/SonarQube安装使用调试/","link":"","permalink":"https://aaronshi32.github.io/2017/03/14/SonarQube安装使用调试/","excerpt":"","text":"最近研究了下SonarQube工程，发现其如此强大，可以用于质量保证的任何阶段，刚好由于项目需要建立一个完整的基础质量保证”设施”，所以快马加鞭，赶紧安装部署了下，这里总结一些问题和经历 安装与运行SonarQube分为两个部分：Server 和 Scanner，其中 Server：https://www.sonarqube.org/downloads/ Scanner：https://docs.sonarqube.org/display/SCAN/Analyzing+Source+Code 其余安装环境就不赘述了：MySQL 和 Java 参考这篇文章：http://blog.csdn.net/hunterno4/article/details/11687269 问题 在配置了 sonar-project.properties 之后，运行 sonar-scanner 报如下错误： 1org.sonarqube.ws.client.HttpException: Error 500 on http://localhost:9000/api/ce/submit?projectKey=xxx&amp;projectName=yyy : &#123;&quot;errors&quot;:[&#123;&quot;msg&quot;:&quot;An error has occurred. Please contact your administrator.&quot;&#125;]&#125; 解决方法如下： 查看 /logs/web.log 得知详细原因是：Packet for query is too large (6082109 &gt; 4194304). You can change this value on the server by setting the max_allowed_packet&#39; variable 修改 MySQL 的配置，用命令行登入执行：show VARIABLES like &#39;%max_allowed_packet%&#39;; SET GLOBAL max_allowed_packet=268435456; 重启 MySQL 和 Sonar-Server 服务 尝试分析一个工程，发现已经成功打通的整套流程，SonarQube的高集成特性以及丰富的插件，可以根据实际项目需要丰富的定制。接下来我会围绕它打造一个后期的基础质量保障“设施”，首先来看看如何调试自定义扩展插件 扩展插件官方给出了扩展插件的示例：https://github.com/SonarSource/sonar-custom-plugin-example，下载下来导入工程，可以清晰的看到 org.sonarsource.plugins.example， 插件入口目录，相当于 Main 函数的作用 org.sonarsource.plugins.example.hooks ，工具钩子，可以在各个阶段自定义一些需要的操作，继承自：PostJob/PostProjectAnalysisTask org.sonarsource.plugins.example.languages ，自定义语言支持，继承自：AbstractLanguage org.sonarsource.plugins.example.measures， 自定义质量阀的衡量标准，继承自：Metrics org.sonarsource.plugins.example.rules，自定义代码规则，继承自：Sensor org.sonarsource.plugins.example.settings，自定义设置，继承自：Sensor org.sonarsource.plugins.example.web，自定义网页显示部分，继承自：web.xxx 以上这些自定义的各个模块，分别作用于 SonarQube 的基本流程里，即： 12345678910111213public void execute(Project project) &#123; ... sensorsExecutor.execute(sensorContext); decoratorsExecutor.execute(); persistenceManager.dump(); persistenceManager.setDelayedMode(false); if (project.isRoot()) &#123; if (updateStatusJob != null) &#123; updateStatusJob.execute(); &#125; postJobsExecutor.execute(sensorContext); &#125; ... 首先，初始化整个分析过程，包括加载所有的分析任务，其次，分析这些任务（Sensor和Decorator），并且把结果存储到数据库，最后，执行postjob，做相关的一些分析，因此需要根据插件作用需要，可以自行在各个阶段模块中进行扩展 调试官方提供了调试方法：https://docs.sonarqube.org/display/DEV/Build+Plugin#BuildPlugin-Debug 这里总结下步骤，避免踩坑： 在 conf/sonar.properties 中设置sonar.web.javaAdditionalOpts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8001 将插件打包部署到 extensions/plugins 中 如果你安装了SonarQube服务，请先关掉，然后用 bin/StartSonar启动，可以检查是否出现：Listening for transport dt_socket at address: 8001 在IDE中新建远程调试，打上断点，启动调试，打开localhost:8001，此时IDE会进到debug模式 好了，学会了调试，接下来就可以进一步打造我们的插件了","categories":[],"tags":[{"name":"工具","slug":"工具","permalink":"https://aaronshi32.github.io/tags/工具/"}],"keywords":[]},{"title":"疑难杂症-解决MSYS2在Window上越用越慢的方法","slug":"疑难杂症-解决MSYS2在Window上越用越慢的方法","date":"2017-02-27T09:45:50.000Z","updated":"2017-03-16T08:39:44.000Z","comments":true,"path":"2017/02/27/疑难杂症-解决MSYS2在Window上越用越慢的方法/","link":"","permalink":"https://aaronshi32.github.io/2017/02/27/疑难杂症-解决MSYS2在Window上越用越慢的方法/","excerpt":"","text":"最近折腾电脑真是费了不少脑细胞，谁让咱用着 window 还操着 linux 的心呢，事情起因是因为习惯了在 cmd 中使用 linux 的命令，但是 cygwin 新版的界面真是不该恭维的美感，所以就把 git/bin 目录加到了 PATH 中，这样一举两得，谁知 git for windows 现在用的是 MSYS2 的环境，运行个 ls，grep要等上 1 分钟才能出结果，于是乎忍无可忍，终于在今天解决了这个问题 问题描述解决 MSYS2 在使用过程中越用越慢的问题 解决方法123456789101112mkpasswd -l -c &gt; /etc/passwdmkgroup -l -c &gt; /etc/group# 如果是window系统就在 git/etc 下面执行# 修改 /etc/nsswitch.conf，其中passwd: files #dbgroup: files #db 参考 https://gist.github.com/k-takata/9b8d143f0f3fef5abdab http://bjg.io/guide/cygwin-ad/ 终于，系统恢复了丝滑版的顺畅~","categories":[],"tags":[{"name":"疑难杂症","slug":"疑难杂症","permalink":"https://aaronshi32.github.io/tags/疑难杂症/"}],"keywords":[]},{"title":"机器学习","slug":"Tensorflow学习-机器学习","date":"2017-02-13T10:27:59.000Z","updated":"2017-02-20T10:48:02.000Z","comments":true,"path":"2017/02/13/Tensorflow学习-机器学习/","link":"","permalink":"https://aaronshi32.github.io/2017/02/13/Tensorflow学习-机器学习/","excerpt":"","text":"分类 机器学习 有监督学习(Supervised) 无监督学习(UnSupervised) 理解 有一组称之为”正确”的数据提供给算法，用于监督每次的训练成果 从数据集中发掘出规律，从而将数据进行某种程度划分 问题 回归预测（Regression Predict）特征分类（Feature Classification） 聚类（Cluster） 线性回归 模型表示 关键词：训练集（Training Set）| 学习算法（Learning Algorithm）| 假说（Hypothesis） 通过学习算法找到某种假说使得训练集的数据呈现该假说规律，即：H maps from TraningSet 如何检测假说的效果，使用成本函数（cost function or loss function），有时候也叫损失函数，通常来说，假说越准确，越接近真实，其值就越小。成本函数的出现通常伴随着目标，即 When cost function defined，the goal has appearanced 线性回归通常使用的成本函数和目标为： 学习算法： 梯度下降法（Gradient Descent） 梯度下降法的缺点包括：1）靠近极小值时速度减慢，2）直线搜索可能会产生一些问题，3）可能会“之字型”地下降。 如何调节机器学习的结果： 特征缩放（Feature Scaling） 解释：在运用一些机器学习算法的时候不可避免地要对数据进行特征缩放（feature scaling），比如：在随机梯度下降（stochastic gradient descent）算法中，特征缩放有时能提高算法的收敛速度","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://aaronshi32.github.io/tags/人工智能/"}],"keywords":[]},{"title":"Tensorflow学习-环境搭建","slug":"Tensorflow学习-环境搭建","date":"2017-02-08T03:34:59.000Z","updated":"2017-02-10T06:55:24.000Z","comments":true,"path":"2017/02/08/Tensorflow学习-环境搭建/","link":"","permalink":"https://aaronshi32.github.io/2017/02/08/Tensorflow学习-环境搭建/","excerpt":"","text":"耳闻 Tensorflow 已经很久了，一直没有时间亲自动手体验，忙里抽闲把环境都配置好了，以此来记录学习的开始 环境配置 VMware Player 12 （免费） Ubuntu 16.04 LTS Anaconda 4.3.0 Tensorflow Library Sublime Text + Plugin 翻墙 基于 Anaconda 搭建 Tensorflow 的环境非常方便，避免GFW的干扰，提供一个环境网站：https://mirrors.tuna.tsinghua.edu.cn/ 创建虚拟机的步骤就不赘述了，用过VMware的很简单，记得安装VMware Tools ，这个很关键 下载好 Anaconda 之后，有 2 和 3 两个版本，分别对应 python 2.7 和 python 3.5，chmod 之后就可以一路安装了，记得添加到PATH中，这样默认的python环境就升级了 打开终端，输入以下命令 123456789$ conda create -n tensorflow python=2.7$ source activate tensorflow(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl(tensorflow)$ pip install --ignore-installed --upgrade $TF_BINARY_URL$ source deactivate 下载安装 Sublime Text，安装 Package Control 之后依次添加以下插件： Anaconda 其中 Anaconda 配置参考：http://www.cnblogs.com/nx520zj/p/5787393.html 去掉代码检测提示框：https://segmentfault.com/q/1010000002415020 这样，Tensorflow 的环境就搭建成功了，总体来说还是非常方便的，保证一个通畅的网络，翻墙很关键！ 试运行一个线性拟合 Demo 点此下载：line.py 得到如下输出：","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://aaronshi32.github.io/tags/人工智能/"}],"keywords":[]},{"title":"Tensorflow学习-神经网络","slug":"Tensorflow学习-神经网络","date":"2017-02-08T03:34:59.000Z","updated":"2017-02-15T10:33:20.000Z","comments":true,"path":"2017/02/08/Tensorflow学习-神经网络/","link":"","permalink":"https://aaronshi32.github.io/2017/02/08/Tensorflow学习-神经网络/","excerpt":"","text":"学校里人工智能的课程，都会以神经网络作为入门，因此，也以神经网络（NNs）作为回顾和新的学习开始 首先来回忆几个概念 神经元神经元是神经网络中的一个基本单元，作用是：求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果 这个非线性传递函数也称之为：激励函数（ReLu） 因此对于一个基本单位神经元，他的目的就是汇合其所有的输入（外界刺激），与该神经元上的所有权值相乘（神经敏感度），通过激励（激励函数）产生相应的反应（标量结果） 神经网络神经网络由多个神经元组成，通过层的概念划分，可以分为输入层，隐含层和输出层。 神经元之间的信息传递强度，是通过权向量来定义的，结合一定的训练学习算法，权向量会不断的进行调整，即学习算法（learning algorithm）会不断的调整神经元的敏感度，让神经元在训练过程中产生不同的刺激反应 同时，神经网络结合学习算法，依赖于大量的数据来训练，每一次的训练都需要有一个评估方法，称之为成本函数（cost function），用来定量评估根据特定输入值， 计算出来的输出结果，离正确值有多远，结果有多靠谱，换句话说就是神经网络的反应与预期是否一致 为此，一个神经网络的基本可变关键因素就是：网络拓扑设计（输入层，隐含层和输出层的关系），学习算法（以何种方式来调整神经元敏感度），激励函数（神经元对外界刺激做出如何的反应），成本函数（衡量评估神经网络距离预期的差距），当然，随着深入的研究，神经网络还有许多高级可变因素，比如，dropout等 成果 最常用的两个激活函数：Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid） 实战来源：维基百科-人工神经网络 斯坦福大学UFLDL-神经网络","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://aaronshi32.github.io/tags/人工智能/"}],"keywords":[]},{"title":"2017书单","slug":"2017书单","date":"2017-01-09T01:27:38.000Z","updated":"2017-12-21T08:56:58.000Z","comments":true,"path":"2017/01/09/2017书单/","link":"","permalink":"https://aaronshi32.github.io/2017/01/09/2017书单/","excerpt":"","text":"思想类 《世界很大，幸好有你》 杨澜 《局座的悄悄话》 张召忠 《有味》 汪涵 《道德经》 老子 《你从未真正拼过》 Linkedin 小说类 《摆渡人》 克莱儿麦克福尔 《殉罪者》 雷米 技术类 《Tensorflow》 《程序员的自我修养》 潘爱民 《利用Python进行数据分析》 Wes McKinney 《Spring Boot 实战》 Craig Walls","categories":[],"tags":[{"name":"日常","slug":"日常","permalink":"https://aaronshi32.github.io/tags/日常/"},{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"Markdown解析器-Showdown","slug":"Markdown解析器-Showdown","date":"2016-12-01T07:26:35.000Z","updated":"2017-03-14T03:04:50.000Z","comments":true,"path":"2016/12/01/Markdown解析器-Showdown/","link":"","permalink":"https://aaronshi32.github.io/2016/12/01/Markdown解析器-Showdown/","excerpt":"","text":"由于工作中需要将 markdown 格式的文件转换成 html，因此抽空研究了下业界的工具Showdown，Showdown 是一个可以将标准 markdown 规范解析生成基础 html 的工具，使用起来很简单，通过命令：showdown makehtml -i &lt;input&gt; -o &lt;output&gt;即可完成转化，详细的介绍请参考官方文档 Showdown，在此就不过多赘述了。 Extensions 与 Listeners通过研究发现，当需要解析一些”自定义”的 markdown 规范时，Showdown 提供了清晰良好的代码格式，方便我们扩展，包括extensions, listener的方式，可以进行注册，监听，来看 Showdown 的入口文件，里面有这样三行代码： 12345678910//src/cli/makehtml.cmd.jsfunction run() &#123; var converter = new showdown.Converter(argv); ... input = fs.readFileSync(argv.i, enc); ... output = converter.makeHtml(input);&#125; 可以分析出，converter通过接受参数，来配置各种extensions和listener，然后直接从源文件中读取，经过 makeHtml 一转换就变成 html 了 官方也提供了几个现有的 extensions， 如 table-extension prettify-extension 等，这些现有的extensions都是对 markdown 的元素进行了自定义的解析和渲染，如果是基于标准的 markdown 语法，使用这种方式进行自定义渲染就非常方便了 关于 Listeners 可以参考下面的工作原理章节 工作原理关于工作原理，可以用一句话来概括：Showdown使用正则表达式解析 markdown 语法，然后直接渲染成 html，注意这里面有两个关键词： 正则表达式解析 来看一个 Parser 的源码片段： 123456789101112131415161718192021222324// 斜体和黑体的解析器：italicsAndBoldshowdown.subParser('italicsAndBold', function (text, options, globals) &#123; 'use strict'; text = globals.converter._dispatch('italicsAndBold.before', text, options, globals); if (options.literalMidWordUnderscores) &#123; //underscores // Since we are consuming a \\s character, we need to add it text = text.replace(/(^|\\s|&gt;|\\b)__(?=\\S)([\\s\\S]+?)__(?=\\b|&lt;|\\s|$)/gm, '$1&lt;strong&gt;$2&lt;/strong&gt;'); text = text.replace(/(^|\\s|&gt;|\\b)_(?=\\S)([\\s\\S]+?)_(?=\\b|&lt;|\\s|$)/gm, '$1&lt;em&gt;$2&lt;/em&gt;'); //asterisks text = text.replace(/(\\*\\*)(?=\\S)([^\\r]*?\\S[*]*)\\1/g, '&lt;strong&gt;$2&lt;/strong&gt;'); text = text.replace(/(\\*)(?=\\S)([^\\r]*?\\S)\\1/g, '&lt;em&gt;$2&lt;/em&gt;'); &#125; else &#123; // &lt;strong&gt; must go first: text = text.replace(/(\\*\\*|__)(?=\\S)([^\\r]*?\\S[*_]*)\\1/g, '&lt;strong&gt;$2&lt;/strong&gt;'); text = text.replace(/(\\*|_)(?=\\S)([^\\r]*?\\S)\\1/g, '&lt;em&gt;$2&lt;/em&gt;'); &#125; text = globals.converter._dispatch('italicsAndBold.after', text, options, globals); return text;&#125;); 每个 Parser 的开始和结束，都抛出了相应的事件供监听，这就是 listener 扩展的机制，每次处理的结果都是返回 text 文本，可以看到在解析器中，对输入的文本进行正则表达式的匹配并直接替换成了相应的 html 元素 直接渲染 所谓直接渲染，就是上面代码中的replace，这样的处理为后续的需求埋下隐患： 如果某个 markdown 片段需要作为整体的渲染，则无法满足 如果对渲染之后的 html 进行样式更改，加入css，则无法满足 可以看出，replace 替换的仅仅是 html 基本元素样式，且没有 id，class 等信息，因此极大程度上约束了生成的 html 样式，并不利于满足一些实际需求，然而 Showdown 的代码架构，将各个 markdown 元素分别定义了单独的解析器，这就为我们提供了一个思路，可以根据自定义的 markdown 的格式通过自定义的 parser 解析出来就好了，例如： 123456### start 修订记录 ###1.0date: 11.32.0date: 12.9### end 修订记录 ### 解析器可以以 start 和 end 作为判断，整体将修订记录渲染成想要的格式，因此，对 Showdown 工程有以下重构想法： 解析和渲染解耦，不通过 replace 直接完成, 而是通过 taffydb 将解析出来的数据进行存储(参考jsdoc)，这样做的好处是：规则制定者可以自定义 markdown 语法片段并通过解析器解析出来，然后存放到数据库中，不需要关心接下来如何渲染，前端工程师可以直接从数据库中取出相应的片段信息，配合css，进行定制化渲染。借助于中间数据库的解耦，使得最后生成出来的html可以加入css样式，从而满足一切定制化需求，即 解析 - 数据 -渲染 总结以上是在结合工作需求的基础上研究 Showdown 工具的一些想法，还是软件工程里的一句老话：从需求出发的改动才附有意义","categories":[],"tags":[{"name":"工具","slug":"工具","permalink":"https://aaronshi32.github.io/tags/工具/"}],"keywords":[]},{"title":"前端资源打包工具-WebPack","slug":"前端资源打包工具-WebPack","date":"2016-11-30T01:08:26.000Z","updated":"2016-11-30T05:48:24.000Z","comments":true,"path":"2016/11/30/前端资源打包工具-WebPack/","link":"","permalink":"https://aaronshi32.github.io/2016/11/30/前端资源打包工具-WebPack/","excerpt":"","text":"WebPack 是一个前端资源打包工具，借助于Loader它可以将任何前端资源(js/image/css/…)统一转换为原生JS 代码，通过静态分析构建模块之间的依赖关系，将涉及到资源分类打包成不同的模块，这些模块在浏览器，Nodejs 中可以按需进行加载，可提升加载效率。同时，WebPack 的打包也非常高效，Webpack 使用异步 I/O 和多级缓存提高运行效率，这使得 Webpack 能够以令人难以置信的速度快速增量编译。 12官方文档：http://webpack.github.io/docs/中文文档：http://webpackdoc.com/index.html 以上两个文档已经非常全面的阐述了 WebPack 强大的功能，下面针对几个关键点进行实战 解释一切资源：Loaders由于 WebPack 打包的来源只能是原生 JS 代码，因此许多静态资源(图片/CSS), 非JS写的代码(Coffee 和 JSX)需要以一种转换方式转换成 JS 代码，为此 Loader 应运而生 Loader 作为一个单独的模块，其具有以下几个特征： 链式处理方式，资源可被多个 Loader 相互转化，但最终的结果是 JS 支持同步或异步的处理方式 Function 级别存在，可运行于NodeJS甚至其他的环境中 支持正则表达式等特性，可通过npm来发布 总而言之，Loader 作为一个JS代码的转义器，有任何需要被 WebPack 打包的非原生 JS 资源，都可以通过内置/自定义的 Loader 加以转换，供打包使用 使用方式：npm install xxx-loader --save-dev Loader列表参考： List of Loaders 丰富的插件：Plugins在打包过程中，WebPack 还允许借助各种插件，对打包过程、生成的结果进行处理，比如最简单的借助于 UglifyJS 实现压缩和混淆，使用如下命令： new webpack.optimize.UglifyJsPlugin([options]) 即可生成压缩混淆之后的打包文件，同理，内置的插件可以做更多的事情 使用方式：在配置文件中配置 Plugin列表参考： List of Plugin 配置文件：webpack.config.js在大型项目中，WebPack 通常借助于配置文件 webpack.config.js 将打包过程所需执行的步骤，插件统一进行管理，同时方便发布系统实时监控打包，配置文件的内容通常如下： 1234567891011121314151617181920212223//WebPack配置文件var webpack = require('webpack');module.exports = &#123; entry: './entry.js', output: &#123; path: __dirname, filename: 'bundle.js' &#125;, module: &#123; loaders: [&#123; test: /\\.css$/, loader: 'style!css' &#125;] &#125;, plugins: [ new webpack.optimize.UglifyJsPlugin(&#123; mangle: false, compress: false &#125;), new webpack.BannerPlugin('This file is created by ChenS') ]&#125; 可以看出，entry中配置要打包的文件，output打包之后的文件输出路径，module里配置各种 Loader，以 test 的正则表达式匹配资源文件格式，Loader 传入名称，在plugins里配置各种插件以及选项，比如上述 UglifyJsPlugin 禁用了混淆和压缩，有了这个配置文件，在使用 WebPack时就可以直接运行webpack命令，一切轻松搞定 以上Demo工程可点此下载：node-webpack-demo 总结WebPack 的基础实战在上面分析完了，更多完整功能用法，请参考WebPack","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://aaronshi32.github.io/tags/JavaScript/"},{"name":"工具","slug":"工具","permalink":"https://aaronshi32.github.io/tags/工具/"}],"keywords":[]},{"title":"读书笔记-Effective JavaScript","slug":"读书笔记-Effective JavaScript","date":"2016-11-17T09:53:00.000Z","updated":"2016-12-01T02:35:48.000Z","comments":true,"path":"2016/11/17/读书笔记-Effective JavaScript/","link":"","permalink":"https://aaronshi32.github.io/2016/11/17/读书笔记-Effective JavaScript/","excerpt":"","text":"让自己习惯JavaScript 变量作用域 使用函数 对象和原型 数组和字典 库和API设计 并发 让自己习惯 JavaScript 了解你使用的JavaScript版本 Web浏览器，并不支持让程序员指定JS的版本来执行代码 严格模式（”use strict”），允许你选择在受限制的JS版本中禁用一些问题较多或者出错的特性 不要试图将严格模式和非严格模式的文件连接在一起，建议在严格模式下编写代码（ES5 中增加的，兼容ES3） 将其自身包裹在立即调用的函数（IIFE）表达式中的方式连接多个文件，是比较安全的 理解JavaScript的浮点数 大多数编程语言都有几种数值型数据类型，但JS只有一种，就是 number，也就是常理解的 double(双精度浮点数) JS中所有 位运算符 的工作方式是：将操作数转换为整数，然后使用整数位模式进行运算，最后将结果转换为标准的JS浮点数（3步），即将数字视为32位的有符号整数 浮点数的操作是出了名的不准确 0.1 + 0.2 // 0.300000000000004 0.1 + 0.2 + 0.3 ≠ 0.1 +（0.2 + 0.3） 当心浮点运算中的精度陷阱，一般都是转化为最小单位（尽可能转换成整数）去计算，因为整数在表示时不需要舍入 当心隐式的强制转换 类型错误可能被隐式的强制转换所隐藏 重载运算符 + 是进行加法运算还是字符串连接操作取决于其参数类型 对象通过valueOf方法强制转换为数字，通过toString方法强制转换为字符串 具有valueOf方法的对象应该实现toString方法，返回一个valueOf方法产生的数字字符串表示 JS中有且只有7种假值：false、0、-0、””、NaN、null 和 undefined 测试一个值是否为未定义的值，应该使用 typeOf或者与undefined进行比较而不是真值运算 原始类型优于封装对象 5个原始类型：布尔值、数字、字符串、null 和 undefined 隐式封装：当对原始值提取属性和进行方法调用时，他表现得就像已经使用了对应的对象类型封装了该值一样。例如：String的原型对象有一个toUpperCase的方法，你可以对这个原始字符串值调用这个方法 当做相等比较时，原始类型的封装对象 ≠ 原始类型值 获取和设置原始类型值的属性会隐式地创建封装对象，字符串对象的引用在用完之后立即被销毁，所以不能给字符串添加属性 &quot;hello&quot;.property = 1; &quot;hello&quot;.property; // undefined 避免对混合类型使用 == 运算符 当参数类型不同时，==运算符应用了一套难以理解的隐式强制转换规则 当使用 === 运算符时，不需要涉及任何的隐式强制转换就明白你的比较运算符 当比较不同类型的值时，使用你自己的显示强制转换使程序的行为更清晰 null == undefined // true string/number/boolean == Date // Date.toString() --&gt; Date.valueOf() string/number/boolean == 非 Date // 非 Date.valueOf() --&gt; 非 Date.toString() 了解分号插入的局限：分号插入原则 仅在 } 标记之前、一个或多个换行之后和程序输入的结尾 仅在紧接着的标记不能被解析的时候推导分号 在以（、[、+、- 或 / 字符开头的语句钱决不能省略分号 当脚本连接的时候，在脚本之间显式插入分号 在 return、throw、break、contine、++ 或 – 的参数之前决不能换行 分号不能作为for循环的头部或空语句的分隔符 视字符串为16位的代码单元序列 JS字符串有16位的代码单元组成，而不是由Unicode代码点组成 变量作用域 尽量少用全局对象 多使用局部变量 避免对全局对象添加属性 使用全局对象来做平台特性检测 始终声明局部变量 使用 var(函数作用域) 和 let(for循环局部作用域) 来声明新的局部变量 考虑使用lint工具帮助检查未绑定的变量 不要使用with 熟练掌握闭包 JavaScript允许你引用在当前函数以外定义的变量（内部函数可以访问外部成员变量，引了，该函数就成为闭包） 闭包比创建它们的函数有更长的生命周期 闭包在内部存储其外部变量的而引用，并能读写这些变量 function sandwichMaker(){ var magicIngredient = &quot;peanut butter&quot;; function make(filling){ return magicIngredient + &quot; and &quot; + filling } return make; } var f = sandwichMaker(); f(&apos;jelly&apos;) // peanut butter and jelly f(&apos;bananas&apos;) // peanut butter and bananas //那些在其所涵盖的作用域内跟踪变量的函数，称为闭包，make函数就是一个闭包 理解变量生命提升 JavaScript隐式地提升（hoists）声明部分到封闭函数的顶部，而将赋值留在原地，换句话说，变量的作用域是整个函数，但仅在var语句出现的位置进行赋值 重声明变量被视为单个变量 请手动提升局部变量的声明，从而避免混淆 使用立即调用的函数表达式（IIFE）创建局部作用域 闭包存储的是其外部变量的引用，而不是值 function wrapElements(a){ var result = [], i, n; for(i = 0, n = a.length ; i &lt; n ; i++){ result[i] = function(){ return a[i]; }; } return result; } 由于function中引用了变量i，所以该函数为每个result[i]都创建了一个闭包，且保存着对i的引用 所以i = a.length;即result[1...a.length-1]都等于a[a.length],即 undefined 立即调用的函数表达式 function wrapElements(a){ var result = []; for(var i = 0, n = a.length ; i &lt; n ; i++){ (function(j){ result[i] = function(){ return a[i]; }; })(i); } return result; } 当心命名函数表达式笨拙的作用域 var f = function(){} 当心局部块函数声明笨拙的作用域 始终将函数声明至于程序或被包含的函数的最外层，以避免不可移植的行为 局部块函数声明: 使用var声明和有条件的赋值语句代替有条件的函数声明 避免使用eval创建局部变量 不要赋予外部调用者能改变函数内部作用域的能力，即 函数内部过分依赖于 动态绑定（eval(“var x = 1”)） 如果 eval 函数代码可能创建全局变量，将此调用封装到嵌套的函数中以防止作用域污染 var y = &quot;global&quot;; function test(src){ (function(){ eval(src); })(); return y; } test(&quot;var y = &apos;local&apos;;&quot;) // global test(&quot;var z = &apos;local&apos;;&quot;) // global 间接调用eval函数优于直接调用(Node里面没有臭名昭著的eval) 尽可能间接调用eval函数，而不要直接调用eval函数：（0，eval）(src) 使用函数 理解函数调用、方法调用及构造函数调用之间的不同 在方法调用中是由调用表达式自身来确定this变量的绑定。绑定到this变量的对象被称为调用接收者(receiver) 方法调用将被查找方法属性的对象作为调用接收者 函数调用将全局对象作为其接受者，一般很少使用函数调用语法来调用方法 构造函数需要通过new运算符调用，并产生一个新的对象作为其接收者 熟练掌握高阶函数 高阶函数就是那些将函数作为 参数 或者 返回值 的函数 当发现自己在重复地写一些相同的模式时，学会借助于一个高阶函数（提炼逻辑，让callback作为参数传递）可以使代码更简洁、更高效、更可读，学会发现可以被高阶函数所取代的常见的编码模式 掌握现有库中的高阶函数，比如数组的map 使用call方法自定义接受者来调用方法 f.call（obj,arg1,arg2,arg3）与 f（arg1,arg2,arg3）不同的是第一个参数提供了一个显式的接收者对象 使用 call 方法自定义接收者来调用函数 使用 call 方法可以调用在给定的对象中不存在的方法 使用 call 方法定义高阶函数允许使用者给回调函数指定接收者 使用apply方法通过不同数量的参数调用函数 使用 apply 方法制定一个可计算的参数数组来调用可变参数的函数，apply方法需要一个参数数组，然后将数组的每一个元素作为调用的单独参数调用该函数 使用 apply 方法的第一个参数给可变参数的方法提供一个接收者 使用arguments创建可变参数的函数 JavaScript给每个函数都隐式得提供了一个名为arguments的局部变量。arguments对象给实参提供了一个类似的数组接口，它为每个实参提供了一个索引属性，还包含一个length属性用来指示参数的个数，从而可以通过遍历arguments对象的每个元素来实现可变元数的函数 如果提供了一个便利的可变参数的函数，也最好提供一个需要显示指定数组的固定元数版本 function average(){ return averageOfArray(arguments); } 永远不要修改 arguments 对象 使用 [].slice.call(arguments) 将 arguments 对象复制到一个真正的数组中再进行修改 使用变量保存 arguments 的引用 先用变量绑定到 arguments，明确作用域之后，再在嵌套函数中引用它 使用 bind 方法提取具有确定接受者的方法 提取一个方法不会将方法的接收者绑定到该方法的对象上 当给高阶函数传递对象方法时，使用匿名函数在适当的接收者上调用该方法 使用 bind 方法创建绑定到适当接收者的函数 使用 bind 方法实现函数柯里化(不懂) 将函数与其参数的一个子集绑定的技术成为函数柯里化 function.bind(null, arg1, arg2) 使用闭包而不是字符串来封装代码 当将字符串传递给eval函数以执行时，不要在字符串中包含局部变量的引用，容易在函数中引起冲突 优先接收函数，而不是eval执行的字符串，即 function repeat(n, action){ for(var i = 0 ; i &lt; n ; i++){ action(); } } 不要信赖函数的toString方法 在不同的引擎下调用toString方法的结果可能不同，所以绝不要信赖函数源代码的详细细节 toString方法的执行结果并不会暴露存储在闭包中的局部变量值 通常情况下，应该避免使用函数对象的toString方法 避免使用非标准的栈检查属性 调用栈实质当前正在执行的活动函数链 不要使用非标准的 arguments.caller 和 arguments.callee 属性 对象和原型 理解 prototype、getPrototypeOf 和 proto 之间的不同(有一个重点图) C.prototype 属性是 new C() 创建的对象的原型 Object.getPrototypeOf(obj)是ES5中检索对象原型的标准函数 Object.getPrototypeOf(u) === User.prototype obj.proto 是检索对象原型的非标准方法（不支持ES5的情况下采用） 类：是由一个构造函数 和 一个关联的原型 组成的一种设计模式 function User(name, passwordHash){ this.name = name; this.passwordHash = passwordHash; } User.prototype.toString = function(){ return &quot;sdsds&quot;; } var instance = new User(&quot;abc&quot;,&quot;abc&quot;); Function.prototype(.call/.bind/.apply) --&gt; User(.prototype) --&gt; User.prototype(.toString) --&gt; instance(.name/.passwordHash) 使用 Object.getPrototypeOf 函数而不要使用 proto 属性 获取对象原型的标准API是 Object.getPrototypeOf（） 在支持proto属性的非ES5环境中实现 getPrototypeOf if(type Object.getPrototypeOf === &quot;undefined&quot;){ Object.getPrototypeOf = function(obj){ var t = typeof obj; if(!t || ( t !== &quot;onject&quot; &amp;&amp; t !== &quot;function&quot;)){ throw new TypeError（&quot;not an object&quot;）； } return obj._proto_; } } 始终不要修改 proto 属性 使用Object.create函数给新对象设置自定义的原型 使构造函数与 new 操作符无关 防范误用构造函数可能不值得费心去做，但在跨大型代码库中共享构造函数或者构造函数来自一个共享库的时候，就需要多加防范了 当一个函数期望使用new操作符调用时，清晰地文档化该函数 通过使用 new 操作符或Object.create 方法在构造函数定义中调用自身使得该构造函数与调用语法无关 在原型中存储方法 原型是共享方法的渠道之一，将方法存储于原型中由于存储在示例对象中 将方法存储在示例对象中将创建该函数的多个副本，因为每个实例对象都有一份副本 使用闭包存储私有数据 闭包变量是私有的，只能通过局部的引用获取 将局部变量作为私有数据从而通过方法实现信息隐藏 只将实例状态存储在实例对象中 共享可变数据可能会出现问题，因为原型是被其所有的实例共享的 将可变的实力状态存储在实例对象中 认识到 this 变量的隐式绑定问题 this 变量的作用域总是由其最近的封闭函数所确定的 使用一个局部变量（通常命名为 self、me 和 that）使得this绑定对于内部函数是可用的 数组和字典 原型污染 定义：Object 是 js 对象的根，如果在 Object.prototype 上增加任何方法，那么随之使用 for…in 遍历对象属性的时候，此时增加的方法也会计算进去从而造成的原型污染，此外，一些特殊命名的变量名也会对对象造成污染。 原因：for…in 循环除了枚举出对象”自身”的属性外，还会枚举出继承过来的属性 预防： 使用 Object.create(null) 来创建一个没有原型的对象，因此原型污染就无法影响这样的对象行为 使用 hasOwnProperty 方法（只过来对象自身属性）以避免原型污染： var hasOwn= Object.prototype.hasOwnProperty; hasOwn.call(dic，”alice”) 别在 Object.prototype 中增加属性，若不小心增加了属性，也要用 Object.defineProperty方法把属性 enumerable置成false 循环和迭代 使用数组而不要使用字典来存储有序集合 for…in 循环会挑选一定的顺序来枚举对象的属性，如果是有序的，还是用for循环遍历 避免在 for…in 期间修改对象，如果需要修改，应该使用while或for循环 数组的循环优先使用for循环而不是for…in循环 使用迭代方法由于循环：forEach，map，every，some 处理数组内容的改变：map，筛选数组的内容：filter 循环只有一点优于迭代函数，那就是控制流操作，如 break 和 continue 创建 数组、对象，直接使用 var x = []，{} 即可，因为使用 Array 和 Object 不能保证是不是被污染了 库和API设计-【参数】 保持一致的约定 - 约定参数的顺序 - 宽度第一，然后高度： new Widget(320, 640) // width:320, height:240 - CSS顺时针约定：top，right，bottom，left -【参数】 在变量命名和函数签名中使用一致的约定 -【多元化】 不要偏离用户在其他开发平台中很可能遇到的约定 -【检查】真值测试是实现参数默认值的一种简明的方式（this.hostname = hostname || “localhost”），但不适用于允许0、NaN或空字符串为有效参数的情况，比如this.hostname就是等于undefined -【检查】应该提供参数默认值应当采用测试undefined的方式，而不是检查arguments.length，即 this.width = width === undefined ? 320 : width -【参数】使用接收关键字参数的选项对象，options object，既接收Alert({xxx:yyy})的方式，避免参数过多，API失去可扩展性 -【设计】避免不必要的状态，API有时候被归为两类：有状态的(Date对象)和无状态的（幂等性，输入对应固定输出），无状态的API比有状态的API，从易用性，学习容易性而言，更优。无状态的API简洁，易于扩展 -【设计】API绝对不应该重载与其他类型有重叠的类型，一个方法不应该既接收type 1又接收type 2，当重载一个结构类型与其他类型时，先测试其他类型 - 判断是不是数组：Array.isArray 和 toString.call(x) === &quot;[Object Array]&quot; - 将对象转换成数组：[].slice.call(arguments) -【设计】避免过度的强制转换，应该显示的转换，用代码写出来，而不是交给编译器 -【设计】支持方法链 - 使用方法连来连接无状态的操作 - 通过在无状态的方法中返回新对象来支持方法链 - 通过在有状态的方法中返回this来支持方法连 并发 异步的强大能力：JS的运行到完成机制(run-to-complete)，在系统里维护了一个按时间发生顺序的内部事件队列，一次调用一个已注册的回调函数，通过轮询这个队列来完成异步的操作，有序的调用回调函数。 Workers API提供了线程的能力，在一个完全隔离的状态下执行，没有获取全局作用域或应用程序主线程Web页面内容的能力 应该避免将可被并行执行的操作顺序化，嵌套的回调函数应该用Promise 对于异步的代码，多步的处理通常被分隔到时间队列的单独伦次中，因此不可能将它们全部包装在一个try语句块中。事实上，异步的API甚至根本不可能抛出异常，因为，当一个异步的错误发生时，没有一个明显的执行上下文来抛出异常！因此，异步API更加倾向于将错误表示为回调函数的特定参数，即err，在Nodejs平台中，err是回调函数的第一个参数 目前典型的JS环境中一个递归函数同步调用自身过多次会导致失败，原因是：存储过多的栈帧需要的空间量会消耗JS环境分配的空间 循环不能是异步的 使用递归函数在事件循环的单独轮次中执行迭代 function downloadOneAsync(urls, onsuccess, onfailure){ var n = urls.length; function tryNextURL(i){ if(i&gt;=n){ onfailure(&quot;xxxxxxx&quot;); return; } downloadAsync(url[i], onsuccess,function(){ tryNextURL(i+1); }); } tryNextURL(0); } 避免在主事件队列中执行代价高昂的算法，在支持Worker API的平台，该API可以用来在一个独立的事件队列中运行场计算程序，在Work API不可用或代价昂贵的环境中，考虑将计算程序分解到事件循环的多个轮次中 使用计数器来执行并行操作可以避免竞争，Js应用程序中的事件发生时不确定的，即顺序是不可预测的 同步地调用异步的回调函数扰乱了预期的操作序列，并可能导致意想不到的交错代码，也可能导致栈溢出或者错误的异常处理 Promise promise代表最终值，即并行操作完成时最终产生的结果 使用promise组合不同的并行操作，避免数据竞争 在要求有意的竞争条件时，使用select","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"JS文档生成工具-Jsdoc","slug":"JS文档生成工具-Jsdoc","date":"2016-11-17T09:53:00.000Z","updated":"2017-07-13T07:05:42.000Z","comments":true,"path":"2016/11/17/JS文档生成工具-Jsdoc/","link":"","permalink":"https://aaronshi32.github.io/2016/11/17/JS文档生成工具-Jsdoc/","excerpt":"","text":"Jsdoc是一款JS文档的生成工具，提供了丰富的标签，写起文档来十分方便，具体标签用法参考 Jsdoc，这里就不多赘述。但它的用处，远远不止于生成文档，最近通过研究，发现利用Jsdoc工具，可以干更多的事情，待下面剖析。 生成原理首先来谈谈Jsdoc的生成原理，即如何从注释代码生成最后的网页。这个过程其实很简单，通过文档解析器parser对每个js文件进行分析，提取出文档信息，之后的一步很关键，Jsdoc将提取出来的信息存放到了taffydb中，进而根据这些信息来生成网页。 为了更好的解释这个过程，来看Jsdoc的源码 1234/* templates/default/publish.js */exports.publish = function(taffyData, opts, tutorials) &#123; //generate Jsdoc html &#125; 生成html的代码逻辑，就在publish这关键函数中，从接受的参数来看，taffyData中已经存放好解析之后的文档信息，opts是配置文件的解析，tutorials是教程的信息，未做过多研究。那么taffyData中存放的信息试什么样子的呢？来看如下 12345678910111213141516171819202122***** Debug ******[ &#123; comment: '/**\\n* &lt;p&gt;Start ...... @public\\n* @since 1\\n */', meta: &#123; range: [Object], filename: 'LocationManager.js', lineno: 69, code: [Object], vars: [Object] &#125;, description: '&lt;p&gt;Start request location.&lt;/p&gt; ......more.&lt;/p&gt;\\n', permission: [ [Object] ], see: [ '[removeLocationU...r#removeLocationUpdates&#125;' ], params: [ [Object] ], returns: [ [Object] ], access: 'public', since: '1', name: 'requestLocationUpdates', longname: 'xxx.location.LocationManager#requestLocationUpdates', kind: 'function', memberof: 'xxx.location.LocationManager', scope: 'instance', ___id: 'T000002R003347', ___s: true &#125; ] 以上信息就是taffyData中存放的文档信息，可以看到，关键的Jsdoc信息，比如@since 1解析成了since:&#39;1&#39;，@public解析成了access:public，kind:&#39;function&#39;，经过这般解析，taffyData中的数据已经足够可以用来生成网页，从而可以看出，在publish函数中，围绕着taffydb的数据，通过各种数据处理，可以做一些更多的事情，这里只是抛砖引玉，比如统计代码的各种信息、检查Jsdoc是否符合规范等等 以一张图简要描述下Jsdoc的原理 至此，我们分析出了Jsdoc的关键信息：Jsdoc解析之后的信息全部存放到taffyData中，而这些信息作为接口publish的参数，提供了良好的扩展性 如果进一步了解信息转储，请查阅：app.js # app.jsdoc.parser.parse -&gt; parser.js # parser.createParser 这个方法 自定义扩展接下来来看看Jsdoc提供的三种自定义扩展方式，分别是：自定义EventHandler级别的插件，自定义标签，自定义抽象语法树访问器，下面来分别解释下这三种扩展方式的区别 自定义EventHandler级别的插件 这种扩展方式是最高级别的扩展，如果学过监听器设计模式，看如下代码就一目了然了，一个EventHandler插件包含了以下几个阶段的监听 1234567891011exports.handlers = &#123; parseBegin: function(sourcefiles)&#123;&#125;, fileBegin: function(filename)&#123;&#125;, beforeParse: function(filename,content)&#123;&#125;, JsdocCommentFound: function(filename, comment, lineno)&#123;&#125;, symbolFound: function(..params)&#123; &#125;, newDoclet: function(doclet) &#123;&#125;, fileComplete: function(filename,content)&#123;&#125;, parseComplete: function(sourcefiles, doclets)&#123;&#125;, processingComplete: function(doclets)&#123;&#125;&#125;; 不难理解，Jsdoc在生成过程中，抛出了以上这些阶段事件供扩展，每个事件的不同参数，代表的当前阶段产生的中间结果。比如在解析之前(parseBegin)，传入的参数代表源文件文件名列表，可以做一些正比表达式的匹配，指定生成某些文件的文档。在解析完一个文件的Jsdoc信息时(newDoclet)，doclet中就包括了上面taffydb中的信息。有了这几个阶段，相信很轻松的就可以扩展出任何想要的插件，关键在于插件的行为是由哪些阶段完成的。 自定义标签 这种扩展方式是中间级别的扩展，主要用于当Jsdoc官方提供的标签不能满足文档注释需要的时候，选择自定义标签来补充完善Jsdoc的能力，比如，我需要在文档中注释一个权限声明的标签，@permission，那么可以这样做 1234567891011exports.defineTags = function(dictionary) &#123; var opts = &#123; canHaveName: true, mustHaveValue: true, onTagged: function(doclet, tag) &#123; doclet.permission = doclet.permission || []; doclet.permission.push(tag); &#125; &#125; dictionary.defineTag(\"permission\", opts);&#125; Jsdoc提供了defineTags接口来自定义标签，其参数dictionary可以理解为标签字典，如上述代码描述，将自定义标签和属性，添加到dictionary中，这样就能被Jsdoc在解析中识别了，onTagged方法定义了当解析到该标签时，Jsdoc应该做出的操作，这里还是doclet，还记得吗？就是那个taffydb的数据，翻到上面那个taffydb数据信息看看，permission就在其中。自定义标签相比上面的EventHandler级别的插件，实现起来简单、方便。 自定义抽象语法树访问器 这种扩展方式时最低级别的扩展，官方文档中用了(lowest)一词。同样，来看代码 12345exports.astNodeVisitor = &#123; visitNode: function(node, e, parser, currentSourceName) &#123; // do all sorts of crazy things here &#125;&#125;; 由于直接接触的对象是抽象语法数，那么可做的事情就更多了，就好比越是接近系统底层，可扩展的能力越高，越是上层的东西，可扩展能力就越小，Jsdoc提供了astNodeVisitor接口来扩展访问到AST每个节点的行为，感兴趣的同学可以好好实战一番，这里就不过多赘述了 更多Jsdoc的主要原理和扩展方式在上面分析完了，更多完整功能用法，请参考Jsdoc","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://aaronshi32.github.io/tags/JavaScript/"},{"name":"工具","slug":"工具","permalink":"https://aaronshi32.github.io/tags/工具/"}],"keywords":[]},{"title":"读书笔记-大话设计模式","slug":"读书笔记-大话设计模式","date":"2016-11-17T09:53:00.000Z","updated":"2018-04-25T09:00:29.157Z","comments":true,"path":"2016/11/17/读书笔记-大话设计模式/","link":"","permalink":"https://aaronshi32.github.io/2016/11/17/读书笔记-大话设计模式/","excerpt":"","text":"迪米特法则 合成/聚合复用原则 原型模式 模板模式 外观模式 建造者模式 适配器模式 备忘录模式 组合模式 迭代器模式 桥接模式 命令模式 职责链模式 中介模式 享元模式 解释器模式 访问者模式 迪米特法则 也叫最少知识原则，即：类的设计要降低耦合，如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要调用另一个类的某一个方法的话，可以通过第三者转发这个调用 在类的结构设计上，每一个类都应当尽量降低成员的访问权限，设计时应当考虑两个类是直接发生调用关系，还是通过第三方来搭建关系 合成/聚合复用原则 尽量使用合成/聚合，尽量不要使用类继承，因为父类的改变，会影响到所有子类 聚合表示一种弱的拥有关系，体现的是A对象可以包含B对象，但B对象不是A对象的一部分。合成是一种强的拥有关系，体现的是严格的部分和整体的关系。 大雁拥有翅膀（合成），许多大雁聚合成雁群（聚合） 原型模式 深复制：把引用对象的变量指向复制过的新对象，而不是原有的被引用的对象 浅复制：被复制对象的鄋变量都含有与原来的对象相同的值，而所有对其他对象的引用都仍然指向原来的对象 如何实现深复制： 用 私有构造函数 创建引用对象，然后再clone方法里调用私有构造函数，完成克隆，其他的值引用则在clone里面直接赋值就好了 模板模式 定义一个操作中的算法的骨架，而将一些步骤延迟到子类中，模板方法是的子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤 一切逻辑都在父类中写好，需要延迟交给子类的部分，通过让子类@Override方法来实现，或者抽象类实现逻辑，具体类实现细节 外观模式 为子系统中的一组接口提供一个一致的界面，此模式定义了一个高层接口，这个接口使得这一子系统更加容易使用，比如，基金是各种股票的外观模式，购买者只需要和基金打交道就好了 分层的处理：Facade类，承上启下的作用，对外暴露的接口简单，统一 建造者模式 Builder 和 Director： Builder是抽象类，用于让开发者实现一些细节步骤，Director里面包含了Buider的实例，用于封装一些操作逻辑，这样开发者每次只需要根据自己的需求实现Builder，然后交给Director就可以了，不用care操作逻辑是否有遗漏 适用于当创建复杂对象的算法应该独立于该对象的组成部分以及他们的装配方式时适用的模式，即将一个复杂对象的构建与它的表示分离，使得同样的构建过程（Director）可以创建不同的表示(Builder) 状态模式 当一个对象的内在状态改变时允许改变其行为，这个对象看起来像是改变了其类 状态模式主要解决的是当控制一个对象状态转换的条件表达式过于复杂时，把状态的判断逻辑转移到表示不同状态的一系列类当中（实际！） 适配器模式 将一个类的接口转换成客户希望的另外一个接口，Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 使用一个已经存在的类，但如果它的接口，也就是它的方法和你的要求不相同时，就应该考虑用适配器模式，即：使用一个Adapter类实现抽象接口，里面持有待适配的对象，在Override接口方法的时候，将待适配对象的方法实现到接口中 备忘录模式 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。 Originator(发起人，负责创建一个备忘录，即在SaveStatus时创建个Mememto) 和 Memento(备忘录，负责创建状态副本，保存在自己的类中) 和 Caretaker（管理者，负责管理内部的状态副本get和set） 1234567891011121314151617181920212223242526272829303132333435363738class Originator &#123; public Mememto SaveState()&#123; return new Memento(vit,atk,def) &#125; public void RecoveryState(Mememto state)&#123; this.vit = state.vit; this.atk = state.atk; this.def = state.def; &#125;&#125;class Memento &#123; private int vit; private int atk; private int def; public Memento(int vit, int atk, int def)&#123; this.vit = vit; this.atk = atk; this.def = def; &#125; /... get/set .../&#125;class Caretaker &#123; private Memento memento; /... get/set .../&#125;//----------To Do----------Caretaker.memento = Originator.SaveState();Originator.RecoveryState(Caretaker.memento); 组合模式 将对象组合成属性结构以表示”部分-整体”的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性 “部分-整体” 的关系实质上为 “树叶-根”的关系，所有的组件实现统一的接口，这个接口包含扩展组件的能力，即 123456789101112abstarct class Component&#123; protected string name; public Component(string name)&#123; this.name = name; &#125; public abstarct void Add(Compoent c); // 扩展能力 public abstarct void Remove(Compoent c); // 扩展能力 public abstarct void Display(Compoent c); // 各司其职&#125; 迭代器模式 提供一种方法顺序访问一个聚合对象中各个元素，而又不暴露该对象的内部表示，即当你需要访问一个聚集对象，而且不管这些对象是什么，就应该考虑用迭代器模式 迭代器抽象类（Iterator） 和 迭代器具体类 (ConcreteIterator) 聚集抽象类（Aggregate）和 聚集具体类（ConcreteAggregate） 123456789101112131415161718192021222324252627282930abstaract class Iterator &#123; public abstract object First(); public abstract object Next(); public abstract bool IsDone(); public abstract object CurrentItem();&#125;abstaract class Aggregate &#123; public abstract Iterator createIterator();&#125; class ConcreteAggregate：Aggregate &#123; private IList&lt;object&gt; items = new List&lt;object&gt;(); public orerride Iterator createIterator&#123; return new ConcreteIterator(this); &#125;&#125; class ConcreteIterator : Iterator &#123; private ConcreteAggregate aggregate; public ConcreteIterator(ConcreteAggregate aggregate)&#123; this.aggregate = aggregate; &#125; /** * Override methods in Iterator * /&#125; 桥接模式 将抽象部分与它的实现部分分离，使它们都可以独立的变化 Abstraction 和 Implementor 123456789101112131415161718192021222324252627282930313233abstract class Implementor &#123; public abstarct void Operation();&#125;class ConcreteImplementorA : Implenmentor&#123; public override void Operation()&#125;class ConcreteImplementorB : Implenmentor&#123; public override void Operation()&#125;class Abstraction &#123; protected Implementor implementor; public void SetImplementor(Implementor implementor)&#123; this.implementor = implementor; &#125; public virtual void Operation()&#123;&#125;&#125;class RefinedAbstraction: Abstraction&#123; public override void Operation()&#123; implementor.Operation(); &#125;&#125;static void Main(string[] args)&#123; Abstraction ab = new RefinedAbstraction(); ab.SetImpelementor(new ConcreteImplementorA()); ad.Operation(); ab.SetImpelementor(new ConcreteImplementorB()); ad.Operation();&#125; 命令模式 分离两个类的职责，统一规划到中间的消息队列管理：客户 - 服务员 - 烤肉者 将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化，对请求排队或记录请求日志，以及支持可撤销的操作，它将请求一个操作的对象与知道怎么执行一个操作的对象分隔开: Command 和 Receiver 绑定，而 Command 的调度执行，由 Invoker 负责 123456789101112131415161718192021222324252627282930313233343536373839abstract class Command&#123; protected Receiver receiver; public Command(Receiver receiver)&#123; this.receiver = receiver; &#125; abstract public void Execute();&#125;class ConcreteCommand : Command&#123; public ConcreteCommand(Receiver receiver): base(receiver)&#123;&#125; public override void Execute()&#123; receiver.Action(); &#125;&#125;class Invoker&#123; private Command command; public void SetCommand(Command command)&#123; this.command = command; &#125; public void ExecuteCommand()&#123; command.Execute(); &#125;&#125;class Receiver&#123; public void Action()&#123;&#125;&#125;public static void Main()&#123; Receiver r = new Receiver(); Command c = new ConcreteCommand(r); Invoker i = new Invoker(); i.SetCommand(c); i.ExecuteCommand();&#125; 职责链模式 使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系，将这个对象连成一个链，并沿着这条链传递该请求，知道有一个对象处理它为止（Chain） 接收者和发送者都没有对方的明确信息，且链中的对象自己也并不知道链的结构。结果是职责链可简化对象的相互连接，仅需要保持一个向后引用 随时增加或修改处理一个请求的结构，增强了给对象指派职责的灵活性，最坏情况：到末端都得不到处理，需要事先考虑全面 123456789101112131415161718abstract class Handler&#123; protected Handler successor; public void SetSuccessor(Handler successor)&#123; this.successor = successor; &#125; public abstract void HandleRequest(int request);&#125;class ConcreteHandler1 : Handler &#123; public override void HandleRequest(int request)&#123; if(request &gt;= 0 &amp;&amp; request &lt; 10)&#123; // do handle request &#125;else if(successor != null)&#123; successor.HandleRequest(request); &#125; &#125;&#125; 中介模式 对象之间大量的连接使得一个对象不可能在没有其他对象的支持下工作，系统表现为一个不可分割的整理，所谓低耦合的解决方案：中介模式其中之一 用一个中介对象来封装一系列的对象交互。中介者使得各对象不需要显示地相互引用，从而使其耦合松散，并且可以独立的改变他们之间的交互。 缺点：把交互的复杂性变成了中介者的复杂性，需要着重考虑，对象之间多对多的交互，首先应当考虑系统设计上的合理性 享元模式 如果一个应用程序使用了大量的对象，而大量的这些对象造成了很大的存储开销时就应该考虑使用享元模式，还有就是对象的大多数的外部状态，如果删除不影响的话，那么可以用相对较少的共享对象取代很多组对象，此时也应该考虑使用享元模式，比如：线程池 享元模式共享的是对象，对象数量是有限制的，因此会在工厂内部保存对象的引用，而工厂模式是生成对象，对象数量是无限制的 1234567891011121314151617181920212223abstract class Flyweight&#123; public abstract void Operation(int ee)&#125;class ConcreteFlyweight : Flyweight&#123; public override void Operation(int ee)&#123;&#125;&#125;class FlyweightFactory&#123; private Hashtable flyweights = new Hashtable(); public FlyweightFactory()&#123; //内部保留有限的对象数 flyweights.Add(\"X\", new ConcreteFlyweight()); flyweights.Add(\"Y\", new ConcreteFlyweight()); flyweights.Add(\"Z\", new ConcreteFlyweight()); &#125; public Flyweight GetFlyweight(string key)&#123; return ((Flyweight)flyweights[key]); &#125;&#125; 解释器模式 同样的一个抽象操作，需要不同的实施方法，则考虑解释器模式 缺点在于需要维护多个解释器实现 1234567891011121314151617181920212223242526abstract class AbstractExpression &#123; public abstract void Interpret(Context ctx)&#125;TerminalExpression: AbstractExpression &#123; public override void Interpret(Context ctx)&#123;&#125;&#125;NonterminalExpression: AbstractExpression &#123; public override void Interpret(Context ctx)&#123;&#125;&#125;// 全局信息class Context &#123; private string input; private string output;&#125;static void Main(string[] args)&#123; Context ctx = new Context(); AbstractExpression terminal = new TerminalExpression(); terminal.interpret(ctx); AbstractExpression terminal = new NonterminalExpression(); nonterminal.interpret(ctx);&#125; 访问者模式 表示一个作用于某对象结构中的各元素的操作，它使你可以在不改变各元素的类前提下定义作用于这些元素的新操作。 访问者模式的目的是要把处理从数据结构分离出来，有稳定的数据结构，又有易于变化的算法，使用访问者模式就是比较合理的，因为增加算法的操作变得尤为容易 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647abstract class Visitor &#123; public abstract void useAlgorithmA(concreteElementA A); public abstract void useAlgorithmB(concreteElementB B)&#125;ConcreteVisitor1: Visitor &#123; // implements&#125;ConcreteVisitor2: Visitor &#123; // implements&#125;abstract class Element&#123; public abstract void Accept(Visitor visitor);&#125; ConcreteElementr1 : Element &#123; public override void Accept(Visitor visitor)&#123; visitor.useAlgorithmA(this); &#125;&#125;ConcreteElementr2 : Element &#123; public override void Accept(Visitor visitor)&#123; visitor.useAlgorithmB(this); &#125;&#125;class Contorller &#123; //整合 visitor 和 element, 使得对外操作统一 private IList&lt;Element&gt; elements = new List&lt;Element&gt;(); public void Attach(Element element)&#123; elements.Add(element); &#125; public void Detach(Element element)&#123; elements.Remove(element); &#125; public void Accept(Visitor visitor)&#123; foreach(Element e in elements)&#123; e.Accept(visitor); &#125; &#125;&#125;","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]},{"title":"Node命令行工具-Yargs和Commander","slug":"Node命令行工具-Yargs和Commander","date":"2016-11-16T10:25:40.000Z","updated":"2016-11-17T09:15:38.000Z","comments":true,"path":"2016/11/16/Node命令行工具-Yargs和Commander/","link":"","permalink":"https://aaronshi32.github.io/2016/11/16/Node命令行工具-Yargs和Commander/","excerpt":"","text":"日常使用Node时，避免不了编写几个命令行工具。作为命令行工具，最直接的就是针对输入的各种参数、选项等，执行相应的命令。使用原生的progress.argv虽然可以进行参数判断，但使用起来十分不方便，为此研究了下Node第三方命令行工具 yargs 和 commander Yargs首先来看一个简单的例子 1234567891011121314λ index -h Usage: index.js &lt;command&gt; -a [num] -b [num] Commands: add same as a + b plus same as a - b Options: -a, -A load a parameter [number] [required] -b, -B load b parameter [number] [required] -h, --help Show help [boolean] Examples: index.js add -a 10 -b 3 //compute 10 add 3 这段帮助信息是用 yargs 编写的，实现起来很简单，来看代码 123456789101112131415161718192021#!/usr/bin/env nodevar yargs = require('yargs');ARGV = yargs.usage('Usage: $0 &lt;command&gt; -a [num] -b [num]') .command('add', 'same as a + b') .command('plus', 'same as a - b') .example('$0 add -a 10 -b 3', '//compute 10 add 3') .alias('a', 'A') .alias('b', 'B') .alias('h', 'help') .number('a') .number('b') .describe('a', 'load a parameters') .describe('b', 'load b parameters') .demand(['a','b']) .help('h') .argvif(ARGV.h || ARGV.help)&#123; print(ARGV.help()); process.exit(0);&#125; 上述代码可以分为几个阶段 usage: 命令行工具的使用方法 command: 如果需要输入命令，则用 command(&#39;命令&#39;，&#39;描述&#39;) 的方式标注 example: 给出使用的例子，用 example(&#39;命令&#39;，&#39;描述&#39;) 的方式标注 alias: 对输入参数别名化，比如 -a 和 -A 是等效的 number(string,boolean,array…): 对输入的参数进行类型转换，比如.array(a), -a 10 4 3 5，那么 ARGV.a 是 [10,4,3,5] describe: 对输入参数进行描述，用 describe(&#39;参数&#39;，&#39;描述&#39;) 的方式标注 require：标注哪些参数是必备的，比如 a 和 b 的后面加入了 [require] 的描述 help/epilog: 辅助的帮助/版权信息 就这样，一个简单的命令行信息交互帮助信息就生成了，后面可以用 ARGV.xxx 获得输入的参数，做后续程序的逻辑判断。 进阶用法 ARGV._ : 获得未指定的输入参数，比如 index -a 10 -b 3 5 “no” “13”，则 ARGV._ 获得 [‘5’,’no’,’13’] .check(cb)：检查输入参数合法性，如果 cb 返回 false，则输出帮助信息并退出 .fail(cb)：当工具运行出错时，输出错误信息 更多用法请参考 yargs，一般情况下，用 yargs 快速搭建一个命令行的使用帮助信息就足够了，不建议把判断的复杂逻辑也集成到 yargs 中 Commander还是之前的例子，用 commander 实现的运行结果如下 123456789101112131415λ commander --help Usage: commander &lt;command&gt; -a [num] -b [num] Commands: add same as a + b plus same as a - b Options: -h, --help output usage information -a, -A [num] load a parameter -b, -B [num] load b parameter 来看代码 123456789101112131415161718#!/usr/bin/env nodevar program = require('commander');program .usage('&lt;command&gt; -a [num] -b [num]') .option('-a, -A [num]', 'load a parameter') .option('-b, -B [num]', 'load b parameter');program .command('add') .description('same as a + b');program .command('plus') .description('same as a - b');program.parse(process.argv); 通过对比，个人比较喜欢 commander 的代码风格，每个 command 单独处理，可以各种自定义，比如针对某个命令输出 help 信息，而 yargs 的那种分块编码，也值得借鉴，从代码量而言，yargs 和 commander 指定复杂的命令时，前者的代码量会比较少，同样，前者在功能上比后者更加丰富，适合复杂通用的命令行工具，更多用法请参考 commander 后记：Windows 上如何调用nodejs命令行工具在本机上找到任意一个npm的安装包，都能发现有 xxx.cmd 这样的文件，打开如下 1234567@IF EXIST \"%~dp0\\node.exe\" ( \"%~dp0\\node.exe\" \"%~dp0\\xxx.js\" %*) ELSE ( @SETLOCAL @SET PATHEXT=%PATHEXT:;.JS;=;% node \"%~dp0\\xxx.js\" %*) 不难理解，在window上要执行nodejs，必须要用 node xxx.js 的方式，而这段代码就是省略了前面的node，直接用 xxx 即可，比如之前的例子中，#!/usr/bin/env node用来在linux上指定脚本运行环境为node，所以可以直接用 ./script.js 加参数运行，在window上，将上面的文件中 xxx 替换成script，即可用 script 加参数运行了 如果以后涉及到用Node写两个平台通用的工具，请记住编写这个文件","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://aaronshi32.github.io/tags/JavaScript/"},{"name":"工具","slug":"工具","permalink":"https://aaronshi32.github.io/tags/工具/"}],"keywords":[]},{"title":"JS压缩混淆工具-UglifyJS","slug":"JS压缩混淆工具-UglifyJS","date":"2016-11-14T08:08:52.000Z","updated":"2016-11-16T09:39:58.000Z","comments":true,"path":"2016/11/14/JS压缩混淆工具-UglifyJS/","link":"","permalink":"https://aaronshi32.github.io/2016/11/14/JS压缩混淆工具-UglifyJS/","excerpt":"","text":"最近写了一些JS的项目，顺便研究了下UglifyJS工具，它是一款JS代码处理工具，提供了压缩，混淆和代码规范化等功能，同时还支持命令行和NodeJS两种使用方式，在代码基本”格式化”功能的基础上，可自定义扩展一些功能选项，满足具体项目中的使用。在业界内，JQuery使用UglifyJS进行代码压缩混淆，可见其结果具备准确性和稳定性。 使用方式：uglifyjs [input files] [options] SourceMapUglifyJS支持使用/生成SourceMap, 那什么是SourceMap，简单来说就是一个解码表，JS代码从原生到压缩混淆之后，从原来的几百行甚至几千行中，压缩到几十行。不仅代码行数骤减，同时经过混淆，代码的变量声明都进行了替换，因此当我们看混淆之后的代码，很难读懂代码的本意，这也是压缩混淆的初衷，减少JS代码的大小，方便网络传输，理论上对代码进行了保护。利弊相对，压缩混淆之后的代码对调试来说尤为困难，因此SourceMap就出现了。 简单来讲，压缩混淆之后的代码通过SourceMap，即可还原成原始的JS代码，这就是为什么说它是一个解码表的原因，因为反向还原了JS代码，结合到UglifyJS而言，其支持以下几个选项： source-map 指定生成一个SourceMap的文件名 source-map-root 指定SourceMap的根目录位置 source-map-url 指定SourceMap的网络路径 source-map-inline 在压缩混淆的JS代码后，追加SourceMap内容 in-source-map 指定一个SourceMap 功能：压缩使用UglifyJS进行代码压缩，-c 选项即启用压缩，例如： 12345678910111213\"use strict\";var net = require('net');const IP = \"127.0.0.1\";const PORT = 8887;var client = new net.Socket();client.connect(PORT, IP, function() &#123; console.log('CONNECTED TO: ' + IP + ':' + PORT); client.write('I am Chuck Norris!');&#125;); 压缩后变成 1\"use strict\";var net=require(\"net\");const IP=\"127.0.0.1\",PORT=8887;var client=new net.Socket;client.connect(PORT,IP,function()&#123;console.log(\"CONNECTED TO: \"+IP+\":\"+PORT),client.write(\"I am Chuck Norris!\")&#125;); 可以看出，压缩之后去掉了空行以及连续声明时候的关键字const，如果让你调试这样的出错代码，没有SourceMap，则无从下手 功能：混淆使用UglifyJS进行代码混淆，-m 选项即启用混淆，例如，还是上面的代码，经过混淆后变成 1\"use strict\";var c=require(\"net\");const e=\"127.0.0.1\";const n=8887;var o=new c.Socket;o.connect(n,e,function()&#123;console.log(\"CONNECTED TO: \"+e+\":\"+n);o.write(\"I am Chuck Norris!\")&#125;); 可以看出，混淆把变量都替换成了单字符，相比于压缩，进一步的减小了代码量 总结UglifyJS是一款出色的JS代码压缩，混淆工具，其具有丰富的功能，例如代码美化，支持NodeJS自定义编程等，更多细节请参考 UglifyJS","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://aaronshi32.github.io/tags/JavaScript/"},{"name":"工具","slug":"工具","permalink":"https://aaronshi32.github.io/tags/工具/"}],"keywords":[]},{"title":"读书笔记-Google工作整理术","slug":"读书笔记-Google工作整理术","date":"2016-10-26T09:53:00.000Z","updated":"2016-12-02T01:02:26.000Z","comments":true,"path":"2016/10/26/读书笔记-Google工作整理术/","link":"","permalink":"https://aaronshi32.github.io/2016/10/26/读书笔记-Google工作整理术/","excerpt":"","text":"认知大脑 为了实现大脑压力最小化，要把生活组织的有条不紊 让信息尽可能快地离开大脑 多重任务通常让你降低效率 利用故事去记忆 共勉句 我们做决定经常是建立在害怕失去什么的基础上，而不是考虑希望得到什么 紊乱无需让你搞到重重压力，进而举措失当，承受等大的压力，让你更加失误连连 音乐可以减缓压力 识别要比回忆轻松地多 要回忆一个事实，先努力想想第一次注意到那个事实时你正在干什么，这是很有帮助的 面对太多选择的时候，我们经常倾向于选择最熟悉的东西 做决定的关键在于你的目标是什么，并确定他们的优先次序，展望各不相同的最终结果，比较各种决定的效果。 多样性会让你和你的同事在实现目标的过程中更加出类拔萃 错误的有序 仅仅因为一直都按照某种特定方式做某事，并不意味着就该永远这样做（朝九晚五、暑假） 知识不是力量，共享知识才是力量 共勉句 分享自己的所知，鼓励别人也这么做，你就会从他们那里学有所获，他们也可以从你这里学到东西，然后你们就会胜任更好的工作 解决问题的办法不是更加辛苦的工作，而是利用现有的工具和技术，巧妙的工作 突破制约 进行组织安排时，要绕开的是实际制约而不是假性制约 对自己要坦诚，但是千万不要自我评判，坦诚的检视下你所处的环境，多听听可信赖的人的意见 要懂得什么时候忽略制约，不要第一步都迈不出去 共勉句 学会识别、接受并绕开你的实际制约，从而不在你无法改变的事情上浪费时间和精力 实际制约通常涉及你绝对无法控制的某些因素 有时候，超出我们控制范围的制约刚好就潜伏在你能控制的哪些制约的表面之下 源自情绪的制约最难识别和控制，无论面对什么制约，控制好自己的举止，行动，愿望和情绪 在不可预料的意外发生之前，对于那些面临压力时还会变本加厉的制约，要及早辨别并准备好绕开它的有效策略，这是十分重要的 目标决策 在发动汽车之前，一定要确切的搞清楚自己要到哪里去，还要知道选择什么途径去 在实现目标的方式上灵活变通 共勉句 在一件事之前，要搞清楚 你的问题是什么？，也就间接的让自己清楚目标是什么 所做的每一件事都要从自己的目标出发，切勿盲目，偏离初衷 一旦确定了某个项目的目标，先学会四处看看，确定一下可能面临的问题是否有人已经解决了，别人所用的解决办法中哪些要素可以采纳或借鉴 给别人委派任务涉及信任问题，请求别人做事情，就一定要信任别人，更不要剥夺别人学习新的东西、担负更多责任的机会 针对你的决策将会产生的不同结果，逐个试试看，形象的展望一个决定可能产生的不同结果，可以帮你明确什么才是你真正想要的 任何决定都需要经过反复推敲做出假设，写下来，每天看一遍 Google搜索的时代 不要给信息处处归档，用的时候去搜索就行了 共勉句 搜索功能是新式组织管理的基础，我们不必在文件归档方面耗时间和精力，也用不着为了找到重要信息而劳神费力 基本搜索引擎技巧 PageRank：别人说他好，才是真的好（引用的越多，越有价值） 技巧 尽量增强搜索条件的描述性（关键词多一些） 在搜索条件短语中使用引号（引号表示确认在搜索的网页中包含） 使用形容词搜索（”apple ~cheap”） 排除不想要的结果（”opera -browser”） 通过数值范围获取特定信息（…表示数值范围，”digital camera” $100…$300） 搜索特定网站（”paris hotels” site:nytimes.com） 搜索特定类型的文件（”paris hotels” filetype:xls） 信息过滤器 目标是信息存储的向导，大脑中只保存真正需要记忆的内容 大块的内容要化整为零 每周拿出些时间回顾关键信息，再次梳理归类 共勉句 幸运的是，我们接收的绝大部分信息都没有必要记住，因为大部分信息不值得我们记忆 信息的有效性取决于你的目标是什么，不断的分门别类，建立信息等级（忽略，今后它用，记忆），离目标越近的信息就是越需要的 重复和回归通常可以帮你确定目标 只要有可能，就把大块信息分解成小块，就不会感到困难了，分解信息会当你找出其中的规律和主题 纸质文档 与 电子文档 没有一个完美无缺的组织方法 通常消化吸收之后的信息，写下来会比较好 做笔记的过程就是锻炼信息筛选的过程，好的笔记不是全，而是精 共勉句 每当文件柜即将塞满的时候，我会直接筛选一遍文档，不再需要的财务票据及其它敏感信息，我会当场销毁，其它不用的文件继续回收利用 重要文档，必须纸质保存 信息多的时候，先分类，后处理 要使用信誉好的在线存储或备份业务 针对需要处理的信息，你要确定什么时候最可能用到，如何去使用，需要保存的期限有多长，以及你打算与谁共享，从而决定存储方式。 何时使用纸质工具 解决某个问题或者提出一种想法，请随时记录下来 目标是记住或者真正吸收的信息，打印出来比盯着屏幕要好 随身携带一个小笔记本 何时使用数字工具 需要与别人共享的信息 需要快速查看的信息 存储比较久的信息 电子邮箱 Gmail 在数字信息中加上相应的关键词，以便日后容易找到 共勉句 Gmail 是一种近乎理想的脚手架，简单易用，免费，容量大 标签比文件夹更有效，一封邮件可以拥有多个标签 aaronshicf9032+shopping@gmail.cn 简直爽的不要不要的，自动过滤 把密码提示线索保存在Gmail邮箱之类的便于访问的安全场所 日历 Google Calendar 跟特定人士共享特定日程，其实就是为他们过滤你的日程信息 尽量使用已经上手的工具 协同文档 Google doc 轻量级，适用于协作：存放重要本地文档、组织一个待办事项总清单、掌握资金使用情况、共享临时信息 每周拿出些时间回顾关键信息 也许是精华 如何减轻大脑的压力：及时加注解，有助于以后了解背景信息 思维转换：从一件事情过度到另一件事，转换气味的时候，大脑必须要吧短期记忆中的当前内容转移到长期记忆中，以便为新信息腾出空间，这样一来，有些信息就会丢失 思维转换次数越多，各种思维背景之间的关联度就月底，大脑的工作难度就越大 晚上要睡好，在你和你的大脑都能休息好的时候，转换思维要容易的多 如何减少思维转换的次数 当需要转换的时候，随手加上背景注解，方便下次直接还原当时的想法 把类似的任务或会议安排在一起 分辨哪些分神的因素导致频繁进行并不必要的思维转换，然后减少这些因素 心无旁骛的工作状态：定期查收邮件，关闭手机干扰，隔间里工作 召开会议之前先的明确会议主题和目标 把工作和生活融为一体，而不是力图在二者之间求平衡 在效率下降的时候自我放松一下，同时需要工作的时候随时工作 如何处理意外事件 无论如何都要”把握当前”，在危机出现之前，如果已经做到未雨绸缪，胸有成竹，那现在就会更加从容不迫，有备无患 在你面对重大挑战的时候，典型反应就是寻求更多信息 在出现危机时，依然具备处理好当前事情的能力，即掌控当前局面的能力，而这个能力需要实践经验的积累 你无法预知未来，但你可以确认正在发生的坏事表现出来的征兆，从而未雨绸缪，以便坏事发生时，知道如何应对 多跟哪些不同于自己的朋友沟通交流，这在艰难时期更加重要 过分的考虑未来情况，经常会失去对当前的把握 找工作：真的可以如此高效？ 面试的时候，希望对方记住你，那么请利用故事去记忆 仅仅因为一直都是按照某种特定方式做某事，并不意味着就该永远这样做 进行组织安排时，要绕开的是实际制约而不是假性制约，面试的时候要理清楚什么是自身的实际制约，才能专注于一个目标 每周拿出写关键事件回顾关键信息 学会面试记录：尽量使用已经上手的工具 + 在数字信息上加上相应的关键词","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://aaronshi32.github.io/tags/读书/"}],"keywords":[]}]}